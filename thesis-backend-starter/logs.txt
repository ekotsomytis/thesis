
==> Audit <==
|------------|------------------------|----------|-----------------|---------|----------------------|----------------------|
|  Command   |          Args          | Profile  |      User       | Version |      Start Time      |       End Time       |
|------------|------------------------|----------|-----------------|---------|----------------------|----------------------|
| start      | --driver=hyperkit      | minikube | christinamanara | v1.35.0 | 18 May 25 21:46 EEST | 18 May 25 21:50 EEST |
| ip         |                        | minikube | christinamanara | v1.35.0 | 18 May 25 21:51 EEST | 18 May 25 21:51 EEST |
| docker-env |                        | minikube | christinamanara | v1.35.0 | 18 May 25 22:00 EEST | 18 May 25 22:00 EEST |
| service    | thesis-backend-service | minikube | christinamanara | v1.35.0 | 18 May 25 22:04 EEST |                      |
| docker-env |                        | minikube | christinamanara | v1.35.0 | 18 May 25 22:05 EEST | 18 May 25 22:05 EEST |
| service    | thesis-backend-service | minikube | christinamanara | v1.35.0 | 18 May 25 22:07 EEST |                      |
|------------|------------------------|----------|-----------------|---------|----------------------|----------------------|


==> Last Start <==
Log file created at: 2025/05/18 21:46:47
Running on machine: MacBookPro
Binary: Built with gc go1.23.4 for darwin/amd64
Log line format: [IWEF]mmdd hh:mm:ss.uuuuuu threadid file:line] msg
I0518 21:46:47.158986    1457 out.go:345] Setting OutFile to fd 1 ...
I0518 21:46:47.160839    1457 out.go:397] isatty.IsTerminal(1) = true
I0518 21:46:47.160855    1457 out.go:358] Setting ErrFile to fd 2...
I0518 21:46:47.160874    1457 out.go:397] isatty.IsTerminal(2) = true
I0518 21:46:47.161780    1457 root.go:338] Updating PATH: /Users/christinamanara/.minikube/bin
W0518 21:46:47.163961    1457 root.go:314] Error reading config file at /Users/christinamanara/.minikube/config/config.json: open /Users/christinamanara/.minikube/config/config.json: no such file or directory
I0518 21:46:47.168210    1457 out.go:352] Setting JSON to false
I0518 21:46:47.283528    1457 start.go:129] hostinfo: {"hostname":"MacBookPro.home","uptime":138,"bootTime":1747593869,"procs":514,"os":"darwin","platform":"darwin","platformFamily":"Standalone Workstation","platformVersion":"15.0","kernelVersion":"24.0.0","kernelArch":"x86_64","virtualizationSystem":"","virtualizationRole":"","hostId":"99bc5631-69b8-5e2d-8ff5-5a821759ee96"}
W0518 21:46:47.283754    1457 start.go:137] gopshost.Virtualization returned error: not implemented yet
I0518 21:46:47.314327    1457 out.go:177] ðŸ˜„  minikube v1.35.0 on Darwin 15.0
W0518 21:46:47.375113    1457 preload.go:293] Failed to list preload files: open /Users/christinamanara/.minikube/cache/preloaded-tarball: no such file or directory
I0518 21:46:47.376008    1457 notify.go:220] Checking for updates...
I0518 21:46:47.378072    1457 driver.go:394] Setting default libvirt URI to qemu:///system
I0518 21:46:47.503266    1457 out.go:177] âœ¨  Using the hyperkit driver based on user configuration
I0518 21:46:47.547103    1457 start.go:297] selected driver: hyperkit
I0518 21:46:47.547136    1457 start.go:901] validating driver "hyperkit" against <nil>
I0518 21:46:47.547164    1457 start.go:912] status for hyperkit: {Installed:true Healthy:true Running:true NeedsImprovement:false Error:<nil> Reason: Fix: Doc: Version:}
I0518 21:46:47.549791    1457 install.go:52] acquiring lock: {Name:mk4023283b30b374c3f04c8805d539e68824c0b8 Clock:{} Delay:500ms Timeout:10m0s Cancel:<nil>}
I0518 21:46:47.551071    1457 install.go:117] Validating docker-machine-driver-hyperkit, PATH=/Users/christinamanara/.minikube/bin:/Users/christinamanara/opt/anaconda3/bin:/opt/miniconda3/condabin:/usr/local/bin:/System/Cryptexes/App/usr/bin:/usr/bin:/bin:/usr/sbin:/sbin:/var/run/com.apple.security.cryptexd/codex.system/bootstrap/usr/local/bin:/var/run/com.apple.security.cryptexd/codex.system/bootstrap/usr/bin:/var/run/com.apple.security.cryptexd/codex.system/bootstrap/usr/appleinternal/bin:/Library/TeX/texbin
W0518 21:46:47.551255    1457 install.go:62] docker-machine-driver-hyperkit: exec: "docker-machine-driver-hyperkit": executable file not found in $PATH
I0518 21:46:47.574340    1457 out.go:177] ðŸ’¾  Downloading driver docker-machine-driver-hyperkit:
I0518 21:46:47.617999    1457 download.go:108] Downloading: https://github.com/kubernetes/minikube/releases/download/v1.35.0/docker-machine-driver-hyperkit-amd64?checksum=file:https://github.com/kubernetes/minikube/releases/download/v1.35.0/docker-machine-driver-hyperkit-amd64.sha256 -> /Users/christinamanara/.minikube/bin/docker-machine-driver-hyperkit
I0518 21:46:48.128804    1457 driver.go:46] failed to download arch specific driver: getter: &{Ctx:context.Background Src:https://github.com/kubernetes/minikube/releases/download/v1.35.0/docker-machine-driver-hyperkit-amd64?checksum=file:https://github.com/kubernetes/minikube/releases/download/v1.35.0/docker-machine-driver-hyperkit-amd64.sha256 Dst:/Users/christinamanara/.minikube/bin/docker-machine-driver-hyperkit.download Pwd: Mode:2 Umask:---------- Detectors:[0x109f44260 0x109f44260 0x109f44260 0x109f44260 0x109f44260 0x109f44260 0x109f44260] Decompressors:map[bz2:0xc0004f4b20 gz:0xc0004f4b28 tar:0xc0004f4a30 tar.bz2:0xc0004f4a40 tar.gz:0xc0004f4a50 tar.xz:0xc0004f4b00 tar.zst:0xc0004f4b10 tbz2:0xc0004f4a40 tgz:0xc0004f4a50 txz:0xc0004f4b00 tzst:0xc0004f4b10 xz:0xc0004f4b30 zip:0xc0004f4b40 zst:0xc0004f4b38] Getters:map[file:0xc0013fc6f0 http:0xc000912b40 https:0xc000912b90] Dir:false ProgressListener:0x109f0e3c0 Insecure:false DisableSymlinks:false Options:[0x105d32780]}: invalid checksum: Error downloading checksum file: bad response code: 404. trying to get the common version
I0518 21:46:48.128930    1457 download.go:108] Downloading: https://github.com/kubernetes/minikube/releases/download/v1.35.0/docker-machine-driver-hyperkit?checksum=file:https://github.com/kubernetes/minikube/releases/download/v1.35.0/docker-machine-driver-hyperkit.sha256 -> /Users/christinamanara/.minikube/bin/docker-machine-driver-hyperkit
I0518 21:46:52.949505    1457 install.go:79] stdout: 
I0518 21:46:52.979679    1457 out.go:177] ðŸ”‘  The 'hyperkit' driver requires elevated permissions. The following commands will be executed:

    $ sudo chown root:wheel /Users/christinamanara/.minikube/bin/docker-machine-driver-hyperkit 
    $ sudo chmod u+s /Users/christinamanara/.minikube/bin/docker-machine-driver-hyperkit 


I0518 21:46:53.003149    1457 install.go:99] testing: [sudo -n chown root:wheel /Users/christinamanara/.minikube/bin/docker-machine-driver-hyperkit]
I0518 21:46:53.173327    1457 install.go:101] [sudo chown root:wheel /Users/christinamanara/.minikube/bin/docker-machine-driver-hyperkit] may require a password: exit status 1
I0518 21:46:53.173415    1457 install.go:106] running: [sudo chown root:wheel /Users/christinamanara/.minikube/bin/docker-machine-driver-hyperkit]
I0518 21:46:57.083131    1457 install.go:99] testing: [sudo -n chmod u+s /Users/christinamanara/.minikube/bin/docker-machine-driver-hyperkit]
I0518 21:46:57.161098    1457 install.go:106] running: [sudo chmod u+s /Users/christinamanara/.minikube/bin/docker-machine-driver-hyperkit]
I0518 21:46:57.236681    1457 start_flags.go:310] no existing cluster config was found, will generate one from the flags 
I0518 21:46:57.251128    1457 start_flags.go:393] Using suggested 2200MB memory alloc based on sys=8192MB, container=0MB
I0518 21:46:57.257341    1457 start_flags.go:929] Wait components to verify : map[apiserver:true system_pods:true]
I0518 21:46:57.262189    1457 cni.go:84] Creating CNI manager for ""
I0518 21:46:57.264476    1457 cni.go:158] "hyperkit" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I0518 21:46:57.264518    1457 start_flags.go:319] Found "bridge CNI" CNI - setting NetworkPlugin=cni
I0518 21:46:57.265953    1457 start.go:340] cluster config:
{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279 Memory:2200 CPUs:2 DiskSize:20000 Driver:hyperkit HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.32.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP: Port:8443 KubernetesVersion:v1.32.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/Users:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s}
I0518 21:46:57.267572    1457 iso.go:125] acquiring lock: {Name:mk3112ae9ef20abcf2f5c7e8f9c24b6061a9c9d7 Clock:{} Delay:500ms Timeout:10m0s Cancel:<nil>}
I0518 21:46:57.294889    1457 out.go:177] ðŸ’¿  Downloading VM boot image ...
I0518 21:46:57.346279    1457 download.go:108] Downloading: https://storage.googleapis.com/minikube/iso/minikube-v1.35.0-amd64.iso?checksum=file:https://storage.googleapis.com/minikube/iso/minikube-v1.35.0-amd64.iso.sha256 -> /Users/christinamanara/.minikube/cache/iso/amd64/minikube-v1.35.0-amd64.iso
I0518 21:47:39.656245    1457 out.go:177] ðŸ‘  Starting "minikube" primary control-plane node in "minikube" cluster
I0518 21:47:39.696947    1457 preload.go:131] Checking if preload exists for k8s version v1.32.0 and runtime docker
I0518 21:47:39.857977    1457 preload.go:118] Found remote preload: https://storage.googleapis.com/minikube-preloaded-volume-tarballs/v18/v1.32.0/preloaded-images-k8s-v18-v1.32.0-docker-overlay2-amd64.tar.lz4
I0518 21:47:39.858378    1457 cache.go:56] Caching tarball of preloaded images
I0518 21:47:39.860183    1457 preload.go:131] Checking if preload exists for k8s version v1.32.0 and runtime docker
I0518 21:47:39.885893    1457 out.go:177] ðŸ’¾  Downloading Kubernetes v1.32.0 preload ...
I0518 21:47:39.925961    1457 preload.go:236] getting checksum for preloaded-images-k8s-v18-v1.32.0-docker-overlay2-amd64.tar.lz4 ...
I0518 21:47:40.220846    1457 download.go:108] Downloading: https://storage.googleapis.com/minikube-preloaded-volume-tarballs/v18/v1.32.0/preloaded-images-k8s-v18-v1.32.0-docker-overlay2-amd64.tar.lz4?checksum=md5:4da2ed9bc13e09e8e9b7cf53d01335db -> /Users/christinamanara/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.32.0-docker-overlay2-amd64.tar.lz4
I0518 21:48:23.725247    1457 preload.go:247] saving checksum for preloaded-images-k8s-v18-v1.32.0-docker-overlay2-amd64.tar.lz4 ...
I0518 21:48:23.725643    1457 preload.go:254] verifying checksum of /Users/christinamanara/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.32.0-docker-overlay2-amd64.tar.lz4 ...
I0518 21:48:25.098843    1457 cache.go:59] Finished verifying existence of preloaded tar for v1.32.0 on docker
I0518 21:48:25.101748    1457 profile.go:143] Saving config to /Users/christinamanara/.minikube/profiles/minikube/config.json ...
I0518 21:48:25.101821    1457 lock.go:35] WriteFile acquiring /Users/christinamanara/.minikube/profiles/minikube/config.json: {Name:mk6df4819542228d280a7b758f45be40e2185ac3 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0518 21:48:25.104619    1457 start.go:360] acquireMachinesLock for minikube: {Name:mk932dd873c920391170ea64012b9397852eb6fe Clock:{} Delay:500ms Timeout:13m0s Cancel:<nil>}
I0518 21:48:25.105210    1457 start.go:364] duration metric: took 564.911Âµs to acquireMachinesLock for "minikube"
I0518 21:48:25.107758    1457 start.go:93] Provisioning new machine with config: &{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO:https://storage.googleapis.com/minikube/iso/minikube-v1.35.0-amd64.iso KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279 Memory:2200 CPUs:2 DiskSize:20000 Driver:hyperkit HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.32.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP: Port:8443 KubernetesVersion:v1.32.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/Users:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s} &{Name: IP: Port:8443 KubernetesVersion:v1.32.0 ContainerRuntime:docker ControlPlane:true Worker:true}
I0518 21:48:25.107964    1457 start.go:125] createHost starting for "" (driver="hyperkit")
I0518 21:48:25.133820    1457 out.go:235] ðŸ”¥  Creating hyperkit VM (CPUs=2, Memory=2200MB, Disk=20000MB) ...
I0518 21:48:25.137054    1457 main.go:141] libmachine: Found binary path at /Users/christinamanara/.minikube/bin/docker-machine-driver-hyperkit
I0518 21:48:25.137209    1457 main.go:141] libmachine: Launching plugin server for driver hyperkit
I0518 21:48:25.797798    1457 main.go:141] libmachine: Plugin server listening at address 127.0.0.1:49495
I0518 21:48:25.802105    1457 main.go:141] libmachine: () Calling .GetVersion
I0518 21:48:25.807851    1457 main.go:141] libmachine: Using API Version  1
I0518 21:48:25.807884    1457 main.go:141] libmachine: () Calling .SetConfigRaw
I0518 21:48:25.808430    1457 main.go:141] libmachine: () Calling .GetMachineName
I0518 21:48:25.808692    1457 main.go:141] libmachine: (minikube) Calling .GetMachineName
I0518 21:48:25.808879    1457 main.go:141] libmachine: (minikube) Calling .DriverName
I0518 21:48:25.809074    1457 start.go:159] libmachine.API.Create for "minikube" (driver="hyperkit")
I0518 21:48:25.809721    1457 client.go:168] LocalClient.Create starting
I0518 21:48:25.811424    1457 main.go:141] libmachine: Creating CA: /Users/christinamanara/.minikube/certs/ca.pem
I0518 21:48:26.227104    1457 main.go:141] libmachine: Creating client certificate: /Users/christinamanara/.minikube/certs/cert.pem
I0518 21:48:26.312453    1457 main.go:141] libmachine: Running pre-create checks...
I0518 21:48:26.312481    1457 main.go:141] libmachine: (minikube) Calling .PreCreateCheck
I0518 21:48:26.312781    1457 main.go:141] libmachine: (minikube) DBG | exe=/Users/christinamanara/.minikube/bin/docker-machine-driver-hyperkit uid=0
I0518 21:48:26.313630    1457 main.go:141] libmachine: (minikube) Calling .GetConfigRaw
I0518 21:48:26.314702    1457 main.go:141] libmachine: Creating machine...
I0518 21:48:26.314718    1457 main.go:141] libmachine: (minikube) Calling .Create
I0518 21:48:26.314959    1457 main.go:141] libmachine: (minikube) DBG | exe=/Users/christinamanara/.minikube/bin/docker-machine-driver-hyperkit uid=0
I0518 21:48:26.315162    1457 main.go:141] libmachine: (minikube) DBG | I0518 21:48:26.314938    1815 common.go:144] Making disk image using store path: /Users/christinamanara/.minikube
I0518 21:48:26.315296    1457 main.go:141] libmachine: (minikube) Downloading /Users/christinamanara/.minikube/cache/boot2docker.iso from file:///Users/christinamanara/.minikube/cache/iso/amd64/minikube-v1.35.0-amd64.iso...
I0518 21:48:27.859131    1457 main.go:141] libmachine: (minikube) DBG | I0518 21:48:27.858884    1815 common.go:151] Creating ssh key: /Users/christinamanara/.minikube/machines/minikube/id_rsa...
I0518 21:48:28.069849    1457 main.go:141] libmachine: (minikube) DBG | I0518 21:48:28.069676    1815 common.go:157] Creating raw disk image: /Users/christinamanara/.minikube/machines/minikube/minikube.rawdisk...
I0518 21:48:28.069973    1457 main.go:141] libmachine: (minikube) DBG | Writing magic tar header
I0518 21:48:28.070029    1457 main.go:141] libmachine: (minikube) DBG | Writing SSH key tar header
I0518 21:48:28.070710    1457 main.go:141] libmachine: (minikube) DBG | I0518 21:48:28.070647    1815 common.go:171] Fixing permissions on /Users/christinamanara/.minikube/machines/minikube ...
I0518 21:48:29.863130    1457 main.go:141] libmachine: (minikube) DBG | exe=/Users/christinamanara/.minikube/bin/docker-machine-driver-hyperkit uid=0
I0518 21:48:29.863563    1457 main.go:141] libmachine: (minikube) DBG | clean start, hyperkit pid file doesn't exist: /Users/christinamanara/.minikube/machines/minikube/hyperkit.pid
I0518 21:48:29.863640    1457 main.go:141] libmachine: (minikube) DBG | Using UUID 73343776-b042-4ed2-8665-9b3f7d008e6a
I0518 21:48:31.723895    1457 main.go:141] libmachine: (minikube) DBG | Generated MAC 5e:55:5e:43:7c:49
I0518 21:48:31.723964    1457 main.go:141] libmachine: (minikube) DBG | Starting with cmdline: loglevel=3 console=ttyS0 console=tty0 noembed nomodeset norestore waitusb=10 systemd.legacy_systemd_cgroup_controller=yes random.trust_cpu=on hw_rng_model=virtio base host=minikube
I0518 21:48:31.724356    1457 main.go:141] libmachine: (minikube) DBG | 2025/05/18 21:48:31 DEBUG: hyperkit: Start &hyperkit.HyperKit{HyperKit:"/usr/local/bin/hyperkit", Argv0:"", StateDir:"/Users/christinamanara/.minikube/machines/minikube", VPNKitSock:"", VPNKitUUID:"", VPNKitPreferredIPv4:"", UUID:"73343776-b042-4ed2-8665-9b3f7d008e6a", Disks:[]hyperkit.Disk{(*hyperkit.RawDisk)(0xc00022a5d0)}, ISOImages:[]string{"/Users/christinamanara/.minikube/machines/minikube/boot2docker.iso"}, VSock:false, VSockDir:"", VSockPorts:[]int(nil), VSockGuestCID:3, VMNet:true, Sockets9P:[]hyperkit.Socket9P(nil), Kernel:"/Users/christinamanara/.minikube/machines/minikube/bzimage", Initrd:"/Users/christinamanara/.minikube/machines/minikube/initrd", Bootrom:"", CPUs:2, Memory:2200, Console:1, Serials:[]hyperkit.Serial(nil), Pid:0, Arguments:[]string(nil), CmdLine:"", process:(*os.Process)(nil)}
I0518 21:48:31.724421    1457 main.go:141] libmachine: (minikube) DBG | 2025/05/18 21:48:31 DEBUG: hyperkit: check &hyperkit.HyperKit{HyperKit:"/usr/local/bin/hyperkit", Argv0:"", StateDir:"/Users/christinamanara/.minikube/machines/minikube", VPNKitSock:"", VPNKitUUID:"", VPNKitPreferredIPv4:"", UUID:"73343776-b042-4ed2-8665-9b3f7d008e6a", Disks:[]hyperkit.Disk{(*hyperkit.RawDisk)(0xc00022a5d0)}, ISOImages:[]string{"/Users/christinamanara/.minikube/machines/minikube/boot2docker.iso"}, VSock:false, VSockDir:"", VSockPorts:[]int(nil), VSockGuestCID:3, VMNet:true, Sockets9P:[]hyperkit.Socket9P(nil), Kernel:"/Users/christinamanara/.minikube/machines/minikube/bzimage", Initrd:"/Users/christinamanara/.minikube/machines/minikube/initrd", Bootrom:"", CPUs:2, Memory:2200, Console:1, Serials:[]hyperkit.Serial(nil), Pid:0, Arguments:[]string(nil), CmdLine:"", process:(*os.Process)(nil)}
I0518 21:48:31.724537    1457 main.go:141] libmachine: (minikube) DBG | 2025/05/18 21:48:31 DEBUG: hyperkit: Arguments: []string{"-A", "-u", "-F", "/Users/christinamanara/.minikube/machines/minikube/hyperkit.pid", "-c", "2", "-m", "2200M", "-s", "0:0,hostbridge", "-s", "31,lpc", "-s", "1:0,virtio-net", "-U", "73343776-b042-4ed2-8665-9b3f7d008e6a", "-s", "2:0,virtio-blk,/Users/christinamanara/.minikube/machines/minikube/minikube.rawdisk", "-s", "3,ahci-cd,/Users/christinamanara/.minikube/machines/minikube/boot2docker.iso", "-s", "4,virtio-rnd", "-l", "com1,autopty=/Users/christinamanara/.minikube/machines/minikube/tty,log=/Users/christinamanara/.minikube/machines/minikube/console-ring", "-f", "kexec,/Users/christinamanara/.minikube/machines/minikube/bzimage,/Users/christinamanara/.minikube/machines/minikube/initrd,earlyprintk=serial loglevel=3 console=ttyS0 console=tty0 noembed nomodeset norestore waitusb=10 systemd.legacy_systemd_cgroup_controller=yes random.trust_cpu=on hw_rng_model=virtio base host=minikube"}
I0518 21:48:31.724594    1457 main.go:141] libmachine: (minikube) DBG | 2025/05/18 21:48:31 DEBUG: hyperkit: CmdLine: "/usr/local/bin/hyperkit -A -u -F /Users/christinamanara/.minikube/machines/minikube/hyperkit.pid -c 2 -m 2200M -s 0:0,hostbridge -s 31,lpc -s 1:0,virtio-net -U 73343776-b042-4ed2-8665-9b3f7d008e6a -s 2:0,virtio-blk,/Users/christinamanara/.minikube/machines/minikube/minikube.rawdisk -s 3,ahci-cd,/Users/christinamanara/.minikube/machines/minikube/boot2docker.iso -s 4,virtio-rnd -l com1,autopty=/Users/christinamanara/.minikube/machines/minikube/tty,log=/Users/christinamanara/.minikube/machines/minikube/console-ring -f kexec,/Users/christinamanara/.minikube/machines/minikube/bzimage,/Users/christinamanara/.minikube/machines/minikube/initrd,earlyprintk=serial loglevel=3 console=ttyS0 console=tty0 noembed nomodeset norestore waitusb=10 systemd.legacy_systemd_cgroup_controller=yes random.trust_cpu=on hw_rng_model=virtio base host=minikube"
I0518 21:48:31.724629    1457 main.go:141] libmachine: (minikube) DBG | 2025/05/18 21:48:31 DEBUG: hyperkit: Redirecting stdout/stderr to logger
I0518 21:48:31.733402    1457 main.go:141] libmachine: (minikube) DBG | 2025/05/18 21:48:31 DEBUG: hyperkit: Pid is 1838
I0518 21:48:31.738751    1457 main.go:141] libmachine: (minikube) DBG | Attempt 0
I0518 21:48:31.738784    1457 main.go:141] libmachine: (minikube) DBG | exe=/Users/christinamanara/.minikube/bin/docker-machine-driver-hyperkit uid=0
I0518 21:48:31.738835    1457 main.go:141] libmachine: (minikube) DBG | hyperkit pid from json: 1838
I0518 21:48:31.743696    1457 main.go:141] libmachine: (minikube) DBG | Searching for 5e:55:5e:43:7c:49 in /var/db/dhcpd_leases ...
I0518 21:48:31.744298    1457 main.go:141] libmachine: (minikube) DBG | Found 3 entries in /var/db/dhcpd_leases!
I0518 21:48:31.744361    1457 main.go:141] libmachine: (minikube) DBG | dhcp entry: {Name:minikube IPAddress:192.168.65.2 HWAddress:be:86:b0:56:bb:5c ID:1,be:86:b0:56:bb:5c Lease:0x682a3453}
I0518 21:48:31.745120    1457 main.go:141] libmachine: (minikube) DBG | dhcp entry: {Name: IPAddress:192.168.64.2 HWAddress:2e:99:75:27:6a:47 ID:1,2e:99:75:27:6a:47 Lease:0x682a28d7}
I0518 21:48:31.745136    1457 main.go:141] libmachine: (minikube) DBG | dhcp entry: {Name:minikube IPAddress:192.168.64.3 HWAddress:be:86:b0:56:bb:5c ID:1,be:86:b0:56:bb:5c Lease:0x68111e84}
I0518 21:48:31.804945    1457 main.go:141] libmachine: (minikube) DBG | 2025/05/18 21:48:31 INFO : hyperkit: stderr: Using fd 5 for I/O notifications
I0518 21:48:31.944904    1457 main.go:141] libmachine: (minikube) DBG | 2025/05/18 21:48:31 INFO : hyperkit: stderr: /Users/christinamanara/.minikube/machines/minikube/boot2docker.iso: fcntl(F_PUNCHHOLE) Operation not permitted: block device will not support TRIM/DISCARD
I0518 21:48:31.946508    1457 main.go:141] libmachine: (minikube) DBG | 2025/05/18 21:48:31 INFO : hyperkit: stderr: vmx_set_ctlreg: cap_field: 2 bit: 22 unspecified don't care: bit is 0
I0518 21:48:31.946564    1457 main.go:141] libmachine: (minikube) DBG | 2025/05/18 21:48:31 INFO : hyperkit: stderr: vmx_set_ctlreg: cap_field: 4 bit: 12 unspecified don't care: bit is 0
I0518 21:48:31.946580    1457 main.go:141] libmachine: (minikube) DBG | 2025/05/18 21:48:31 INFO : hyperkit: stderr: vmx_set_ctlreg: cap_field: 4 bit: 20 unspecified don't care: bit is 0
I0518 21:48:31.946604    1457 main.go:141] libmachine: (minikube) DBG | 2025/05/18 21:48:31 INFO : hyperkit: stderr: vmx_set_ctlreg: cap_field: 3 bit: 13 unspecified don't care: bit is 0
I0518 21:48:32.992005    1457 main.go:141] libmachine: (minikube) DBG | 2025/05/18 21:48:32 INFO : hyperkit: stderr: rdmsr to register 0x3a on vcpu 0
I0518 21:48:32.992022    1457 main.go:141] libmachine: (minikube) DBG | 2025/05/18 21:48:32 INFO : hyperkit: stderr: rdmsr to register 0x140 on vcpu 0
I0518 21:48:33.027401    1457 main.go:141] libmachine: (minikube) DBG | 2025/05/18 21:48:33 INFO : hyperkit: stderr: vmx_set_ctlreg: cap_field: 2 bit: 22 unspecified don't care: bit is 0
I0518 21:48:33.027426    1457 main.go:141] libmachine: (minikube) DBG | 2025/05/18 21:48:33 INFO : hyperkit: stderr: vmx_set_ctlreg: cap_field: 4 bit: 12 unspecified don't care: bit is 0
I0518 21:48:33.027444    1457 main.go:141] libmachine: (minikube) DBG | 2025/05/18 21:48:33 INFO : hyperkit: stderr: vmx_set_ctlreg: cap_field: 4 bit: 20 unspecified don't care: bit is 0
I0518 21:48:33.027455    1457 main.go:141] libmachine: (minikube) DBG | 2025/05/18 21:48:33 INFO : hyperkit: stderr: vmx_set_ctlreg: cap_field: 3 bit: 13 unspecified don't care: bit is 0
I0518 21:48:33.029682    1457 main.go:141] libmachine: (minikube) DBG | 2025/05/18 21:48:33 INFO : hyperkit: stderr: rdmsr to register 0x3a on vcpu 1
I0518 21:48:33.029707    1457 main.go:141] libmachine: (minikube) DBG | 2025/05/18 21:48:33 INFO : hyperkit: stderr: rdmsr to register 0x140 on vcpu 1
I0518 21:48:33.746563    1457 main.go:141] libmachine: (minikube) DBG | Attempt 1
I0518 21:48:33.746579    1457 main.go:141] libmachine: (minikube) DBG | exe=/Users/christinamanara/.minikube/bin/docker-machine-driver-hyperkit uid=0
I0518 21:48:33.746686    1457 main.go:141] libmachine: (minikube) DBG | hyperkit pid from json: 1838
I0518 21:48:33.752363    1457 main.go:141] libmachine: (minikube) DBG | Searching for 5e:55:5e:43:7c:49 in /var/db/dhcpd_leases ...
I0518 21:48:33.752572    1457 main.go:141] libmachine: (minikube) DBG | Found 3 entries in /var/db/dhcpd_leases!
I0518 21:48:33.752592    1457 main.go:141] libmachine: (minikube) DBG | dhcp entry: {Name:minikube IPAddress:192.168.65.2 HWAddress:be:86:b0:56:bb:5c ID:1,be:86:b0:56:bb:5c Lease:0x682a3453}
I0518 21:48:33.752947    1457 main.go:141] libmachine: (minikube) DBG | dhcp entry: {Name: IPAddress:192.168.64.2 HWAddress:2e:99:75:27:6a:47 ID:1,2e:99:75:27:6a:47 Lease:0x682a28d7}
I0518 21:48:33.752985    1457 main.go:141] libmachine: (minikube) DBG | dhcp entry: {Name:minikube IPAddress:192.168.64.3 HWAddress:be:86:b0:56:bb:5c ID:1,be:86:b0:56:bb:5c Lease:0x68111e84}
I0518 21:48:35.753754    1457 main.go:141] libmachine: (minikube) DBG | Attempt 2
I0518 21:48:35.753805    1457 main.go:141] libmachine: (minikube) DBG | exe=/Users/christinamanara/.minikube/bin/docker-machine-driver-hyperkit uid=0
I0518 21:48:35.753874    1457 main.go:141] libmachine: (minikube) DBG | hyperkit pid from json: 1838
I0518 21:48:35.757111    1457 main.go:141] libmachine: (minikube) DBG | Searching for 5e:55:5e:43:7c:49 in /var/db/dhcpd_leases ...
I0518 21:48:35.757129    1457 main.go:141] libmachine: (minikube) DBG | Found 3 entries in /var/db/dhcpd_leases!
I0518 21:48:35.757152    1457 main.go:141] libmachine: (minikube) DBG | dhcp entry: {Name:minikube IPAddress:192.168.65.2 HWAddress:be:86:b0:56:bb:5c ID:1,be:86:b0:56:bb:5c Lease:0x682a3453}
I0518 21:48:35.757168    1457 main.go:141] libmachine: (minikube) DBG | dhcp entry: {Name: IPAddress:192.168.64.2 HWAddress:2e:99:75:27:6a:47 ID:1,2e:99:75:27:6a:47 Lease:0x682a28d7}
I0518 21:48:35.757204    1457 main.go:141] libmachine: (minikube) DBG | dhcp entry: {Name:minikube IPAddress:192.168.64.3 HWAddress:be:86:b0:56:bb:5c ID:1,be:86:b0:56:bb:5c Lease:0x68111e84}
I0518 21:48:37.757053    1457 main.go:141] libmachine: (minikube) DBG | Attempt 3
I0518 21:48:37.757074    1457 main.go:141] libmachine: (minikube) DBG | exe=/Users/christinamanara/.minikube/bin/docker-machine-driver-hyperkit uid=0
I0518 21:48:37.757429    1457 main.go:141] libmachine: (minikube) DBG | hyperkit pid from json: 1838
I0518 21:48:37.762056    1457 main.go:141] libmachine: (minikube) DBG | Searching for 5e:55:5e:43:7c:49 in /var/db/dhcpd_leases ...
I0518 21:48:37.762253    1457 main.go:141] libmachine: (minikube) DBG | Found 3 entries in /var/db/dhcpd_leases!
I0518 21:48:37.762486    1457 main.go:141] libmachine: (minikube) DBG | dhcp entry: {Name:minikube IPAddress:192.168.65.2 HWAddress:be:86:b0:56:bb:5c ID:1,be:86:b0:56:bb:5c Lease:0x682a3453}
I0518 21:48:37.762511    1457 main.go:141] libmachine: (minikube) DBG | dhcp entry: {Name: IPAddress:192.168.64.2 HWAddress:2e:99:75:27:6a:47 ID:1,2e:99:75:27:6a:47 Lease:0x682a28d7}
I0518 21:48:37.762534    1457 main.go:141] libmachine: (minikube) DBG | dhcp entry: {Name:minikube IPAddress:192.168.64.3 HWAddress:be:86:b0:56:bb:5c ID:1,be:86:b0:56:bb:5c Lease:0x68111e84}
I0518 21:48:39.762708    1457 main.go:141] libmachine: (minikube) DBG | Attempt 4
I0518 21:48:39.762765    1457 main.go:141] libmachine: (minikube) DBG | exe=/Users/christinamanara/.minikube/bin/docker-machine-driver-hyperkit uid=0
I0518 21:48:39.762921    1457 main.go:141] libmachine: (minikube) DBG | hyperkit pid from json: 1838
I0518 21:48:39.765308    1457 main.go:141] libmachine: (minikube) DBG | Searching for 5e:55:5e:43:7c:49 in /var/db/dhcpd_leases ...
I0518 21:48:39.765654    1457 main.go:141] libmachine: (minikube) DBG | Found 3 entries in /var/db/dhcpd_leases!
I0518 21:48:39.765675    1457 main.go:141] libmachine: (minikube) DBG | dhcp entry: {Name:minikube IPAddress:192.168.65.2 HWAddress:be:86:b0:56:bb:5c ID:1,be:86:b0:56:bb:5c Lease:0x682a3453}
I0518 21:48:39.765694    1457 main.go:141] libmachine: (minikube) DBG | dhcp entry: {Name: IPAddress:192.168.64.2 HWAddress:2e:99:75:27:6a:47 ID:1,2e:99:75:27:6a:47 Lease:0x682a28d7}
I0518 21:48:39.765707    1457 main.go:141] libmachine: (minikube) DBG | dhcp entry: {Name:minikube IPAddress:192.168.64.3 HWAddress:be:86:b0:56:bb:5c ID:1,be:86:b0:56:bb:5c Lease:0x68111e84}
I0518 21:48:41.766467    1457 main.go:141] libmachine: (minikube) DBG | Attempt 5
I0518 21:48:41.766548    1457 main.go:141] libmachine: (minikube) DBG | exe=/Users/christinamanara/.minikube/bin/docker-machine-driver-hyperkit uid=0
I0518 21:48:41.766667    1457 main.go:141] libmachine: (minikube) DBG | hyperkit pid from json: 1838
I0518 21:48:41.769743    1457 main.go:141] libmachine: (minikube) DBG | Searching for 5e:55:5e:43:7c:49 in /var/db/dhcpd_leases ...
I0518 21:48:41.770172    1457 main.go:141] libmachine: (minikube) DBG | Found 3 entries in /var/db/dhcpd_leases!
I0518 21:48:41.770191    1457 main.go:141] libmachine: (minikube) DBG | dhcp entry: {Name:minikube IPAddress:192.168.65.2 HWAddress:be:86:b0:56:bb:5c ID:1,be:86:b0:56:bb:5c Lease:0x682a3453}
I0518 21:48:41.770206    1457 main.go:141] libmachine: (minikube) DBG | dhcp entry: {Name: IPAddress:192.168.64.2 HWAddress:2e:99:75:27:6a:47 ID:1,2e:99:75:27:6a:47 Lease:0x682a28d7}
I0518 21:48:41.770216    1457 main.go:141] libmachine: (minikube) DBG | dhcp entry: {Name:minikube IPAddress:192.168.64.3 HWAddress:be:86:b0:56:bb:5c ID:1,be:86:b0:56:bb:5c Lease:0x68111e84}
I0518 21:48:43.770646    1457 main.go:141] libmachine: (minikube) DBG | Attempt 6
I0518 21:48:43.770689    1457 main.go:141] libmachine: (minikube) DBG | exe=/Users/christinamanara/.minikube/bin/docker-machine-driver-hyperkit uid=0
I0518 21:48:43.770966    1457 main.go:141] libmachine: (minikube) DBG | hyperkit pid from json: 1838
I0518 21:48:43.774208    1457 main.go:141] libmachine: (minikube) DBG | Searching for 5e:55:5e:43:7c:49 in /var/db/dhcpd_leases ...
I0518 21:48:43.774730    1457 main.go:141] libmachine: (minikube) DBG | Found 3 entries in /var/db/dhcpd_leases!
I0518 21:48:43.774752    1457 main.go:141] libmachine: (minikube) DBG | dhcp entry: {Name:minikube IPAddress:192.168.65.2 HWAddress:be:86:b0:56:bb:5c ID:1,be:86:b0:56:bb:5c Lease:0x682a3453}
I0518 21:48:43.774788    1457 main.go:141] libmachine: (minikube) DBG | dhcp entry: {Name: IPAddress:192.168.64.2 HWAddress:2e:99:75:27:6a:47 ID:1,2e:99:75:27:6a:47 Lease:0x682a28d7}
I0518 21:48:43.774809    1457 main.go:141] libmachine: (minikube) DBG | dhcp entry: {Name:minikube IPAddress:192.168.64.3 HWAddress:be:86:b0:56:bb:5c ID:1,be:86:b0:56:bb:5c Lease:0x68111e84}
I0518 21:48:45.775150    1457 main.go:141] libmachine: (minikube) DBG | Attempt 7
I0518 21:48:45.775217    1457 main.go:141] libmachine: (minikube) DBG | exe=/Users/christinamanara/.minikube/bin/docker-machine-driver-hyperkit uid=0
I0518 21:48:45.775453    1457 main.go:141] libmachine: (minikube) DBG | hyperkit pid from json: 1838
I0518 21:48:45.778321    1457 main.go:141] libmachine: (minikube) DBG | Searching for 5e:55:5e:43:7c:49 in /var/db/dhcpd_leases ...
I0518 21:48:45.778582    1457 main.go:141] libmachine: (minikube) DBG | Found 3 entries in /var/db/dhcpd_leases!
I0518 21:48:45.778607    1457 main.go:141] libmachine: (minikube) DBG | dhcp entry: {Name:minikube IPAddress:192.168.65.2 HWAddress:be:86:b0:56:bb:5c ID:1,be:86:b0:56:bb:5c Lease:0x682a3453}
I0518 21:48:45.778628    1457 main.go:141] libmachine: (minikube) DBG | dhcp entry: {Name: IPAddress:192.168.64.2 HWAddress:2e:99:75:27:6a:47 ID:1,2e:99:75:27:6a:47 Lease:0x682a28d7}
I0518 21:48:45.778646    1457 main.go:141] libmachine: (minikube) DBG | dhcp entry: {Name:minikube IPAddress:192.168.64.3 HWAddress:be:86:b0:56:bb:5c ID:1,be:86:b0:56:bb:5c Lease:0x68111e84}
I0518 21:48:47.780175    1457 main.go:141] libmachine: (minikube) DBG | Attempt 8
I0518 21:48:47.780218    1457 main.go:141] libmachine: (minikube) DBG | exe=/Users/christinamanara/.minikube/bin/docker-machine-driver-hyperkit uid=0
I0518 21:48:47.780241    1457 main.go:141] libmachine: (minikube) DBG | hyperkit pid from json: 1838
I0518 21:48:47.784341    1457 main.go:141] libmachine: (minikube) DBG | Searching for 5e:55:5e:43:7c:49 in /var/db/dhcpd_leases ...
I0518 21:48:47.784690    1457 main.go:141] libmachine: (minikube) DBG | Found 3 entries in /var/db/dhcpd_leases!
I0518 21:48:47.785007    1457 main.go:141] libmachine: (minikube) DBG | dhcp entry: {Name:minikube IPAddress:192.168.65.2 HWAddress:be:86:b0:56:bb:5c ID:1,be:86:b0:56:bb:5c Lease:0x682a3453}
I0518 21:48:47.785034    1457 main.go:141] libmachine: (minikube) DBG | dhcp entry: {Name: IPAddress:192.168.64.2 HWAddress:2e:99:75:27:6a:47 ID:1,2e:99:75:27:6a:47 Lease:0x682a28d7}
I0518 21:48:47.785070    1457 main.go:141] libmachine: (minikube) DBG | dhcp entry: {Name:minikube IPAddress:192.168.64.3 HWAddress:be:86:b0:56:bb:5c ID:1,be:86:b0:56:bb:5c Lease:0x68111e84}
I0518 21:48:49.785135    1457 main.go:141] libmachine: (minikube) DBG | Attempt 9
I0518 21:48:49.785168    1457 main.go:141] libmachine: (minikube) DBG | exe=/Users/christinamanara/.minikube/bin/docker-machine-driver-hyperkit uid=0
I0518 21:48:49.785304    1457 main.go:141] libmachine: (minikube) DBG | hyperkit pid from json: 1838
I0518 21:48:49.788719    1457 main.go:141] libmachine: (minikube) DBG | Searching for 5e:55:5e:43:7c:49 in /var/db/dhcpd_leases ...
I0518 21:48:49.788995    1457 main.go:141] libmachine: (minikube) DBG | Found 3 entries in /var/db/dhcpd_leases!
I0518 21:48:49.789015    1457 main.go:141] libmachine: (minikube) DBG | dhcp entry: {Name:minikube IPAddress:192.168.65.2 HWAddress:be:86:b0:56:bb:5c ID:1,be:86:b0:56:bb:5c Lease:0x682a3453}
I0518 21:48:49.789033    1457 main.go:141] libmachine: (minikube) DBG | dhcp entry: {Name: IPAddress:192.168.64.2 HWAddress:2e:99:75:27:6a:47 ID:1,2e:99:75:27:6a:47 Lease:0x682a28d7}
I0518 21:48:49.789045    1457 main.go:141] libmachine: (minikube) DBG | dhcp entry: {Name:minikube IPAddress:192.168.64.3 HWAddress:be:86:b0:56:bb:5c ID:1,be:86:b0:56:bb:5c Lease:0x68111e84}
I0518 21:48:51.790606    1457 main.go:141] libmachine: (minikube) DBG | Attempt 10
I0518 21:48:51.790664    1457 main.go:141] libmachine: (minikube) DBG | exe=/Users/christinamanara/.minikube/bin/docker-machine-driver-hyperkit uid=0
I0518 21:48:51.790717    1457 main.go:141] libmachine: (minikube) DBG | hyperkit pid from json: 1838
I0518 21:48:51.794895    1457 main.go:141] libmachine: (minikube) DBG | Searching for 5e:55:5e:43:7c:49 in /var/db/dhcpd_leases ...
I0518 21:48:51.795267    1457 main.go:141] libmachine: (minikube) DBG | Found 3 entries in /var/db/dhcpd_leases!
I0518 21:48:51.795285    1457 main.go:141] libmachine: (minikube) DBG | dhcp entry: {Name:minikube IPAddress:192.168.65.2 HWAddress:be:86:b0:56:bb:5c ID:1,be:86:b0:56:bb:5c Lease:0x682a3453}
I0518 21:48:51.795337    1457 main.go:141] libmachine: (minikube) DBG | dhcp entry: {Name: IPAddress:192.168.64.2 HWAddress:2e:99:75:27:6a:47 ID:1,2e:99:75:27:6a:47 Lease:0x682a28d7}
I0518 21:48:51.795358    1457 main.go:141] libmachine: (minikube) DBG | dhcp entry: {Name:minikube IPAddress:192.168.64.3 HWAddress:be:86:b0:56:bb:5c ID:1,be:86:b0:56:bb:5c Lease:0x68111e84}
I0518 21:48:52.131962    1457 main.go:141] libmachine: (minikube) DBG | 2025/05/18 21:48:52 INFO : hyperkit: stderr: rdmsr to register 0x64d on vcpu 0
I0518 21:48:52.132398    1457 main.go:141] libmachine: (minikube) DBG | 2025/05/18 21:48:52 INFO : hyperkit: stderr: rdmsr to register 0x64e on vcpu 0
I0518 21:48:52.132435    1457 main.go:141] libmachine: (minikube) DBG | 2025/05/18 21:48:52 INFO : hyperkit: stderr: rdmsr to register 0x34 on vcpu 0
I0518 21:48:52.246101    1457 main.go:141] libmachine: (minikube) DBG | 2025/05/18 21:48:52 INFO : hyperkit: stderr: rdmsr to register 0xc0011029 on vcpu 0
I0518 21:48:53.795669    1457 main.go:141] libmachine: (minikube) DBG | Attempt 11
I0518 21:48:53.795693    1457 main.go:141] libmachine: (minikube) DBG | exe=/Users/christinamanara/.minikube/bin/docker-machine-driver-hyperkit uid=0
I0518 21:48:53.795904    1457 main.go:141] libmachine: (minikube) DBG | hyperkit pid from json: 1838
I0518 21:48:53.801966    1457 main.go:141] libmachine: (minikube) DBG | Searching for 5e:55:5e:43:7c:49 in /var/db/dhcpd_leases ...
I0518 21:48:53.802100    1457 main.go:141] libmachine: (minikube) DBG | Found 3 entries in /var/db/dhcpd_leases!
I0518 21:48:53.802115    1457 main.go:141] libmachine: (minikube) DBG | dhcp entry: {Name:minikube IPAddress:192.168.65.2 HWAddress:be:86:b0:56:bb:5c ID:1,be:86:b0:56:bb:5c Lease:0x682a3453}
I0518 21:48:53.802145    1457 main.go:141] libmachine: (minikube) DBG | dhcp entry: {Name: IPAddress:192.168.64.2 HWAddress:2e:99:75:27:6a:47 ID:1,2e:99:75:27:6a:47 Lease:0x682a28d7}
I0518 21:48:53.802152    1457 main.go:141] libmachine: (minikube) DBG | dhcp entry: {Name:minikube IPAddress:192.168.64.3 HWAddress:be:86:b0:56:bb:5c ID:1,be:86:b0:56:bb:5c Lease:0x68111e84}
I0518 21:48:55.802831    1457 main.go:141] libmachine: (minikube) DBG | Attempt 12
I0518 21:48:55.802866    1457 main.go:141] libmachine: (minikube) DBG | exe=/Users/christinamanara/.minikube/bin/docker-machine-driver-hyperkit uid=0
I0518 21:48:55.803075    1457 main.go:141] libmachine: (minikube) DBG | hyperkit pid from json: 1838
I0518 21:48:55.808188    1457 main.go:141] libmachine: (minikube) DBG | Searching for 5e:55:5e:43:7c:49 in /var/db/dhcpd_leases ...
I0518 21:48:55.808499    1457 main.go:141] libmachine: (minikube) DBG | Found 3 entries in /var/db/dhcpd_leases!
I0518 21:48:55.808521    1457 main.go:141] libmachine: (minikube) DBG | dhcp entry: {Name:minikube IPAddress:192.168.65.2 HWAddress:be:86:b0:56:bb:5c ID:1,be:86:b0:56:bb:5c Lease:0x682a3453}
I0518 21:48:55.808540    1457 main.go:141] libmachine: (minikube) DBG | dhcp entry: {Name: IPAddress:192.168.64.2 HWAddress:2e:99:75:27:6a:47 ID:1,2e:99:75:27:6a:47 Lease:0x682a28d7}
I0518 21:48:55.808552    1457 main.go:141] libmachine: (minikube) DBG | dhcp entry: {Name:minikube IPAddress:192.168.64.3 HWAddress:be:86:b0:56:bb:5c ID:1,be:86:b0:56:bb:5c Lease:0x68111e84}
I0518 21:48:57.808876    1457 main.go:141] libmachine: (minikube) DBG | Attempt 13
I0518 21:48:57.808965    1457 main.go:141] libmachine: (minikube) DBG | exe=/Users/christinamanara/.minikube/bin/docker-machine-driver-hyperkit uid=0
I0518 21:48:57.809111    1457 main.go:141] libmachine: (minikube) DBG | hyperkit pid from json: 1838
I0518 21:48:57.814469    1457 main.go:141] libmachine: (minikube) DBG | Searching for 5e:55:5e:43:7c:49 in /var/db/dhcpd_leases ...
I0518 21:48:57.814520    1457 main.go:141] libmachine: (minikube) DBG | Found 3 entries in /var/db/dhcpd_leases!
I0518 21:48:57.814562    1457 main.go:141] libmachine: (minikube) DBG | dhcp entry: {Name:minikube IPAddress:192.168.65.2 HWAddress:be:86:b0:56:bb:5c ID:1,be:86:b0:56:bb:5c Lease:0x682a3453}
I0518 21:48:57.814623    1457 main.go:141] libmachine: (minikube) DBG | dhcp entry: {Name: IPAddress:192.168.64.2 HWAddress:2e:99:75:27:6a:47 ID:1,2e:99:75:27:6a:47 Lease:0x682a28d7}
I0518 21:48:57.814644    1457 main.go:141] libmachine: (minikube) DBG | dhcp entry: {Name:minikube IPAddress:192.168.64.3 HWAddress:be:86:b0:56:bb:5c ID:1,be:86:b0:56:bb:5c Lease:0x68111e84}
I0518 21:48:59.814954    1457 main.go:141] libmachine: (minikube) DBG | Attempt 14
I0518 21:48:59.814980    1457 main.go:141] libmachine: (minikube) DBG | exe=/Users/christinamanara/.minikube/bin/docker-machine-driver-hyperkit uid=0
I0518 21:48:59.815064    1457 main.go:141] libmachine: (minikube) DBG | hyperkit pid from json: 1838
I0518 21:48:59.821476    1457 main.go:141] libmachine: (minikube) DBG | Searching for 5e:55:5e:43:7c:49 in /var/db/dhcpd_leases ...
I0518 21:48:59.821972    1457 main.go:141] libmachine: (minikube) DBG | Found 3 entries in /var/db/dhcpd_leases!
I0518 21:48:59.821997    1457 main.go:141] libmachine: (minikube) DBG | dhcp entry: {Name:minikube IPAddress:192.168.65.2 HWAddress:be:86:b0:56:bb:5c ID:1,be:86:b0:56:bb:5c Lease:0x682a3453}
I0518 21:48:59.822105    1457 main.go:141] libmachine: (minikube) DBG | dhcp entry: {Name: IPAddress:192.168.64.2 HWAddress:2e:99:75:27:6a:47 ID:1,2e:99:75:27:6a:47 Lease:0x682a28d7}
I0518 21:48:59.822129    1457 main.go:141] libmachine: (minikube) DBG | dhcp entry: {Name:minikube IPAddress:192.168.64.3 HWAddress:be:86:b0:56:bb:5c ID:1,be:86:b0:56:bb:5c Lease:0x68111e84}
I0518 21:49:01.828625    1457 main.go:141] libmachine: (minikube) DBG | Attempt 15
I0518 21:49:01.828706    1457 main.go:141] libmachine: (minikube) DBG | exe=/Users/christinamanara/.minikube/bin/docker-machine-driver-hyperkit uid=0
I0518 21:49:01.828733    1457 main.go:141] libmachine: (minikube) DBG | hyperkit pid from json: 1838
I0518 21:49:01.850222    1457 main.go:141] libmachine: (minikube) DBG | Searching for 5e:55:5e:43:7c:49 in /var/db/dhcpd_leases ...
I0518 21:49:01.850239    1457 main.go:141] libmachine: (minikube) DBG | Found 3 entries in /var/db/dhcpd_leases!
I0518 21:49:01.851112    1457 main.go:141] libmachine: (minikube) DBG | dhcp entry: {Name:minikube IPAddress:192.168.65.2 HWAddress:be:86:b0:56:bb:5c ID:1,be:86:b0:56:bb:5c Lease:0x682a3453}
I0518 21:49:01.851141    1457 main.go:141] libmachine: (minikube) DBG | dhcp entry: {Name: IPAddress:192.168.64.2 HWAddress:2e:99:75:27:6a:47 ID:1,2e:99:75:27:6a:47 Lease:0x682a28d7}
I0518 21:49:01.854306    1457 main.go:141] libmachine: (minikube) DBG | dhcp entry: {Name:minikube IPAddress:192.168.64.3 HWAddress:be:86:b0:56:bb:5c ID:1,be:86:b0:56:bb:5c Lease:0x68111e84}
I0518 21:49:03.844445    1457 main.go:141] libmachine: (minikube) DBG | Attempt 16
I0518 21:49:03.844570    1457 main.go:141] libmachine: (minikube) DBG | exe=/Users/christinamanara/.minikube/bin/docker-machine-driver-hyperkit uid=0
I0518 21:49:03.845337    1457 main.go:141] libmachine: (minikube) DBG | hyperkit pid from json: 1838
I0518 21:49:03.851883    1457 main.go:141] libmachine: (minikube) DBG | Searching for 5e:55:5e:43:7c:49 in /var/db/dhcpd_leases ...
I0518 21:49:03.852089    1457 main.go:141] libmachine: (minikube) DBG | Found 4 entries in /var/db/dhcpd_leases!
I0518 21:49:03.852921    1457 main.go:141] libmachine: (minikube) DBG | dhcp entry: {Name:minikube IPAddress:192.168.65.3 HWAddress:5e:55:5e:43:7c:49 ID:1,5e:55:5e:43:7c:49 Lease:0x682a39af}
I0518 21:49:03.852938    1457 main.go:141] libmachine: (minikube) DBG | Found match: 5e:55:5e:43:7c:49
I0518 21:49:03.852956    1457 main.go:141] libmachine: (minikube) DBG | IP: 192.168.65.3
I0518 21:49:03.859165    1457 main.go:141] libmachine: (minikube) Calling .GetConfigRaw
I0518 21:49:03.874031    1457 main.go:141] libmachine: (minikube) Calling .DriverName
I0518 21:49:03.876648    1457 main.go:141] libmachine: (minikube) Calling .DriverName
I0518 21:49:03.877606    1457 main.go:141] libmachine: Waiting for machine to be running, this may take a few minutes...
I0518 21:49:03.878007    1457 main.go:141] libmachine: (minikube) Calling .GetState
I0518 21:49:03.878557    1457 main.go:141] libmachine: (minikube) DBG | exe=/Users/christinamanara/.minikube/bin/docker-machine-driver-hyperkit uid=0
I0518 21:49:03.878744    1457 main.go:141] libmachine: (minikube) DBG | hyperkit pid from json: 1838
I0518 21:49:03.886942    1457 main.go:141] libmachine: Detecting operating system of created instance...
I0518 21:49:03.888046    1457 main.go:141] libmachine: Waiting for SSH to be available...
I0518 21:49:03.888061    1457 main.go:141] libmachine: Getting to WaitForSSH function...
I0518 21:49:03.888072    1457 main.go:141] libmachine: (minikube) Calling .GetSSHHostname
I0518 21:49:03.888344    1457 main.go:141] libmachine: (minikube) Calling .GetSSHPort
I0518 21:49:03.888630    1457 main.go:141] libmachine: (minikube) Calling .GetSSHKeyPath
I0518 21:49:03.888867    1457 main.go:141] libmachine: (minikube) Calling .GetSSHKeyPath
I0518 21:49:03.889056    1457 main.go:141] libmachine: (minikube) Calling .GetSSHUsername
I0518 21:49:03.892629    1457 main.go:141] libmachine: Using SSH client type: native
I0518 21:49:03.905693    1457 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x104ddf6a0] 0x104de2380 <nil>  [] 0s} 192.168.65.3 22 <nil> <nil>}
I0518 21:49:03.905706    1457 main.go:141] libmachine: About to run SSH command:
exit 0
I0518 21:49:04.912005    1457 main.go:141] libmachine: Error dialing TCP: dial tcp 192.168.65.3:22: connect: connection refused
I0518 21:49:08.050627    1457 main.go:141] libmachine: SSH cmd err, output: <nil>: 
I0518 21:49:08.050642    1457 main.go:141] libmachine: Detecting the provisioner...
I0518 21:49:08.050660    1457 main.go:141] libmachine: (minikube) Calling .GetSSHHostname
I0518 21:49:08.050999    1457 main.go:141] libmachine: (minikube) Calling .GetSSHPort
I0518 21:49:08.051197    1457 main.go:141] libmachine: (minikube) Calling .GetSSHKeyPath
I0518 21:49:08.051370    1457 main.go:141] libmachine: (minikube) Calling .GetSSHKeyPath
I0518 21:49:08.051555    1457 main.go:141] libmachine: (minikube) Calling .GetSSHUsername
I0518 21:49:08.051934    1457 main.go:141] libmachine: Using SSH client type: native
I0518 21:49:08.053062    1457 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x104ddf6a0] 0x104de2380 <nil>  [] 0s} 192.168.65.3 22 <nil> <nil>}
I0518 21:49:08.053072    1457 main.go:141] libmachine: About to run SSH command:
cat /etc/os-release
I0518 21:49:08.172952    1457 main.go:141] libmachine: SSH cmd err, output: <nil>: NAME=Buildroot
VERSION=2023.02.9-dirty
ID=buildroot
VERSION_ID=2023.02.9
PRETTY_NAME="Buildroot 2023.02.9"

I0518 21:49:08.178221    1457 main.go:141] libmachine: found compatible host: buildroot
I0518 21:49:08.178260    1457 main.go:141] libmachine: Provisioning with buildroot...
I0518 21:49:08.178272    1457 main.go:141] libmachine: (minikube) Calling .GetMachineName
I0518 21:49:08.180180    1457 buildroot.go:166] provisioning hostname "minikube"
I0518 21:49:08.180219    1457 main.go:141] libmachine: (minikube) Calling .GetMachineName
I0518 21:49:08.180589    1457 main.go:141] libmachine: (minikube) Calling .GetSSHHostname
I0518 21:49:08.180866    1457 main.go:141] libmachine: (minikube) Calling .GetSSHPort
I0518 21:49:08.182059    1457 main.go:141] libmachine: (minikube) Calling .GetSSHKeyPath
I0518 21:49:08.182415    1457 main.go:141] libmachine: (minikube) Calling .GetSSHKeyPath
I0518 21:49:08.182615    1457 main.go:141] libmachine: (minikube) Calling .GetSSHUsername
I0518 21:49:08.182957    1457 main.go:141] libmachine: Using SSH client type: native
I0518 21:49:08.183307    1457 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x104ddf6a0] 0x104de2380 <nil>  [] 0s} 192.168.65.3 22 <nil> <nil>}
I0518 21:49:08.183320    1457 main.go:141] libmachine: About to run SSH command:
sudo hostname minikube && echo "minikube" | sudo tee /etc/hostname
I0518 21:49:08.361910    1457 main.go:141] libmachine: SSH cmd err, output: <nil>: minikube

I0518 21:49:08.363300    1457 main.go:141] libmachine: (minikube) Calling .GetSSHHostname
I0518 21:49:08.363730    1457 main.go:141] libmachine: (minikube) Calling .GetSSHPort
I0518 21:49:08.363956    1457 main.go:141] libmachine: (minikube) Calling .GetSSHKeyPath
I0518 21:49:08.364306    1457 main.go:141] libmachine: (minikube) Calling .GetSSHKeyPath
I0518 21:49:08.364522    1457 main.go:141] libmachine: (minikube) Calling .GetSSHUsername
I0518 21:49:08.364877    1457 main.go:141] libmachine: Using SSH client type: native
I0518 21:49:08.365867    1457 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x104ddf6a0] 0x104de2380 <nil>  [] 0s} 192.168.65.3 22 <nil> <nil>}
I0518 21:49:08.365885    1457 main.go:141] libmachine: About to run SSH command:

		if ! grep -xq '.*\sminikube' /etc/hosts; then
			if grep -xq '127.0.1.1\s.*' /etc/hosts; then
				sudo sed -i 's/^127.0.1.1\s.*/127.0.1.1 minikube/g' /etc/hosts;
			else 
				echo '127.0.1.1 minikube' | sudo tee -a /etc/hosts; 
			fi
		fi
I0518 21:49:08.513280    1457 main.go:141] libmachine: SSH cmd err, output: <nil>: 
I0518 21:49:08.513316    1457 buildroot.go:172] set auth options {CertDir:/Users/christinamanara/.minikube CaCertPath:/Users/christinamanara/.minikube/certs/ca.pem CaPrivateKeyPath:/Users/christinamanara/.minikube/certs/ca-key.pem CaCertRemotePath:/etc/docker/ca.pem ServerCertPath:/Users/christinamanara/.minikube/machines/server.pem ServerKeyPath:/Users/christinamanara/.minikube/machines/server-key.pem ClientKeyPath:/Users/christinamanara/.minikube/certs/key.pem ServerCertRemotePath:/etc/docker/server.pem ServerKeyRemotePath:/etc/docker/server-key.pem ClientCertPath:/Users/christinamanara/.minikube/certs/cert.pem ServerCertSANs:[] StorePath:/Users/christinamanara/.minikube}
I0518 21:49:08.513377    1457 buildroot.go:174] setting up certificates
I0518 21:49:08.514513    1457 provision.go:84] configureAuth start
I0518 21:49:08.514542    1457 main.go:141] libmachine: (minikube) Calling .GetMachineName
I0518 21:49:08.514958    1457 main.go:141] libmachine: (minikube) Calling .GetIP
I0518 21:49:08.515313    1457 main.go:141] libmachine: (minikube) Calling .GetSSHHostname
I0518 21:49:08.516130    1457 provision.go:143] copyHostCerts
I0518 21:49:08.520317    1457 exec_runner.go:151] cp: /Users/christinamanara/.minikube/certs/ca.pem --> /Users/christinamanara/.minikube/ca.pem (1103 bytes)
I0518 21:49:08.522143    1457 exec_runner.go:151] cp: /Users/christinamanara/.minikube/certs/cert.pem --> /Users/christinamanara/.minikube/cert.pem (1147 bytes)
I0518 21:49:08.524352    1457 exec_runner.go:151] cp: /Users/christinamanara/.minikube/certs/key.pem --> /Users/christinamanara/.minikube/key.pem (1675 bytes)
I0518 21:49:08.525349    1457 provision.go:117] generating server cert: /Users/christinamanara/.minikube/machines/server.pem ca-key=/Users/christinamanara/.minikube/certs/ca.pem private-key=/Users/christinamanara/.minikube/certs/ca-key.pem org=christinamanara.minikube san=[127.0.0.1 192.168.65.3 localhost minikube]
I0518 21:49:09.092174    1457 provision.go:177] copyRemoteCerts
I0518 21:49:09.093446    1457 ssh_runner.go:195] Run: sudo mkdir -p /etc/docker /etc/docker /etc/docker
I0518 21:49:09.093499    1457 main.go:141] libmachine: (minikube) Calling .GetSSHHostname
I0518 21:49:09.093784    1457 main.go:141] libmachine: (minikube) Calling .GetSSHPort
I0518 21:49:09.093972    1457 main.go:141] libmachine: (minikube) Calling .GetSSHKeyPath
I0518 21:49:09.094154    1457 main.go:141] libmachine: (minikube) Calling .GetSSHUsername
I0518 21:49:09.094720    1457 sshutil.go:53] new ssh client: &{IP:192.168.65.3 Port:22 SSHKeyPath:/Users/christinamanara/.minikube/machines/minikube/id_rsa Username:docker}
I0518 21:49:09.206034    1457 ssh_runner.go:362] scp /Users/christinamanara/.minikube/certs/ca.pem --> /etc/docker/ca.pem (1103 bytes)
I0518 21:49:09.316687    1457 ssh_runner.go:362] scp /Users/christinamanara/.minikube/machines/server.pem --> /etc/docker/server.pem (1204 bytes)
I0518 21:49:09.438288    1457 ssh_runner.go:362] scp /Users/christinamanara/.minikube/machines/server-key.pem --> /etc/docker/server-key.pem (1679 bytes)
I0518 21:49:09.572614    1457 provision.go:87] duration metric: took 1.058044541s to configureAuth
I0518 21:49:09.572744    1457 buildroot.go:189] setting minikube options for container-runtime
I0518 21:49:09.575730    1457 config.go:182] Loaded profile config "minikube": Driver=hyperkit, ContainerRuntime=docker, KubernetesVersion=v1.32.0
I0518 21:49:09.575758    1457 main.go:141] libmachine: (minikube) Calling .DriverName
I0518 21:49:09.576212    1457 main.go:141] libmachine: (minikube) Calling .GetSSHHostname
I0518 21:49:09.576448    1457 main.go:141] libmachine: (minikube) Calling .GetSSHPort
I0518 21:49:09.576725    1457 main.go:141] libmachine: (minikube) Calling .GetSSHKeyPath
I0518 21:49:09.576989    1457 main.go:141] libmachine: (minikube) Calling .GetSSHKeyPath
I0518 21:49:09.577280    1457 main.go:141] libmachine: (minikube) Calling .GetSSHUsername
I0518 21:49:09.578368    1457 main.go:141] libmachine: Using SSH client type: native
I0518 21:49:09.578930    1457 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x104ddf6a0] 0x104de2380 <nil>  [] 0s} 192.168.65.3 22 <nil> <nil>}
I0518 21:49:09.578944    1457 main.go:141] libmachine: About to run SSH command:
df --output=fstype / | tail -n 1
I0518 21:49:09.711528    1457 main.go:141] libmachine: SSH cmd err, output: <nil>: tmpfs

I0518 21:49:09.711543    1457 buildroot.go:70] root file system type: tmpfs
I0518 21:49:09.713247    1457 provision.go:314] Updating docker unit: /lib/systemd/system/docker.service ...
I0518 21:49:09.713296    1457 main.go:141] libmachine: (minikube) Calling .GetSSHHostname
I0518 21:49:09.713594    1457 main.go:141] libmachine: (minikube) Calling .GetSSHPort
I0518 21:49:09.713770    1457 main.go:141] libmachine: (minikube) Calling .GetSSHKeyPath
I0518 21:49:09.713933    1457 main.go:141] libmachine: (minikube) Calling .GetSSHKeyPath
I0518 21:49:09.714106    1457 main.go:141] libmachine: (minikube) Calling .GetSSHUsername
I0518 21:49:09.714508    1457 main.go:141] libmachine: Using SSH client type: native
I0518 21:49:09.714749    1457 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x104ddf6a0] 0x104de2380 <nil>  [] 0s} 192.168.65.3 22 <nil> <nil>}
I0518 21:49:09.714825    1457 main.go:141] libmachine: About to run SSH command:
sudo mkdir -p /lib/systemd/system && printf %s "[Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
After=network.target  minikube-automount.service docker.socket
Requires= minikube-automount.service docker.socket 
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=hyperkit --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP \$MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target
" | sudo tee /lib/systemd/system/docker.service.new
I0518 21:49:09.917650    1457 main.go:141] libmachine: SSH cmd err, output: <nil>: [Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
After=network.target  minikube-automount.service docker.socket
Requires= minikube-automount.service docker.socket 
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=hyperkit --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP $MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target

I0518 21:49:09.918811    1457 main.go:141] libmachine: (minikube) Calling .GetSSHHostname
I0518 21:49:09.919104    1457 main.go:141] libmachine: (minikube) Calling .GetSSHPort
I0518 21:49:09.919335    1457 main.go:141] libmachine: (minikube) Calling .GetSSHKeyPath
I0518 21:49:09.919488    1457 main.go:141] libmachine: (minikube) Calling .GetSSHKeyPath
I0518 21:49:09.919651    1457 main.go:141] libmachine: (minikube) Calling .GetSSHUsername
I0518 21:49:09.919971    1457 main.go:141] libmachine: Using SSH client type: native
I0518 21:49:09.920199    1457 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x104ddf6a0] 0x104de2380 <nil>  [] 0s} 192.168.65.3 22 <nil> <nil>}
I0518 21:49:09.920213    1457 main.go:141] libmachine: About to run SSH command:
sudo diff -u /lib/systemd/system/docker.service /lib/systemd/system/docker.service.new || { sudo mv /lib/systemd/system/docker.service.new /lib/systemd/system/docker.service; sudo systemctl -f daemon-reload && sudo systemctl -f enable docker && sudo systemctl -f restart docker; }
I0518 21:49:13.896204    1457 main.go:141] libmachine: SSH cmd err, output: <nil>: diff: can't stat '/lib/systemd/system/docker.service': No such file or directory
Created symlink /etc/systemd/system/multi-user.target.wants/docker.service â†’ /usr/lib/systemd/system/docker.service.

I0518 21:49:13.896225    1457 main.go:141] libmachine: Checking connection to Docker...
I0518 21:49:13.896258    1457 main.go:141] libmachine: (minikube) Calling .GetURL
I0518 21:49:13.896623    1457 main.go:141] libmachine: Docker is up and running!
I0518 21:49:13.896631    1457 main.go:141] libmachine: Reticulating splines...
I0518 21:49:13.896639    1457 client.go:171] duration metric: took 48.085785393s to LocalClient.Create
I0518 21:49:13.896711    1457 start.go:167] duration metric: took 48.086499003s to libmachine.API.Create "minikube"
I0518 21:49:13.897904    1457 start.go:293] postStartSetup for "minikube" (driver="hyperkit")
I0518 21:49:13.897938    1457 start.go:322] creating required directories: [/etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs]
I0518 21:49:13.897981    1457 main.go:141] libmachine: (minikube) Calling .DriverName
I0518 21:49:13.898378    1457 ssh_runner.go:195] Run: sudo mkdir -p /etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs
I0518 21:49:13.898404    1457 main.go:141] libmachine: (minikube) Calling .GetSSHHostname
I0518 21:49:13.898638    1457 main.go:141] libmachine: (minikube) Calling .GetSSHPort
I0518 21:49:13.898782    1457 main.go:141] libmachine: (minikube) Calling .GetSSHKeyPath
I0518 21:49:13.898970    1457 main.go:141] libmachine: (minikube) Calling .GetSSHUsername
I0518 21:49:13.899182    1457 sshutil.go:53] new ssh client: &{IP:192.168.65.3 Port:22 SSHKeyPath:/Users/christinamanara/.minikube/machines/minikube/id_rsa Username:docker}
I0518 21:49:14.032301    1457 ssh_runner.go:195] Run: cat /etc/os-release
I0518 21:49:14.047218    1457 info.go:137] Remote host: Buildroot 2023.02.9
I0518 21:49:14.047957    1457 filesync.go:126] Scanning /Users/christinamanara/.minikube/addons for local assets ...
I0518 21:49:14.048442    1457 filesync.go:126] Scanning /Users/christinamanara/.minikube/files for local assets ...
I0518 21:49:14.048559    1457 start.go:296] duration metric: took 150.641757ms for postStartSetup
I0518 21:49:14.048940    1457 main.go:141] libmachine: (minikube) Calling .GetConfigRaw
I0518 21:49:14.051994    1457 main.go:141] libmachine: (minikube) Calling .GetIP
I0518 21:49:14.052441    1457 profile.go:143] Saving config to /Users/christinamanara/.minikube/profiles/minikube/config.json ...
I0518 21:49:14.054412    1457 start.go:128] duration metric: took 48.944626384s to createHost
I0518 21:49:14.054793    1457 main.go:141] libmachine: (minikube) Calling .GetSSHHostname
I0518 21:49:14.055286    1457 main.go:141] libmachine: (minikube) Calling .GetSSHPort
I0518 21:49:14.057103    1457 main.go:141] libmachine: (minikube) Calling .GetSSHKeyPath
I0518 21:49:14.057515    1457 main.go:141] libmachine: (minikube) Calling .GetSSHKeyPath
I0518 21:49:14.058022    1457 main.go:141] libmachine: (minikube) Calling .GetSSHUsername
I0518 21:49:14.059041    1457 main.go:141] libmachine: Using SSH client type: native
I0518 21:49:14.059437    1457 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x104ddf6a0] 0x104de2380 <nil>  [] 0s} 192.168.65.3 22 <nil> <nil>}
I0518 21:49:14.059449    1457 main.go:141] libmachine: About to run SSH command:
date +%s.%N
I0518 21:49:14.236041    1457 main.go:141] libmachine: SSH cmd err, output: <nil>: 1747594154.494781442

I0518 21:49:14.236054    1457 fix.go:216] guest clock: 1747594154.494781442
I0518 21:49:14.236064    1457 fix.go:229] Guest: 2025-05-18 21:49:14.494781442 +0300 EEST Remote: 2025-05-18 21:49:14.05476 +0300 EEST m=+147.744413480 (delta=440.021442ms)
I0518 21:49:14.236630    1457 fix.go:200] guest clock delta is within tolerance: 440.021442ms
I0518 21:49:14.236648    1457 start.go:83] releasing machines lock for "minikube", held for 49.130274934s
I0518 21:49:14.236970    1457 main.go:141] libmachine: (minikube) Calling .DriverName
I0518 21:49:14.237304    1457 main.go:141] libmachine: (minikube) Calling .GetIP
I0518 21:49:14.237518    1457 main.go:141] libmachine: (minikube) Calling .DriverName
I0518 21:49:14.238404    1457 main.go:141] libmachine: (minikube) Calling .DriverName
I0518 21:49:14.239042    1457 main.go:141] libmachine: (minikube) Calling .DriverName
I0518 21:49:14.239666    1457 ssh_runner.go:195] Run: cat /version.json
I0518 21:49:14.239686    1457 main.go:141] libmachine: (minikube) Calling .GetSSHHostname
I0518 21:49:14.239957    1457 main.go:141] libmachine: (minikube) Calling .GetSSHPort
I0518 21:49:14.240092    1457 ssh_runner.go:195] Run: curl -sS -m 2 https://registry.k8s.io/
I0518 21:49:14.240182    1457 main.go:141] libmachine: (minikube) Calling .GetSSHKeyPath
I0518 21:49:14.240544    1457 main.go:141] libmachine: (minikube) Calling .GetSSHUsername
I0518 21:49:14.240780    1457 sshutil.go:53] new ssh client: &{IP:192.168.65.3 Port:22 SSHKeyPath:/Users/christinamanara/.minikube/machines/minikube/id_rsa Username:docker}
I0518 21:49:14.241218    1457 main.go:141] libmachine: (minikube) Calling .GetSSHHostname
I0518 21:49:14.241418    1457 main.go:141] libmachine: (minikube) Calling .GetSSHPort
I0518 21:49:14.241634    1457 main.go:141] libmachine: (minikube) Calling .GetSSHKeyPath
I0518 21:49:14.241826    1457 main.go:141] libmachine: (minikube) Calling .GetSSHUsername
I0518 21:49:14.242003    1457 sshutil.go:53] new ssh client: &{IP:192.168.65.3 Port:22 SSHKeyPath:/Users/christinamanara/.minikube/machines/minikube/id_rsa Username:docker}
I0518 21:49:14.600634    1457 ssh_runner.go:195] Run: systemctl --version
I0518 21:49:14.620915    1457 ssh_runner.go:195] Run: sh -c "stat /etc/cni/net.d/*loopback.conf*"
W0518 21:49:14.643111    1457 cni.go:209] loopback cni configuration skipped: "/etc/cni/net.d/*loopback.conf*" not found
I0518 21:49:14.643610    1457 ssh_runner.go:195] Run: sudo find /etc/cni/net.d -maxdepth 1 -type f ( ( -name *bridge* -or -name *podman* ) -and -not -name *.mk_disabled ) -printf "%p, " -exec sh -c "sudo mv {} {}.mk_disabled" ;
I0518 21:49:14.734340    1457 cni.go:262] disabled [/etc/cni/net.d/87-podman-bridge.conflist] bridge cni config(s)
I0518 21:49:14.734357    1457 start.go:495] detecting cgroup driver to use...
I0518 21:49:14.737359    1457 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %s "runtime-endpoint: unix:///run/containerd/containerd.sock
" | sudo tee /etc/crictl.yaml"
I0518 21:49:14.822925    1457 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)sandbox_image = .*$|\1sandbox_image = "registry.k8s.io/pause:3.10"|' /etc/containerd/config.toml"
I0518 21:49:14.881003    1457 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)restrict_oom_score_adj = .*$|\1restrict_oom_score_adj = false|' /etc/containerd/config.toml"
I0518 21:49:14.940593    1457 containerd.go:146] configuring containerd to use "cgroupfs" as cgroup driver...
I0518 21:49:14.940786    1457 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)SystemdCgroup = .*$|\1SystemdCgroup = false|g' /etc/containerd/config.toml"
I0518 21:49:14.983736    1457 ssh_runner.go:195] Run: sh -c "sudo sed -i 's|"io.containerd.runtime.v1.linux"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
I0518 21:49:15.030611    1457 ssh_runner.go:195] Run: sh -c "sudo sed -i '/systemd_cgroup/d' /etc/containerd/config.toml"
I0518 21:49:15.081884    1457 ssh_runner.go:195] Run: sh -c "sudo sed -i 's|"io.containerd.runc.v1"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
I0518 21:49:15.118016    1457 ssh_runner.go:195] Run: sh -c "sudo rm -rf /etc/cni/net.mk"
I0518 21:49:15.157869    1457 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)conf_dir = .*$|\1conf_dir = "/etc/cni/net.d"|g' /etc/containerd/config.toml"
I0518 21:49:15.202228    1457 ssh_runner.go:195] Run: sh -c "sudo sed -i '/^ *enable_unprivileged_ports = .*/d' /etc/containerd/config.toml"
I0518 21:49:15.245548    1457 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)\[plugins."io.containerd.grpc.v1.cri"\]|&\n\1  enable_unprivileged_ports = true|' /etc/containerd/config.toml"
I0518 21:49:15.287971    1457 ssh_runner.go:195] Run: sudo sysctl net.bridge.bridge-nf-call-iptables
I0518 21:49:15.329024    1457 crio.go:166] couldn't verify netfilter by "sudo sysctl net.bridge.bridge-nf-call-iptables" which might be okay. error: sudo sysctl net.bridge.bridge-nf-call-iptables: Process exited with status 255
stdout:

stderr:
sysctl: cannot stat /proc/sys/net/bridge/bridge-nf-call-iptables: No such file or directory
I0518 21:49:15.329169    1457 ssh_runner.go:195] Run: sudo modprobe br_netfilter
I0518 21:49:15.381907    1457 ssh_runner.go:195] Run: sudo sh -c "echo 1 > /proc/sys/net/ipv4/ip_forward"
I0518 21:49:15.441149    1457 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0518 21:49:15.999577    1457 ssh_runner.go:195] Run: sudo systemctl restart containerd
I0518 21:49:16.071696    1457 start.go:495] detecting cgroup driver to use...
I0518 21:49:16.079073    1457 ssh_runner.go:195] Run: sudo systemctl cat docker.service
I0518 21:49:16.169968    1457 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service containerd
I0518 21:49:16.220913    1457 ssh_runner.go:195] Run: sudo systemctl stop -f containerd
I0518 21:49:16.289713    1457 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service containerd
I0518 21:49:16.339140    1457 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service crio
I0518 21:49:16.385542    1457 ssh_runner.go:195] Run: sudo systemctl stop -f crio
I0518 21:49:16.562227    1457 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service crio
I0518 21:49:16.606764    1457 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %s "runtime-endpoint: unix:///var/run/cri-dockerd.sock
" | sudo tee /etc/crictl.yaml"
I0518 21:49:16.677372    1457 ssh_runner.go:195] Run: which cri-dockerd
I0518 21:49:16.691131    1457 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/cri-docker.service.d
I0518 21:49:16.741135    1457 ssh_runner.go:362] scp memory --> /etc/systemd/system/cri-docker.service.d/10-cni.conf (190 bytes)
I0518 21:49:16.804309    1457 ssh_runner.go:195] Run: sudo systemctl unmask docker.service
I0518 21:49:17.220970    1457 ssh_runner.go:195] Run: sudo systemctl enable docker.socket
I0518 21:49:17.891210    1457 docker.go:574] configuring docker to use "cgroupfs" as cgroup driver...
I0518 21:49:17.897072    1457 ssh_runner.go:362] scp memory --> /etc/docker/daemon.json (130 bytes)
I0518 21:49:18.045107    1457 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0518 21:49:18.671981    1457 ssh_runner.go:195] Run: sudo systemctl restart docker
I0518 21:49:23.222611    1457 ssh_runner.go:235] Completed: sudo systemctl restart docker: (4.550402757s)
I0518 21:49:23.223014    1457 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service cri-docker.socket
I0518 21:49:23.298914    1457 ssh_runner.go:195] Run: sudo systemctl stop cri-docker.socket
I0518 21:49:23.398098    1457 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service cri-docker.service
I0518 21:49:23.458402    1457 ssh_runner.go:195] Run: sudo systemctl unmask cri-docker.socket
I0518 21:49:23.963305    1457 ssh_runner.go:195] Run: sudo systemctl enable cri-docker.socket
I0518 21:49:24.902157    1457 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0518 21:49:25.363501    1457 ssh_runner.go:195] Run: sudo systemctl restart cri-docker.socket
I0518 21:49:25.460273    1457 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service cri-docker.service
I0518 21:49:25.516135    1457 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0518 21:49:25.916617    1457 ssh_runner.go:195] Run: sudo systemctl restart cri-docker.service
I0518 21:49:26.211191    1457 start.go:542] Will wait 60s for socket path /var/run/cri-dockerd.sock
I0518 21:49:26.216001    1457 ssh_runner.go:195] Run: stat /var/run/cri-dockerd.sock
I0518 21:49:26.239041    1457 start.go:563] Will wait 60s for crictl version
I0518 21:49:26.239190    1457 ssh_runner.go:195] Run: which crictl
I0518 21:49:26.258533    1457 ssh_runner.go:195] Run: sudo /usr/bin/crictl version
I0518 21:49:26.426281    1457 start.go:579] Version:  0.1.0
RuntimeName:  docker
RuntimeVersion:  27.4.0
RuntimeApiVersion:  v1
I0518 21:49:26.427026    1457 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I0518 21:49:26.498348    1457 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I0518 21:49:26.606862    1457 out.go:235] ðŸ³  Preparing Kubernetes v1.32.0 on Docker 27.4.0 ...
I0518 21:49:26.607289    1457 main.go:141] libmachine: (minikube) Calling .GetIP
I0518 21:49:26.609876    1457 ssh_runner.go:195] Run: grep 192.168.65.1	host.minikube.internal$ /etc/hosts
I0518 21:49:26.619917    1457 ssh_runner.go:195] Run: /bin/bash -c "{ grep -v $'\thost.minikube.internal$' "/etc/hosts"; echo "192.168.65.1	host.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I0518 21:49:26.671754    1457 kubeadm.go:883] updating cluster {Name:minikube KeepContext:false EmbedCerts:false MinikubeISO:https://storage.googleapis.com/minikube/iso/minikube-v1.35.0-amd64.iso KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279 Memory:2200 CPUs:2 DiskSize:20000 Driver:hyperkit HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.32.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.65.3 Port:8443 KubernetesVersion:v1.32.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/Users:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s} ...
I0518 21:49:26.674375    1457 preload.go:131] Checking if preload exists for k8s version v1.32.0 and runtime docker
I0518 21:49:26.674699    1457 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I0518 21:49:26.723160    1457 docker.go:689] Got preloaded images: 
I0518 21:49:26.723180    1457 docker.go:695] registry.k8s.io/kube-apiserver:v1.32.0 wasn't preloaded
I0518 21:49:26.723437    1457 ssh_runner.go:195] Run: sudo cat /var/lib/docker/image/overlay2/repositories.json
I0518 21:49:26.769739    1457 ssh_runner.go:195] Run: which lz4
I0518 21:49:26.784196    1457 ssh_runner.go:195] Run: stat -c "%s %y" /preloaded.tar.lz4
I0518 21:49:26.797027    1457 ssh_runner.go:352] existence check for /preloaded.tar.lz4: stat -c "%s %y" /preloaded.tar.lz4: Process exited with status 1
stdout:

stderr:
stat: cannot statx '/preloaded.tar.lz4': No such file or directory
I0518 21:49:26.797151    1457 ssh_runner.go:362] scp /Users/christinamanara/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.32.0-docker-overlay2-amd64.tar.lz4 --> /preloaded.tar.lz4 (349777613 bytes)
I0518 21:49:33.665300    1457 docker.go:653] duration metric: took 6.881039712s to copy over tarball
I0518 21:49:33.665930    1457 ssh_runner.go:195] Run: sudo tar --xattrs --xattrs-include security.capability -I lz4 -C /var -xf /preloaded.tar.lz4
I0518 21:49:44.109228    1457 ssh_runner.go:235] Completed: sudo tar --xattrs --xattrs-include security.capability -I lz4 -C /var -xf /preloaded.tar.lz4: (10.442937377s)
I0518 21:49:44.109289    1457 ssh_runner.go:146] rm: /preloaded.tar.lz4
I0518 21:49:44.227129    1457 ssh_runner.go:195] Run: sudo cat /var/lib/docker/image/overlay2/repositories.json
I0518 21:49:44.266836    1457 ssh_runner.go:362] scp memory --> /var/lib/docker/image/overlay2/repositories.json (2631 bytes)
I0518 21:49:44.328612    1457 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0518 21:49:44.669949    1457 ssh_runner.go:195] Run: sudo systemctl restart docker
I0518 21:49:48.118322    1457 ssh_runner.go:235] Completed: sudo systemctl restart docker: (3.448248803s)
I0518 21:49:48.119086    1457 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I0518 21:49:48.178490    1457 docker.go:689] Got preloaded images: -- stdout --
registry.k8s.io/kube-apiserver:v1.32.0
registry.k8s.io/kube-controller-manager:v1.32.0
registry.k8s.io/kube-scheduler:v1.32.0
registry.k8s.io/kube-proxy:v1.32.0
registry.k8s.io/etcd:3.5.16-0
registry.k8s.io/coredns/coredns:v1.11.3
registry.k8s.io/pause:3.10
gcr.io/k8s-minikube/storage-provisioner:v5

-- /stdout --
I0518 21:49:48.179017    1457 cache_images.go:84] Images are preloaded, skipping loading
I0518 21:49:48.179059    1457 kubeadm.go:934] updating node { 192.168.65.3 8443 v1.32.0 docker true true} ...
I0518 21:49:48.184405    1457 kubeadm.go:946] kubelet [Unit]
Wants=docker.socket

[Service]
ExecStart=
ExecStart=/var/lib/minikube/binaries/v1.32.0/kubelet --bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --config=/var/lib/kubelet/config.yaml --hostname-override=minikube --kubeconfig=/etc/kubernetes/kubelet.conf --node-ip=192.168.65.3

[Install]
 config:
{KubernetesVersion:v1.32.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:}
I0518 21:49:48.185407    1457 ssh_runner.go:195] Run: docker info --format {{.CgroupDriver}}
I0518 21:49:48.332593    1457 cni.go:84] Creating CNI manager for ""
I0518 21:49:48.332633    1457 cni.go:158] "hyperkit" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I0518 21:49:48.333225    1457 kubeadm.go:84] Using pod CIDR: 10.244.0.0/16
I0518 21:49:48.333850    1457 kubeadm.go:189] kubeadm options: {CertDir:/var/lib/minikube/certs ServiceCIDR:10.96.0.0/12 PodSubnet:10.244.0.0/16 AdvertiseAddress:192.168.65.3 APIServerPort:8443 KubernetesVersion:v1.32.0 EtcdDataDir:/var/lib/minikube/etcd EtcdExtraArgs:map[] ClusterName:minikube NodeName:minikube DNSDomain:cluster.local CRISocket:/var/run/cri-dockerd.sock ImageRepository: ComponentOptions:[{Component:apiServer ExtraArgs:map[enable-admission-plugins:NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota] Pairs:map[certSANs:["127.0.0.1", "localhost", "192.168.65.3"]]} {Component:controllerManager ExtraArgs:map[allocate-node-cidrs:true leader-elect:false] Pairs:map[]} {Component:scheduler ExtraArgs:map[leader-elect:false] Pairs:map[]}] FeatureArgs:map[] NodeIP:192.168.65.3 CgroupDriver:cgroupfs ClientCAFile:/var/lib/minikube/certs/ca.crt StaticPodPath:/etc/kubernetes/manifests ControlPlaneAddress:control-plane.minikube.internal KubeProxyOptions:map[] ResolvConfSearchRegression:false KubeletConfigOpts:map[containerRuntimeEndpoint:unix:///var/run/cri-dockerd.sock hairpinMode:hairpin-veth runtimeRequestTimeout:15m] PrependCriSocketUnix:true}
I0518 21:49:48.335757    1457 kubeadm.go:195] kubeadm config:
apiVersion: kubeadm.k8s.io/v1beta4
kind: InitConfiguration
localAPIEndpoint:
  advertiseAddress: 192.168.65.3
  bindPort: 8443
bootstrapTokens:
  - groups:
      - system:bootstrappers:kubeadm:default-node-token
    ttl: 24h0m0s
    usages:
      - signing
      - authentication
nodeRegistration:
  criSocket: unix:///var/run/cri-dockerd.sock
  name: "minikube"
  kubeletExtraArgs:
    - name: "node-ip"
      value: "192.168.65.3"
  taints: []
---
apiVersion: kubeadm.k8s.io/v1beta4
kind: ClusterConfiguration
apiServer:
  certSANs: ["127.0.0.1", "localhost", "192.168.65.3"]
  extraArgs:
    - name: "enable-admission-plugins"
      value: "NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota"
controllerManager:
  extraArgs:
    - name: "allocate-node-cidrs"
      value: "true"
    - name: "leader-elect"
      value: "false"
scheduler:
  extraArgs:
    - name: "leader-elect"
      value: "false"
certificatesDir: /var/lib/minikube/certs
clusterName: mk
controlPlaneEndpoint: control-plane.minikube.internal:8443
etcd:
  local:
    dataDir: /var/lib/minikube/etcd
    extraArgs:
      - name: "proxy-refresh-interval"
        value: "70000"
kubernetesVersion: v1.32.0
networking:
  dnsDomain: cluster.local
  podSubnet: "10.244.0.0/16"
  serviceSubnet: 10.96.0.0/12
---
apiVersion: kubelet.config.k8s.io/v1beta1
kind: KubeletConfiguration
authentication:
  x509:
    clientCAFile: /var/lib/minikube/certs/ca.crt
cgroupDriver: cgroupfs
containerRuntimeEndpoint: unix:///var/run/cri-dockerd.sock
hairpinMode: hairpin-veth
runtimeRequestTimeout: 15m
clusterDomain: "cluster.local"
# disable disk resource management by default
imageGCHighThresholdPercent: 100
evictionHard:
  nodefs.available: "0%"
  nodefs.inodesFree: "0%"
  imagefs.available: "0%"
failSwapOn: false
staticPodPath: /etc/kubernetes/manifests
---
apiVersion: kubeproxy.config.k8s.io/v1alpha1
kind: KubeProxyConfiguration
clusterCIDR: "10.244.0.0/16"
metricsBindAddress: 0.0.0.0:10249
conntrack:
  maxPerCore: 0
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_established"
  tcpEstablishedTimeout: 0s
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_close"
  tcpCloseWaitTimeout: 0s

I0518 21:49:48.336545    1457 ssh_runner.go:195] Run: sudo ls /var/lib/minikube/binaries/v1.32.0
I0518 21:49:48.372791    1457 binaries.go:44] Found k8s binaries, skipping transfer
I0518 21:49:48.372973    1457 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/kubelet.service.d /lib/systemd/system /var/tmp/minikube
I0518 21:49:48.412825    1457 ssh_runner.go:362] scp memory --> /etc/systemd/system/kubelet.service.d/10-kubeadm.conf (307 bytes)
I0518 21:49:48.498778    1457 ssh_runner.go:362] scp memory --> /lib/systemd/system/kubelet.service (352 bytes)
I0518 21:49:48.568484    1457 ssh_runner.go:362] scp memory --> /var/tmp/minikube/kubeadm.yaml.new (2286 bytes)
I0518 21:49:48.634532    1457 ssh_runner.go:195] Run: grep 192.168.65.3	control-plane.minikube.internal$ /etc/hosts
I0518 21:49:48.646274    1457 ssh_runner.go:195] Run: /bin/bash -c "{ grep -v $'\tcontrol-plane.minikube.internal$' "/etc/hosts"; echo "192.168.65.3	control-plane.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I0518 21:49:48.703773    1457 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0518 21:49:49.086159    1457 ssh_runner.go:195] Run: sudo systemctl start kubelet
I0518 21:49:49.154169    1457 certs.go:68] Setting up /Users/christinamanara/.minikube/profiles/minikube for IP: 192.168.65.3
I0518 21:49:49.154494    1457 certs.go:194] generating shared ca certs ...
I0518 21:49:49.154534    1457 certs.go:226] acquiring lock for ca certs: {Name:mkd4f84069d894728fe24038604a8d209cde62bc Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0518 21:49:49.156766    1457 certs.go:240] generating "minikubeCA" ca cert: /Users/christinamanara/.minikube/ca.key
I0518 21:49:49.327021    1457 crypto.go:156] Writing cert to /Users/christinamanara/.minikube/ca.crt ...
I0518 21:49:49.327041    1457 lock.go:35] WriteFile acquiring /Users/christinamanara/.minikube/ca.crt: {Name:mk4fb6f18a28e8d20567b94406aaf61e53dc3a52 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0518 21:49:49.327687    1457 crypto.go:164] Writing key to /Users/christinamanara/.minikube/ca.key ...
I0518 21:49:49.327696    1457 lock.go:35] WriteFile acquiring /Users/christinamanara/.minikube/ca.key: {Name:mk1e7df37bfe387c2d1fcb1d486e6058dc973628 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0518 21:49:49.329395    1457 certs.go:240] generating "proxyClientCA" ca cert: /Users/christinamanara/.minikube/proxy-client-ca.key
I0518 21:49:49.524722    1457 crypto.go:156] Writing cert to /Users/christinamanara/.minikube/proxy-client-ca.crt ...
I0518 21:49:49.524770    1457 lock.go:35] WriteFile acquiring /Users/christinamanara/.minikube/proxy-client-ca.crt: {Name:mk29a3caa3239598e032a2801c2ef17d785e46d6 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0518 21:49:49.525711    1457 crypto.go:164] Writing key to /Users/christinamanara/.minikube/proxy-client-ca.key ...
I0518 21:49:49.525722    1457 lock.go:35] WriteFile acquiring /Users/christinamanara/.minikube/proxy-client-ca.key: {Name:mkdd1f89c491c0ad34f7a212f94547fb2049d875 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0518 21:49:49.526607    1457 certs.go:256] generating profile certs ...
I0518 21:49:49.527304    1457 certs.go:363] generating signed profile cert for "minikube-user": /Users/christinamanara/.minikube/profiles/minikube/client.key
I0518 21:49:49.527326    1457 crypto.go:68] Generating cert /Users/christinamanara/.minikube/profiles/minikube/client.crt with IP's: []
I0518 21:49:49.735696    1457 crypto.go:156] Writing cert to /Users/christinamanara/.minikube/profiles/minikube/client.crt ...
I0518 21:49:49.735712    1457 lock.go:35] WriteFile acquiring /Users/christinamanara/.minikube/profiles/minikube/client.crt: {Name:mk3a174c289ebee51b77955bf4ea68c8c42b1d31 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0518 21:49:49.736484    1457 crypto.go:164] Writing key to /Users/christinamanara/.minikube/profiles/minikube/client.key ...
I0518 21:49:49.736499    1457 lock.go:35] WriteFile acquiring /Users/christinamanara/.minikube/profiles/minikube/client.key: {Name:mkb3b0a9a6beff62b93f9dc040833fabe621fc07 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0518 21:49:49.737221    1457 certs.go:363] generating signed profile cert for "minikube": /Users/christinamanara/.minikube/profiles/minikube/apiserver.key.7c51c335
I0518 21:49:49.737269    1457 crypto.go:68] Generating cert /Users/christinamanara/.minikube/profiles/minikube/apiserver.crt.7c51c335 with IP's: [10.96.0.1 127.0.0.1 10.0.0.1 192.168.65.3]
I0518 21:49:50.040426    1457 crypto.go:156] Writing cert to /Users/christinamanara/.minikube/profiles/minikube/apiserver.crt.7c51c335 ...
I0518 21:49:50.040441    1457 lock.go:35] WriteFile acquiring /Users/christinamanara/.minikube/profiles/minikube/apiserver.crt.7c51c335: {Name:mkd54977bbcc5f3292716f645746459251a5e601 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0518 21:49:50.041156    1457 crypto.go:164] Writing key to /Users/christinamanara/.minikube/profiles/minikube/apiserver.key.7c51c335 ...
I0518 21:49:50.041169    1457 lock.go:35] WriteFile acquiring /Users/christinamanara/.minikube/profiles/minikube/apiserver.key.7c51c335: {Name:mkcd463bae964485fa9bf5023945e035af48b402 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0518 21:49:50.042113    1457 certs.go:381] copying /Users/christinamanara/.minikube/profiles/minikube/apiserver.crt.7c51c335 -> /Users/christinamanara/.minikube/profiles/minikube/apiserver.crt
I0518 21:49:50.042781    1457 certs.go:385] copying /Users/christinamanara/.minikube/profiles/minikube/apiserver.key.7c51c335 -> /Users/christinamanara/.minikube/profiles/minikube/apiserver.key
I0518 21:49:50.043054    1457 certs.go:363] generating signed profile cert for "aggregator": /Users/christinamanara/.minikube/profiles/minikube/proxy-client.key
I0518 21:49:50.043080    1457 crypto.go:68] Generating cert /Users/christinamanara/.minikube/profiles/minikube/proxy-client.crt with IP's: []
I0518 21:49:50.221249    1457 crypto.go:156] Writing cert to /Users/christinamanara/.minikube/profiles/minikube/proxy-client.crt ...
I0518 21:49:50.221263    1457 lock.go:35] WriteFile acquiring /Users/christinamanara/.minikube/profiles/minikube/proxy-client.crt: {Name:mk1a8c25de2043927f52ccb1d85492daf095da8d Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0518 21:49:50.221782    1457 crypto.go:164] Writing key to /Users/christinamanara/.minikube/profiles/minikube/proxy-client.key ...
I0518 21:49:50.221790    1457 lock.go:35] WriteFile acquiring /Users/christinamanara/.minikube/profiles/minikube/proxy-client.key: {Name:mk0209a34a90296e8e40f4f1fd01268291149460 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0518 21:49:50.224644    1457 certs.go:484] found cert: /Users/christinamanara/.minikube/certs/ca-key.pem (1679 bytes)
I0518 21:49:50.225127    1457 certs.go:484] found cert: /Users/christinamanara/.minikube/certs/ca.pem (1103 bytes)
I0518 21:49:50.225450    1457 certs.go:484] found cert: /Users/christinamanara/.minikube/certs/cert.pem (1147 bytes)
I0518 21:49:50.225833    1457 certs.go:484] found cert: /Users/christinamanara/.minikube/certs/key.pem (1675 bytes)
I0518 21:49:50.242793    1457 ssh_runner.go:362] scp /Users/christinamanara/.minikube/ca.crt --> /var/lib/minikube/certs/ca.crt (1111 bytes)
I0518 21:49:50.347943    1457 ssh_runner.go:362] scp /Users/christinamanara/.minikube/ca.key --> /var/lib/minikube/certs/ca.key (1679 bytes)
I0518 21:49:50.535285    1457 ssh_runner.go:362] scp /Users/christinamanara/.minikube/proxy-client-ca.crt --> /var/lib/minikube/certs/proxy-client-ca.crt (1119 bytes)
I0518 21:49:50.675509    1457 ssh_runner.go:362] scp /Users/christinamanara/.minikube/proxy-client-ca.key --> /var/lib/minikube/certs/proxy-client-ca.key (1679 bytes)
I0518 21:49:50.769313    1457 ssh_runner.go:362] scp /Users/christinamanara/.minikube/profiles/minikube/apiserver.crt --> /var/lib/minikube/certs/apiserver.crt (1411 bytes)
I0518 21:49:50.863394    1457 ssh_runner.go:362] scp /Users/christinamanara/.minikube/profiles/minikube/apiserver.key --> /var/lib/minikube/certs/apiserver.key (1675 bytes)
I0518 21:49:50.957084    1457 ssh_runner.go:362] scp /Users/christinamanara/.minikube/profiles/minikube/proxy-client.crt --> /var/lib/minikube/certs/proxy-client.crt (1147 bytes)
I0518 21:49:51.052183    1457 ssh_runner.go:362] scp /Users/christinamanara/.minikube/profiles/minikube/proxy-client.key --> /var/lib/minikube/certs/proxy-client.key (1679 bytes)
I0518 21:49:51.161088    1457 ssh_runner.go:362] scp /Users/christinamanara/.minikube/ca.crt --> /usr/share/ca-certificates/minikubeCA.pem (1111 bytes)
I0518 21:49:51.268997    1457 ssh_runner.go:362] scp memory --> /var/lib/minikube/kubeconfig (740 bytes)
I0518 21:49:51.354237    1457 ssh_runner.go:195] Run: openssl version
I0518 21:49:51.375205    1457 ssh_runner.go:195] Run: sudo /bin/bash -c "test -s /usr/share/ca-certificates/minikubeCA.pem && ln -fs /usr/share/ca-certificates/minikubeCA.pem /etc/ssl/certs/minikubeCA.pem"
I0518 21:49:51.427656    1457 ssh_runner.go:195] Run: ls -la /usr/share/ca-certificates/minikubeCA.pem
I0518 21:49:51.440235    1457 certs.go:528] hashing: -rw-r--r-- 1 root root 1111 May 18 18:49 /usr/share/ca-certificates/minikubeCA.pem
I0518 21:49:51.440337    1457 ssh_runner.go:195] Run: openssl x509 -hash -noout -in /usr/share/ca-certificates/minikubeCA.pem
I0518 21:49:51.453867    1457 ssh_runner.go:195] Run: sudo /bin/bash -c "test -L /etc/ssl/certs/b5213941.0 || ln -fs /etc/ssl/certs/minikubeCA.pem /etc/ssl/certs/b5213941.0"
I0518 21:49:51.492968    1457 ssh_runner.go:195] Run: stat /var/lib/minikube/certs/apiserver-kubelet-client.crt
I0518 21:49:51.504564    1457 certs.go:399] 'apiserver-kubelet-client' cert doesn't exist, likely first start: stat /var/lib/minikube/certs/apiserver-kubelet-client.crt: Process exited with status 1
stdout:

stderr:
stat: cannot statx '/var/lib/minikube/certs/apiserver-kubelet-client.crt': No such file or directory
I0518 21:49:51.505146    1457 kubeadm.go:392] StartCluster: {Name:minikube KeepContext:false EmbedCerts:false MinikubeISO:https://storage.googleapis.com/minikube/iso/minikube-v1.35.0-amd64.iso KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279 Memory:2200 CPUs:2 DiskSize:20000 Driver:hyperkit HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.32.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.65.3 Port:8443 KubernetesVersion:v1.32.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/Users:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s}
I0518 21:49:51.505366    1457 ssh_runner.go:195] Run: docker ps --filter status=paused --filter=name=k8s_.*_(kube-system)_ --format={{.ID}}
I0518 21:49:51.555664    1457 ssh_runner.go:195] Run: sudo ls /var/lib/kubelet/kubeadm-flags.env /var/lib/kubelet/config.yaml /var/lib/minikube/etcd
I0518 21:49:51.591880    1457 ssh_runner.go:195] Run: sudo cp /var/tmp/minikube/kubeadm.yaml.new /var/tmp/minikube/kubeadm.yaml
I0518 21:49:51.625101    1457 ssh_runner.go:195] Run: sudo ls -la /etc/kubernetes/admin.conf /etc/kubernetes/kubelet.conf /etc/kubernetes/controller-manager.conf /etc/kubernetes/scheduler.conf
I0518 21:49:51.662786    1457 kubeadm.go:155] config check failed, skipping stale config cleanup: sudo ls -la /etc/kubernetes/admin.conf /etc/kubernetes/kubelet.conf /etc/kubernetes/controller-manager.conf /etc/kubernetes/scheduler.conf: Process exited with status 2
stdout:

stderr:
ls: cannot access '/etc/kubernetes/admin.conf': No such file or directory
ls: cannot access '/etc/kubernetes/kubelet.conf': No such file or directory
ls: cannot access '/etc/kubernetes/controller-manager.conf': No such file or directory
ls: cannot access '/etc/kubernetes/scheduler.conf': No such file or directory
I0518 21:49:51.662808    1457 kubeadm.go:157] found existing configuration files:

I0518 21:49:51.665874    1457 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/admin.conf
I0518 21:49:51.698160    1457 kubeadm.go:163] "https://control-plane.minikube.internal:8443" may not be in /etc/kubernetes/admin.conf - will remove: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/admin.conf: Process exited with status 2
stdout:

stderr:
grep: /etc/kubernetes/admin.conf: No such file or directory
I0518 21:49:51.698316    1457 ssh_runner.go:195] Run: sudo rm -f /etc/kubernetes/admin.conf
I0518 21:49:51.731856    1457 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/kubelet.conf
I0518 21:49:51.765342    1457 kubeadm.go:163] "https://control-plane.minikube.internal:8443" may not be in /etc/kubernetes/kubelet.conf - will remove: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/kubelet.conf: Process exited with status 2
stdout:

stderr:
grep: /etc/kubernetes/kubelet.conf: No such file or directory
I0518 21:49:51.765500    1457 ssh_runner.go:195] Run: sudo rm -f /etc/kubernetes/kubelet.conf
I0518 21:49:51.801093    1457 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/controller-manager.conf
I0518 21:49:51.832772    1457 kubeadm.go:163] "https://control-plane.minikube.internal:8443" may not be in /etc/kubernetes/controller-manager.conf - will remove: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/controller-manager.conf: Process exited with status 2
stdout:

stderr:
grep: /etc/kubernetes/controller-manager.conf: No such file or directory
I0518 21:49:51.832976    1457 ssh_runner.go:195] Run: sudo rm -f /etc/kubernetes/controller-manager.conf
I0518 21:49:51.866291    1457 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/scheduler.conf
I0518 21:49:51.902335    1457 kubeadm.go:163] "https://control-plane.minikube.internal:8443" may not be in /etc/kubernetes/scheduler.conf - will remove: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/scheduler.conf: Process exited with status 2
stdout:

stderr:
grep: /etc/kubernetes/scheduler.conf: No such file or directory
I0518 21:49:51.902481    1457 ssh_runner.go:195] Run: sudo rm -f /etc/kubernetes/scheduler.conf
I0518 21:49:51.940947    1457 ssh_runner.go:286] Start: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.32.0:$PATH" kubeadm init --config /var/tmp/minikube/kubeadm.yaml  --ignore-preflight-errors=DirAvailable--etc-kubernetes-manifests,DirAvailable--var-lib-minikube,DirAvailable--var-lib-minikube-etcd,FileAvailable--etc-kubernetes-manifests-kube-scheduler.yaml,FileAvailable--etc-kubernetes-manifests-kube-apiserver.yaml,FileAvailable--etc-kubernetes-manifests-kube-controller-manager.yaml,FileAvailable--etc-kubernetes-manifests-etcd.yaml,Port-10250,Swap,NumCPU,Mem"
I0518 21:49:52.899428    1457 kubeadm.go:310] 	[WARNING Service-Kubelet]: kubelet service is not enabled, please run 'systemctl enable kubelet.service'
I0518 21:50:27.247101    1457 kubeadm.go:310] [init] Using Kubernetes version: v1.32.0
I0518 21:50:27.247230    1457 kubeadm.go:310] [preflight] Running pre-flight checks
I0518 21:50:27.247380    1457 kubeadm.go:310] [preflight] Pulling images required for setting up a Kubernetes cluster
I0518 21:50:27.248433    1457 kubeadm.go:310] [preflight] This might take a minute or two, depending on the speed of your internet connection
I0518 21:50:27.248623    1457 kubeadm.go:310] [preflight] You can also perform this action beforehand using 'kubeadm config images pull'
I0518 21:50:27.248749    1457 kubeadm.go:310] [certs] Using certificateDir folder "/var/lib/minikube/certs"
I0518 21:50:27.298830    1457 out.go:235]     â–ª Generating certificates and keys ...
I0518 21:50:27.299005    1457 kubeadm.go:310] [certs] Using existing ca certificate authority
I0518 21:50:27.299167    1457 kubeadm.go:310] [certs] Using existing apiserver certificate and key on disk
I0518 21:50:27.299377    1457 kubeadm.go:310] [certs] Generating "apiserver-kubelet-client" certificate and key
I0518 21:50:27.299485    1457 kubeadm.go:310] [certs] Generating "front-proxy-ca" certificate and key
I0518 21:50:27.299640    1457 kubeadm.go:310] [certs] Generating "front-proxy-client" certificate and key
I0518 21:50:27.299745    1457 kubeadm.go:310] [certs] Generating "etcd/ca" certificate and key
I0518 21:50:27.299845    1457 kubeadm.go:310] [certs] Generating "etcd/server" certificate and key
I0518 21:50:27.300102    1457 kubeadm.go:310] [certs] etcd/server serving cert is signed for DNS names [localhost minikube] and IPs [192.168.65.3 127.0.0.1 ::1]
I0518 21:50:27.300184    1457 kubeadm.go:310] [certs] Generating "etcd/peer" certificate and key
I0518 21:50:27.300442    1457 kubeadm.go:310] [certs] etcd/peer serving cert is signed for DNS names [localhost minikube] and IPs [192.168.65.3 127.0.0.1 ::1]
I0518 21:50:27.300557    1457 kubeadm.go:310] [certs] Generating "etcd/healthcheck-client" certificate and key
I0518 21:50:27.300657    1457 kubeadm.go:310] [certs] Generating "apiserver-etcd-client" certificate and key
I0518 21:50:27.300731    1457 kubeadm.go:310] [certs] Generating "sa" key and public key
I0518 21:50:27.300810    1457 kubeadm.go:310] [kubeconfig] Using kubeconfig folder "/etc/kubernetes"
I0518 21:50:27.300876    1457 kubeadm.go:310] [kubeconfig] Writing "admin.conf" kubeconfig file
I0518 21:50:27.300973    1457 kubeadm.go:310] [kubeconfig] Writing "super-admin.conf" kubeconfig file
I0518 21:50:27.301043    1457 kubeadm.go:310] [kubeconfig] Writing "kubelet.conf" kubeconfig file
I0518 21:50:27.301131    1457 kubeadm.go:310] [kubeconfig] Writing "controller-manager.conf" kubeconfig file
I0518 21:50:27.301220    1457 kubeadm.go:310] [kubeconfig] Writing "scheduler.conf" kubeconfig file
I0518 21:50:27.301337    1457 kubeadm.go:310] [etcd] Creating static Pod manifest for local etcd in "/etc/kubernetes/manifests"
I0518 21:50:27.301463    1457 kubeadm.go:310] [control-plane] Using manifest folder "/etc/kubernetes/manifests"
I0518 21:50:27.338744    1457 out.go:235]     â–ª Booting up control plane ...
I0518 21:50:27.338878    1457 kubeadm.go:310] [control-plane] Creating static Pod manifest for "kube-apiserver"
I0518 21:50:27.338986    1457 kubeadm.go:310] [control-plane] Creating static Pod manifest for "kube-controller-manager"
I0518 21:50:27.339058    1457 kubeadm.go:310] [control-plane] Creating static Pod manifest for "kube-scheduler"
I0518 21:50:27.339277    1457 kubeadm.go:310] [kubelet-start] Writing kubelet environment file with flags to file "/var/lib/kubelet/kubeadm-flags.env"
I0518 21:50:27.339517    1457 kubeadm.go:310] [kubelet-start] Writing kubelet configuration to file "/var/lib/kubelet/config.yaml"
I0518 21:50:27.339626    1457 kubeadm.go:310] [kubelet-start] Starting the kubelet
I0518 21:50:27.339840    1457 kubeadm.go:310] [wait-control-plane] Waiting for the kubelet to boot up the control plane as static Pods from directory "/etc/kubernetes/manifests"
I0518 21:50:27.340009    1457 kubeadm.go:310] [kubelet-check] Waiting for a healthy kubelet at http://127.0.0.1:10248/healthz. This can take up to 4m0s
I0518 21:50:27.340122    1457 kubeadm.go:310] [kubelet-check] The kubelet is healthy after 2.004925745s
I0518 21:50:27.340218    1457 kubeadm.go:310] [api-check] Waiting for a healthy API server. This can take up to 4m0s
I0518 21:50:27.340289    1457 kubeadm.go:310] [api-check] The API server is healthy after 20.507133408s
I0518 21:50:27.340419    1457 kubeadm.go:310] [upload-config] Storing the configuration used in ConfigMap "kubeadm-config" in the "kube-system" Namespace
I0518 21:50:27.340578    1457 kubeadm.go:310] [kubelet] Creating a ConfigMap "kubelet-config" in namespace kube-system with the configuration for the kubelets in the cluster
I0518 21:50:27.340642    1457 kubeadm.go:310] [upload-certs] Skipping phase. Please see --upload-certs
I0518 21:50:27.340912    1457 kubeadm.go:310] [mark-control-plane] Marking the node minikube as control-plane by adding the labels: [node-role.kubernetes.io/control-plane node.kubernetes.io/exclude-from-external-load-balancers]
I0518 21:50:27.341068    1457 kubeadm.go:310] [bootstrap-token] Using token: 3gpkhn.4mhdqu2qxvxm65bw
I0518 21:50:27.446703    1457 out.go:235]     â–ª Configuring RBAC rules ...
I0518 21:50:27.446876    1457 kubeadm.go:310] [bootstrap-token] Configuring bootstrap tokens, cluster-info ConfigMap, RBAC Roles
I0518 21:50:27.446977    1457 kubeadm.go:310] [bootstrap-token] Configured RBAC rules to allow Node Bootstrap tokens to get nodes
I0518 21:50:27.447135    1457 kubeadm.go:310] [bootstrap-token] Configured RBAC rules to allow Node Bootstrap tokens to post CSRs in order for nodes to get long term certificate credentials
I0518 21:50:27.447393    1457 kubeadm.go:310] [bootstrap-token] Configured RBAC rules to allow the csrapprover controller automatically approve CSRs from a Node Bootstrap Token
I0518 21:50:27.447587    1457 kubeadm.go:310] [bootstrap-token] Configured RBAC rules to allow certificate rotation for all node client certificates in the cluster
I0518 21:50:27.447674    1457 kubeadm.go:310] [bootstrap-token] Creating the "cluster-info" ConfigMap in the "kube-public" namespace
I0518 21:50:27.447838    1457 kubeadm.go:310] [kubelet-finalize] Updating "/etc/kubernetes/kubelet.conf" to point to a rotatable kubelet client certificate and key
I0518 21:50:27.447936    1457 kubeadm.go:310] [addons] Applied essential addon: CoreDNS
I0518 21:50:27.447988    1457 kubeadm.go:310] [addons] Applied essential addon: kube-proxy
I0518 21:50:27.447992    1457 kubeadm.go:310] 
I0518 21:50:27.448078    1457 kubeadm.go:310] Your Kubernetes control-plane has initialized successfully!
I0518 21:50:27.448081    1457 kubeadm.go:310] 
I0518 21:50:27.448225    1457 kubeadm.go:310] To start using your cluster, you need to run the following as a regular user:
I0518 21:50:27.448230    1457 kubeadm.go:310] 
I0518 21:50:27.448265    1457 kubeadm.go:310]   mkdir -p $HOME/.kube
I0518 21:50:27.448366    1457 kubeadm.go:310]   sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
I0518 21:50:27.448480    1457 kubeadm.go:310]   sudo chown $(id -u):$(id -g) $HOME/.kube/config
I0518 21:50:27.448487    1457 kubeadm.go:310] 
I0518 21:50:27.448568    1457 kubeadm.go:310] Alternatively, if you are the root user, you can run:
I0518 21:50:27.448596    1457 kubeadm.go:310] 
I0518 21:50:27.448673    1457 kubeadm.go:310]   export KUBECONFIG=/etc/kubernetes/admin.conf
I0518 21:50:27.448680    1457 kubeadm.go:310] 
I0518 21:50:27.448754    1457 kubeadm.go:310] You should now deploy a pod network to the cluster.
I0518 21:50:27.448874    1457 kubeadm.go:310] Run "kubectl apply -f [podnetwork].yaml" with one of the options listed at:
I0518 21:50:27.448970    1457 kubeadm.go:310]   https://kubernetes.io/docs/concepts/cluster-administration/addons/
I0518 21:50:27.448979    1457 kubeadm.go:310] 
I0518 21:50:27.449088    1457 kubeadm.go:310] You can now join any number of control-plane nodes by copying certificate authorities
I0518 21:50:27.449189    1457 kubeadm.go:310] and service account keys on each node and then running the following as root:
I0518 21:50:27.449196    1457 kubeadm.go:310] 
I0518 21:50:27.449298    1457 kubeadm.go:310]   kubeadm join control-plane.minikube.internal:8443 --token 3gpkhn.4mhdqu2qxvxm65bw \
I0518 21:50:27.449444    1457 kubeadm.go:310] 	--discovery-token-ca-cert-hash sha256:cf919a2e10638ae6752b40d5ed334742395f3df46b851be2deb2bca9b4f24f72 \
I0518 21:50:27.449477    1457 kubeadm.go:310] 	--control-plane 
I0518 21:50:27.449484    1457 kubeadm.go:310] 
I0518 21:50:27.449610    1457 kubeadm.go:310] Then you can join any number of worker nodes by running the following on each as root:
I0518 21:50:27.449614    1457 kubeadm.go:310] 
I0518 21:50:27.449737    1457 kubeadm.go:310] kubeadm join control-plane.minikube.internal:8443 --token 3gpkhn.4mhdqu2qxvxm65bw \
I0518 21:50:27.449894    1457 kubeadm.go:310] 	--discovery-token-ca-cert-hash sha256:cf919a2e10638ae6752b40d5ed334742395f3df46b851be2deb2bca9b4f24f72 
I0518 21:50:27.449904    1457 cni.go:84] Creating CNI manager for ""
I0518 21:50:27.449933    1457 cni.go:158] "hyperkit" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I0518 21:50:27.551433    1457 out.go:177] ðŸ”—  Configuring bridge CNI (Container Networking Interface) ...
I0518 21:50:27.592064    1457 ssh_runner.go:195] Run: sudo mkdir -p /etc/cni/net.d
I0518 21:50:27.646699    1457 ssh_runner.go:362] scp memory --> /etc/cni/net.d/1-k8s.conflist (496 bytes)
I0518 21:50:27.737520    1457 ssh_runner.go:195] Run: /bin/bash -c "cat /proc/$(pgrep kube-apiserver)/oom_adj"
I0518 21:50:27.740703    1457 ssh_runner.go:195] Run: sudo /var/lib/minikube/binaries/v1.32.0/kubectl create clusterrolebinding minikube-rbac --clusterrole=cluster-admin --serviceaccount=kube-system:default --kubeconfig=/var/lib/minikube/kubeconfig
I0518 21:50:27.751625    1457 ssh_runner.go:195] Run: sudo /var/lib/minikube/binaries/v1.32.0/kubectl --kubeconfig=/var/lib/minikube/kubeconfig label --overwrite nodes minikube minikube.k8s.io/updated_at=2025_05_18T21_50_27_0700 minikube.k8s.io/version=v1.35.0 minikube.k8s.io/commit=dd5d320e41b5451cdf3c01891bc4e13d189586ed minikube.k8s.io/name=minikube minikube.k8s.io/primary=true
I0518 21:50:27.830497    1457 ops.go:34] apiserver oom_adj: -16
I0518 21:50:29.818498    1457 ssh_runner.go:235] Completed: sudo /var/lib/minikube/binaries/v1.32.0/kubectl create clusterrolebinding minikube-rbac --clusterrole=cluster-admin --serviceaccount=kube-system:default --kubeconfig=/var/lib/minikube/kubeconfig: (2.07771408s)
I0518 21:50:29.818530    1457 kubeadm.go:1113] duration metric: took 2.078065965s to wait for elevateKubeSystemPrivileges
I0518 21:50:29.878576    1457 ssh_runner.go:235] Completed: sudo /var/lib/minikube/binaries/v1.32.0/kubectl --kubeconfig=/var/lib/minikube/kubeconfig label --overwrite nodes minikube minikube.k8s.io/updated_at=2025_05_18T21_50_27_0700 minikube.k8s.io/version=v1.35.0 minikube.k8s.io/commit=dd5d320e41b5451cdf3c01891bc4e13d189586ed minikube.k8s.io/name=minikube minikube.k8s.io/primary=true: (2.126863393s)
I0518 21:50:29.878637    1457 kubeadm.go:394] duration metric: took 38.372526441s to StartCluster
I0518 21:50:29.886800    1457 settings.go:142] acquiring lock: {Name:mkd440b491945aefe5771e361f8f58b5439f01d9 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0518 21:50:29.887155    1457 settings.go:150] Updating kubeconfig:  /Users/christinamanara/.kube/config
I0518 21:50:29.955721    1457 lock.go:35] WriteFile acquiring /Users/christinamanara/.kube/config: {Name:mk758abcd163d59bb7e0efa153067af77c17f5ff Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0518 21:50:29.977573    1457 ssh_runner.go:195] Run: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.32.0/kubectl --kubeconfig=/var/lib/minikube/kubeconfig -n kube-system get configmap coredns -o yaml"
I0518 21:50:29.981153    1457 start.go:235] Will wait 6m0s for node &{Name: IP:192.168.65.3 Port:8443 KubernetesVersion:v1.32.0 ContainerRuntime:docker ControlPlane:true Worker:true}
I0518 21:50:29.984386    1457 config.go:182] Loaded profile config "minikube": Driver=hyperkit, ContainerRuntime=docker, KubernetesVersion=v1.32.0
I0518 21:50:29.999019    1457 addons.go:511] enable addons start: toEnable=map[ambassador:false amd-gpu-device-plugin:false auto-pause:false cloud-spanner:false csi-hostpath-driver:false dashboard:false default-storageclass:true efk:false freshpod:false gcp-auth:false gvisor:false headlamp:false inaccel:false ingress:false ingress-dns:false inspektor-gadget:false istio:false istio-provisioner:false kong:false kubeflow:false kubevirt:false logviewer:false metallb:false metrics-server:false nvidia-device-plugin:false nvidia-driver-installer:false nvidia-gpu-device-plugin:false olm:false pod-security-policy:false portainer:false registry:false registry-aliases:false registry-creds:false storage-provisioner:true storage-provisioner-gluster:false storage-provisioner-rancher:false volcano:false volumesnapshots:false yakd:false]
I0518 21:50:30.035967    1457 out.go:177] ðŸ”Ž  Verifying Kubernetes components...
I0518 21:50:30.043928    1457 addons.go:69] Setting default-storageclass=true in profile "minikube"
I0518 21:50:30.043925    1457 addons.go:69] Setting storage-provisioner=true in profile "minikube"
I0518 21:50:30.046917    1457 addons.go:238] Setting addon storage-provisioner=true in "minikube"
I0518 21:50:30.050143    1457 addons_storage_classes.go:33] enableOrDisableStorageClasses default-storageclass=true on "minikube"
I0518 21:50:30.053409    1457 host.go:66] Checking if "minikube" exists ...
I0518 21:50:30.075308    1457 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0518 21:50:30.121605    1457 main.go:141] libmachine: Found binary path at /Users/christinamanara/.minikube/bin/docker-machine-driver-hyperkit
I0518 21:50:30.130155    1457 main.go:141] libmachine: Launching plugin server for driver hyperkit
I0518 21:50:30.133596    1457 main.go:141] libmachine: Found binary path at /Users/christinamanara/.minikube/bin/docker-machine-driver-hyperkit
I0518 21:50:30.133671    1457 main.go:141] libmachine: Launching plugin server for driver hyperkit
I0518 21:50:30.218823    1457 main.go:141] libmachine: Plugin server listening at address 127.0.0.1:49634
I0518 21:50:30.219016    1457 main.go:141] libmachine: Plugin server listening at address 127.0.0.1:49635
I0518 21:50:30.227546    1457 main.go:141] libmachine: () Calling .GetVersion
I0518 21:50:30.228021    1457 main.go:141] libmachine: () Calling .GetVersion
I0518 21:50:30.258809    1457 main.go:141] libmachine: Using API Version  1
I0518 21:50:30.258818    1457 main.go:141] libmachine: Using API Version  1
I0518 21:50:30.258840    1457 main.go:141] libmachine: () Calling .SetConfigRaw
I0518 21:50:30.258858    1457 main.go:141] libmachine: () Calling .SetConfigRaw
I0518 21:50:30.259417    1457 main.go:141] libmachine: () Calling .GetMachineName
I0518 21:50:30.260043    1457 main.go:141] libmachine: Found binary path at /Users/christinamanara/.minikube/bin/docker-machine-driver-hyperkit
I0518 21:50:30.260074    1457 main.go:141] libmachine: Launching plugin server for driver hyperkit
I0518 21:50:30.262138    1457 main.go:141] libmachine: () Calling .GetMachineName
I0518 21:50:30.262924    1457 main.go:141] libmachine: (minikube) Calling .GetState
I0518 21:50:30.263203    1457 main.go:141] libmachine: (minikube) DBG | exe=/Users/christinamanara/.minikube/bin/docker-machine-driver-hyperkit uid=0
I0518 21:50:30.263337    1457 main.go:141] libmachine: (minikube) DBG | hyperkit pid from json: 1838
I0518 21:50:30.284664    1457 main.go:141] libmachine: Plugin server listening at address 127.0.0.1:49638
I0518 21:50:30.285464    1457 main.go:141] libmachine: () Calling .GetVersion
I0518 21:50:30.286058    1457 main.go:141] libmachine: Using API Version  1
I0518 21:50:30.286077    1457 main.go:141] libmachine: () Calling .SetConfigRaw
I0518 21:50:30.286751    1457 main.go:141] libmachine: () Calling .GetMachineName
I0518 21:50:30.287031    1457 main.go:141] libmachine: (minikube) Calling .GetState
I0518 21:50:30.287232    1457 main.go:141] libmachine: (minikube) DBG | exe=/Users/christinamanara/.minikube/bin/docker-machine-driver-hyperkit uid=0
I0518 21:50:30.287396    1457 main.go:141] libmachine: (minikube) DBG | hyperkit pid from json: 1838
I0518 21:50:30.290106    1457 main.go:141] libmachine: (minikube) Calling .DriverName
I0518 21:50:30.338594    1457 out.go:177]     â–ª Using image gcr.io/k8s-minikube/storage-provisioner:v5
I0518 21:50:30.379300    1457 addons.go:435] installing /etc/kubernetes/addons/storage-provisioner.yaml
I0518 21:50:30.379312    1457 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/storage-provisioner.yaml (2676 bytes)
I0518 21:50:30.379334    1457 main.go:141] libmachine: (minikube) Calling .GetSSHHostname
I0518 21:50:30.380031    1457 main.go:141] libmachine: (minikube) Calling .GetSSHPort
I0518 21:50:30.380377    1457 main.go:141] libmachine: (minikube) Calling .GetSSHKeyPath
I0518 21:50:30.380654    1457 main.go:141] libmachine: (minikube) Calling .GetSSHUsername
I0518 21:50:30.380843    1457 sshutil.go:53] new ssh client: &{IP:192.168.65.3 Port:22 SSHKeyPath:/Users/christinamanara/.minikube/machines/minikube/id_rsa Username:docker}
I0518 21:50:30.718776    1457 addons.go:238] Setting addon default-storageclass=true in "minikube"
I0518 21:50:30.718807    1457 host.go:66] Checking if "minikube" exists ...
I0518 21:50:30.722990    1457 main.go:141] libmachine: Found binary path at /Users/christinamanara/.minikube/bin/docker-machine-driver-hyperkit
I0518 21:50:30.723052    1457 main.go:141] libmachine: Launching plugin server for driver hyperkit
I0518 21:50:30.750979    1457 main.go:141] libmachine: Plugin server listening at address 127.0.0.1:49641
I0518 21:50:30.751757    1457 main.go:141] libmachine: () Calling .GetVersion
I0518 21:50:30.752445    1457 main.go:141] libmachine: Using API Version  1
I0518 21:50:30.752468    1457 main.go:141] libmachine: () Calling .SetConfigRaw
I0518 21:50:30.754563    1457 main.go:141] libmachine: () Calling .GetMachineName
I0518 21:50:30.755273    1457 main.go:141] libmachine: Found binary path at /Users/christinamanara/.minikube/bin/docker-machine-driver-hyperkit
I0518 21:50:30.758189    1457 main.go:141] libmachine: Launching plugin server for driver hyperkit
I0518 21:50:30.793694    1457 main.go:141] libmachine: Plugin server listening at address 127.0.0.1:49643
I0518 21:50:30.795765    1457 main.go:141] libmachine: () Calling .GetVersion
I0518 21:50:30.796906    1457 main.go:141] libmachine: Using API Version  1
I0518 21:50:30.797007    1457 main.go:141] libmachine: () Calling .SetConfigRaw
I0518 21:50:30.797717    1457 main.go:141] libmachine: () Calling .GetMachineName
I0518 21:50:30.797997    1457 main.go:141] libmachine: (minikube) Calling .GetState
I0518 21:50:30.798338    1457 main.go:141] libmachine: (minikube) DBG | exe=/Users/christinamanara/.minikube/bin/docker-machine-driver-hyperkit uid=0
I0518 21:50:30.798497    1457 main.go:141] libmachine: (minikube) DBG | hyperkit pid from json: 1838
I0518 21:50:30.803045    1457 main.go:141] libmachine: (minikube) Calling .DriverName
I0518 21:50:30.803695    1457 addons.go:435] installing /etc/kubernetes/addons/storageclass.yaml
I0518 21:50:30.803704    1457 ssh_runner.go:362] scp storageclass/storageclass.yaml --> /etc/kubernetes/addons/storageclass.yaml (271 bytes)
I0518 21:50:30.804646    1457 main.go:141] libmachine: (minikube) Calling .GetSSHHostname
I0518 21:50:30.804933    1457 main.go:141] libmachine: (minikube) Calling .GetSSHPort
I0518 21:50:30.805235    1457 main.go:141] libmachine: (minikube) Calling .GetSSHKeyPath
I0518 21:50:30.805725    1457 main.go:141] libmachine: (minikube) Calling .GetSSHUsername
I0518 21:50:30.806653    1457 sshutil.go:53] new ssh client: &{IP:192.168.65.3 Port:22 SSHKeyPath:/Users/christinamanara/.minikube/machines/minikube/id_rsa Username:docker}
I0518 21:50:31.058133    1457 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml
I0518 21:50:31.060042    1457 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml
I0518 21:50:31.646843    1457 ssh_runner.go:235] Completed: sudo systemctl daemon-reload: (1.571470524s)
I0518 21:50:31.646997    1457 ssh_runner.go:195] Run: sudo systemctl start kubelet
I0518 21:50:31.647071    1457 ssh_runner.go:235] Completed: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.32.0/kubectl --kubeconfig=/var/lib/minikube/kubeconfig -n kube-system get configmap coredns -o yaml": (1.66942259s)
I0518 21:50:31.666419    1457 ssh_runner.go:195] Run: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.32.0/kubectl --kubeconfig=/var/lib/minikube/kubeconfig -n kube-system get configmap coredns -o yaml | sed -e '/^        forward . \/etc\/resolv.conf.*/i \        hosts {\n           192.168.65.1 host.minikube.internal\n           fallthrough\n        }' -e '/^        errors *$/i \        log' | sudo /var/lib/minikube/binaries/v1.32.0/kubectl --kubeconfig=/var/lib/minikube/kubeconfig replace -f -"
I0518 21:50:34.413648    1457 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml: (3.353489447s)
I0518 21:50:34.413701    1457 ssh_runner.go:235] Completed: sudo systemctl start kubelet: (2.766613009s)
I0518 21:50:34.413767    1457 main.go:141] libmachine: Making call to close driver server
I0518 21:50:34.413781    1457 main.go:141] libmachine: (minikube) Calling .Close
I0518 21:50:34.413776    1457 ssh_runner.go:235] Completed: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.32.0/kubectl --kubeconfig=/var/lib/minikube/kubeconfig -n kube-system get configmap coredns -o yaml | sed -e '/^        forward . \/etc\/resolv.conf.*/i \        hosts {\n           192.168.65.1 host.minikube.internal\n           fallthrough\n        }' -e '/^        errors *$/i \        log' | sudo /var/lib/minikube/binaries/v1.32.0/kubectl --kubeconfig=/var/lib/minikube/kubeconfig replace -f -": (2.747263606s)
I0518 21:50:34.414527    1457 main.go:141] libmachine: Successfully made call to close driver server
I0518 21:50:34.414542    1457 main.go:141] libmachine: (minikube) DBG | Closing plugin on server side
I0518 21:50:34.414555    1457 main.go:141] libmachine: Making call to close connection to plugin binary
I0518 21:50:34.414566    1457 main.go:141] libmachine: Making call to close driver server
I0518 21:50:34.414574    1457 main.go:141] libmachine: (minikube) Calling .Close
I0518 21:50:34.414331    1457 start.go:971] {"host.minikube.internal": 192.168.65.1} host record injected into CoreDNS's ConfigMap
I0518 21:50:34.414597    1457 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml: (3.355615005s)
I0518 21:50:34.414752    1457 main.go:141] libmachine: Making call to close driver server
I0518 21:50:34.414782    1457 main.go:141] libmachine: (minikube) Calling .Close
I0518 21:50:34.415358    1457 main.go:141] libmachine: Successfully made call to close driver server
I0518 21:50:34.415418    1457 main.go:141] libmachine: Making call to close connection to plugin binary
I0518 21:50:34.415618    1457 main.go:141] libmachine: Successfully made call to close driver server
I0518 21:50:34.415637    1457 main.go:141] libmachine: Making call to close connection to plugin binary
I0518 21:50:34.415661    1457 main.go:141] libmachine: Making call to close driver server
I0518 21:50:34.415672    1457 main.go:141] libmachine: (minikube) Calling .Close
I0518 21:50:34.416250    1457 main.go:141] libmachine: Successfully made call to close driver server
I0518 21:50:34.416261    1457 main.go:141] libmachine: Making call to close connection to plugin binary
I0518 21:50:34.418409    1457 api_server.go:52] waiting for apiserver process to appear ...
I0518 21:50:34.418583    1457 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0518 21:50:34.487644    1457 api_server.go:72] duration metric: took 4.503125226s to wait for apiserver process to appear ...
I0518 21:50:34.487657    1457 api_server.go:88] waiting for apiserver healthz status ...
I0518 21:50:34.488074    1457 api_server.go:253] Checking apiserver healthz at https://192.168.65.3:8443/healthz ...
I0518 21:50:34.582016    1457 api_server.go:279] https://192.168.65.3:8443/healthz returned 200:
ok
I0518 21:50:34.600563    1457 api_server.go:141] control plane version: v1.32.0
I0518 21:50:34.600585    1457 api_server.go:131] duration metric: took 112.532685ms to wait for apiserver health ...
I0518 21:50:34.600916    1457 system_pods.go:43] waiting for kube-system pods to appear ...
I0518 21:50:34.640922    1457 main.go:141] libmachine: Making call to close driver server
I0518 21:50:34.640945    1457 main.go:141] libmachine: (minikube) Calling .Close
I0518 21:50:34.641474    1457 main.go:141] libmachine: Successfully made call to close driver server
I0518 21:50:34.641487    1457 main.go:141] libmachine: Making call to close connection to plugin binary
I0518 21:50:34.641516    1457 main.go:141] libmachine: (minikube) DBG | Closing plugin on server side
I0518 21:50:34.671624    1457 system_pods.go:59] 8 kube-system pods found
I0518 21:50:34.671660    1457 system_pods.go:61] "coredns-668d6bf9bc-lks28" [5deef1b4-74f4-4fa9-b7ca-03ab9ba6e7ff] Pending / Ready:ContainersNotReady (containers with unready status: [coredns]) / ContainersReady:ContainersNotReady (containers with unready status: [coredns])
I0518 21:50:34.671669    1457 system_pods.go:61] "coredns-668d6bf9bc-xvfzm" [c16b7723-2009-487c-95c5-f2f5d64ed139] Pending / Ready:ContainersNotReady (containers with unready status: [coredns]) / ContainersReady:ContainersNotReady (containers with unready status: [coredns])
I0518 21:50:34.671676    1457 system_pods.go:61] "etcd-minikube" [53b0be12-9eab-41de-91b7-c6cd7ce6b79b] Running / Ready:ContainersNotReady (containers with unready status: [etcd]) / ContainersReady:ContainersNotReady (containers with unready status: [etcd])
I0518 21:50:34.671679    1457 system_pods.go:61] "kube-apiserver-minikube" [a9da33fb-62e3-4fdf-95a0-83e3939d0787] Running
I0518 21:50:34.671688    1457 system_pods.go:61] "kube-controller-manager-minikube" [8314ec42-3315-4787-8450-42e57e894d3a] Running
I0518 21:50:34.671692    1457 system_pods.go:61] "kube-proxy-bwql9" [19e10992-ba94-4831-be20-7cff1c72380b] Pending / Ready:ContainersNotReady (containers with unready status: [kube-proxy]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-proxy])
I0518 21:50:34.671695    1457 system_pods.go:61] "kube-scheduler-minikube" [4c89b285-e0ee-43fc-a031-caff24a90df0] Running
I0518 21:50:34.671698    1457 system_pods.go:61] "storage-provisioner" [63405c88-f9db-4afd-b1c3-eb2ba7d44609] Pending
I0518 21:50:34.671703    1457 system_pods.go:74] duration metric: took 70.778478ms to wait for pod list to return data ...
I0518 21:50:34.671715    1457 kubeadm.go:582] duration metric: took 4.68719861s to wait for: map[apiserver:true system_pods:true]
I0518 21:50:34.671727    1457 node_conditions.go:102] verifying NodePressure condition ...
I0518 21:50:34.681881    1457 out.go:177] ðŸŒŸ  Enabled addons: storage-provisioner, default-storageclass
I0518 21:50:34.719171    1457 addons.go:514] duration metric: took 4.741463117s for enable addons: enabled=[storage-provisioner default-storageclass]
I0518 21:50:34.840768    1457 node_conditions.go:122] node storage ephemeral capacity is 17734596Ki
I0518 21:50:34.840788    1457 node_conditions.go:123] node cpu capacity is 2
I0518 21:50:34.841243    1457 node_conditions.go:105] duration metric: took 169.504197ms to run NodePressure ...
I0518 21:50:34.841258    1457 start.go:241] waiting for startup goroutines ...
I0518 21:50:35.048018    1457 kapi.go:214] "coredns" deployment in "kube-system" namespace and "minikube" context rescaled to 1 replicas
I0518 21:50:35.048047    1457 start.go:246] waiting for cluster config update ...
I0518 21:50:35.048066    1457 start.go:255] writing updated cluster config ...
I0518 21:50:35.050975    1457 ssh_runner.go:195] Run: rm -f paused
I0518 21:50:35.601700    1457 start.go:600] kubectl: 1.32.2, cluster: 1.32.0 (minor skew: 0)
I0518 21:50:35.625744    1457 out.go:177] ðŸ„  Done! kubectl is now configured to use "minikube" cluster and "default" namespace by default


==> Docker <==
May 18 18:50:45 minikube dockerd[1325]: time="2025-05-18T18:50:45.425365933Z" level=info msg="loading plugin \"io.containerd.internal.v1.shutdown\"..." runtime=io.containerd.runc.v2 type=io.containerd.internal.v1
May 18 18:50:45 minikube dockerd[1325]: time="2025-05-18T18:50:45.425901212Z" level=info msg="loading plugin \"io.containerd.ttrpc.v1.task\"..." runtime=io.containerd.runc.v2 type=io.containerd.ttrpc.v1
May 18 18:50:45 minikube dockerd[1325]: time="2025-05-18T18:50:45.427482798Z" level=info msg="loading plugin \"io.containerd.ttrpc.v1.pause\"..." runtime=io.containerd.runc.v2 type=io.containerd.ttrpc.v1
May 18 18:50:45 minikube cri-dockerd[1218]: time="2025-05-18T18:50:45Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/d0f3171381e3f2efbac70687e770ce622b5fe75e5e9b91ad8058a24d78a6b448/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
May 18 18:50:48 minikube dockerd[1319]: time="2025-05-18T18:50:48.131548759Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n" spanID=2e013b37f386dd6b traceID=51249458b11b288283f406d833ec7157
May 18 18:50:48 minikube dockerd[1319]: time="2025-05-18T18:50:48.131734721Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
May 18 18:51:04 minikube dockerd[1319]: time="2025-05-18T18:51:04.818806007Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n" spanID=b05f7068b0cb0ab5 traceID=b82c52c9ae26a99e742f6e6da4c86d4a
May 18 18:51:04 minikube dockerd[1319]: time="2025-05-18T18:51:04.818955810Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
May 18 18:51:08 minikube dockerd[1325]: time="2025-05-18T18:51:08.341286581Z" level=info msg="shim disconnected" id=82267f62abcaa0858d5bbaac436b74c84654ffa1882e8d47e4a650c8b981d229 namespace=moby
May 18 18:51:08 minikube dockerd[1325]: time="2025-05-18T18:51:08.341824124Z" level=warning msg="cleaning up after shim disconnected" id=82267f62abcaa0858d5bbaac436b74c84654ffa1882e8d47e4a650c8b981d229 namespace=moby
May 18 18:51:08 minikube dockerd[1325]: time="2025-05-18T18:51:08.352819104Z" level=info msg="cleaning up dead shim" namespace=moby
May 18 18:51:08 minikube dockerd[1319]: time="2025-05-18T18:51:08.360902438Z" level=info msg="ignoring event" container=82267f62abcaa0858d5bbaac436b74c84654ffa1882e8d47e4a650c8b981d229 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
May 18 18:51:09 minikube dockerd[1325]: time="2025-05-18T18:51:09.326822020Z" level=info msg="loading plugin \"io.containerd.event.v1.publisher\"..." runtime=io.containerd.runc.v2 type=io.containerd.event.v1
May 18 18:51:09 minikube dockerd[1325]: time="2025-05-18T18:51:09.329292660Z" level=info msg="loading plugin \"io.containerd.internal.v1.shutdown\"..." runtime=io.containerd.runc.v2 type=io.containerd.internal.v1
May 18 18:51:09 minikube dockerd[1325]: time="2025-05-18T18:51:09.329474043Z" level=info msg="loading plugin \"io.containerd.ttrpc.v1.task\"..." runtime=io.containerd.runc.v2 type=io.containerd.ttrpc.v1
May 18 18:51:09 minikube dockerd[1325]: time="2025-05-18T18:51:09.329831415Z" level=info msg="loading plugin \"io.containerd.ttrpc.v1.pause\"..." runtime=io.containerd.runc.v2 type=io.containerd.ttrpc.v1
May 18 18:51:31 minikube dockerd[1319]: time="2025-05-18T18:51:31.190480224Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n" spanID=e5926e4c5ed45e58 traceID=c945ea2e51800b52d89e8d6f8e5c6d87
May 18 18:51:31 minikube dockerd[1319]: time="2025-05-18T18:51:31.192033209Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
May 18 18:52:21 minikube dockerd[1319]: time="2025-05-18T18:52:21.265306018Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n" spanID=ca2e828a587a444a traceID=5472d69f86f6cceeb79fc2c118d6bfb7
May 18 18:52:21 minikube dockerd[1319]: time="2025-05-18T18:52:21.265498008Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
May 18 18:53:55 minikube dockerd[1319]: time="2025-05-18T18:53:55.200751121Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n" spanID=e424c0e94ee67122 traceID=0be5436ad869fb95439caab4e482a908
May 18 18:53:55 minikube dockerd[1319]: time="2025-05-18T18:53:55.201253609Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
May 18 18:56:39 minikube dockerd[1319]: time="2025-05-18T18:56:39.576652212Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n" spanID=09e52429b3934ed6 traceID=ff5977c9c8ed82be5b58a8b0bccdb057
May 18 18:56:39 minikube dockerd[1319]: time="2025-05-18T18:56:39.578346779Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
May 18 19:01:47 minikube dockerd[1319]: time="2025-05-18T19:01:47.236807294Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n" spanID=0e43acec6c11c6c6 traceID=982948e7f7fb5f5b16554915a0c3bbc1
May 18 19:01:47 minikube dockerd[1319]: time="2025-05-18T19:01:47.239239789Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
May 18 19:02:26 minikube dockerd[1325]: time="2025-05-18T19:02:26.374220601Z" level=info msg="shim disconnected" id=d0f3171381e3f2efbac70687e770ce622b5fe75e5e9b91ad8058a24d78a6b448 namespace=moby
May 18 19:02:26 minikube dockerd[1325]: time="2025-05-18T19:02:26.374511303Z" level=warning msg="cleaning up after shim disconnected" id=d0f3171381e3f2efbac70687e770ce622b5fe75e5e9b91ad8058a24d78a6b448 namespace=moby
May 18 19:02:26 minikube dockerd[1325]: time="2025-05-18T19:02:26.374574938Z" level=info msg="cleaning up dead shim" namespace=moby
May 18 19:02:26 minikube dockerd[1319]: time="2025-05-18T19:02:26.376892868Z" level=info msg="ignoring event" container=d0f3171381e3f2efbac70687e770ce622b5fe75e5e9b91ad8058a24d78a6b448 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
May 18 19:02:26 minikube dockerd[1325]: time="2025-05-18T19:02:26.646830859Z" level=info msg="loading plugin \"io.containerd.event.v1.publisher\"..." runtime=io.containerd.runc.v2 type=io.containerd.event.v1
May 18 19:02:26 minikube dockerd[1325]: time="2025-05-18T19:02:26.647444132Z" level=info msg="loading plugin \"io.containerd.internal.v1.shutdown\"..." runtime=io.containerd.runc.v2 type=io.containerd.internal.v1
May 18 19:02:26 minikube dockerd[1325]: time="2025-05-18T19:02:26.647481894Z" level=info msg="loading plugin \"io.containerd.ttrpc.v1.task\"..." runtime=io.containerd.runc.v2 type=io.containerd.ttrpc.v1
May 18 19:02:26 minikube dockerd[1325]: time="2025-05-18T19:02:26.648776670Z" level=info msg="loading plugin \"io.containerd.ttrpc.v1.pause\"..." runtime=io.containerd.runc.v2 type=io.containerd.ttrpc.v1
May 18 19:02:27 minikube cri-dockerd[1218]: time="2025-05-18T19:02:27Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/302c886d90e9da3a269c01ebff1997a5434653060e07b89a0ab0424fe8971155/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
May 18 19:02:29 minikube dockerd[1319]: time="2025-05-18T19:02:29.200873215Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n" spanID=1b835756c8ef9e06 traceID=c8e18a4349efc6dc4e128ca00b01ff80
May 18 19:02:29 minikube dockerd[1319]: time="2025-05-18T19:02:29.201162133Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
May 18 19:02:46 minikube dockerd[1319]: time="2025-05-18T19:02:46.831855306Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n" spanID=73f0b31e5f0dd91b traceID=ae1ce0f74a4e28fb9cb99aac547b0717
May 18 19:02:46 minikube dockerd[1319]: time="2025-05-18T19:02:46.832883207Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
May 18 19:03:15 minikube dockerd[1319]: time="2025-05-18T19:03:15.918654187Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n" spanID=00da4e75c917be7c traceID=c8f5aab1a94aea065a698e46f4bfbf1d
May 18 19:03:15 minikube dockerd[1319]: time="2025-05-18T19:03:15.918806094Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
May 18 19:03:58 minikube dockerd[1319]: time="2025-05-18T19:03:58.818428981Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n" spanID=34f623a3e7d6765c traceID=e01eb7012152660d3cc09825c1fdf4f7
May 18 19:03:58 minikube dockerd[1319]: time="2025-05-18T19:03:58.818536524Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
May 18 19:05:23 minikube dockerd[1319]: time="2025-05-18T19:05:23.960939488Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n" spanID=158d510ee3f89463 traceID=25c3ef1b05420d07bf866fad95607daa
May 18 19:05:23 minikube dockerd[1319]: time="2025-05-18T19:05:23.980435438Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
May 18 19:06:44 minikube dockerd[1319]: time="2025-05-18T19:06:44.215461522Z" level=info msg="ignoring event" container=302c886d90e9da3a269c01ebff1997a5434653060e07b89a0ab0424fe8971155 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
May 18 19:06:44 minikube dockerd[1325]: time="2025-05-18T19:06:44.223343348Z" level=info msg="shim disconnected" id=302c886d90e9da3a269c01ebff1997a5434653060e07b89a0ab0424fe8971155 namespace=moby
May 18 19:06:44 minikube dockerd[1325]: time="2025-05-18T19:06:44.240440846Z" level=warning msg="cleaning up after shim disconnected" id=302c886d90e9da3a269c01ebff1997a5434653060e07b89a0ab0424fe8971155 namespace=moby
May 18 19:06:44 minikube dockerd[1325]: time="2025-05-18T19:06:44.241293251Z" level=info msg="cleaning up dead shim" namespace=moby
May 18 19:06:45 minikube dockerd[1325]: time="2025-05-18T19:06:45.488475034Z" level=info msg="loading plugin \"io.containerd.event.v1.publisher\"..." runtime=io.containerd.runc.v2 type=io.containerd.event.v1
May 18 19:06:45 minikube dockerd[1325]: time="2025-05-18T19:06:45.492419681Z" level=info msg="loading plugin \"io.containerd.internal.v1.shutdown\"..." runtime=io.containerd.runc.v2 type=io.containerd.internal.v1
May 18 19:06:45 minikube dockerd[1325]: time="2025-05-18T19:06:45.500435726Z" level=info msg="loading plugin \"io.containerd.ttrpc.v1.task\"..." runtime=io.containerd.runc.v2 type=io.containerd.ttrpc.v1
May 18 19:06:45 minikube dockerd[1325]: time="2025-05-18T19:06:45.503310876Z" level=info msg="loading plugin \"io.containerd.ttrpc.v1.pause\"..." runtime=io.containerd.runc.v2 type=io.containerd.ttrpc.v1
May 18 19:06:46 minikube cri-dockerd[1218]: time="2025-05-18T19:06:46Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/2f8d82c14c38236050d4328624cb7398d2aeea072b569438bd50a4a6d8c90cb6/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
May 18 19:06:48 minikube dockerd[1319]: time="2025-05-18T19:06:48.347810838Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n" spanID=328f00d3835d1d45 traceID=e7578a501f6678e848516b1f8f664fce
May 18 19:06:48 minikube dockerd[1319]: time="2025-05-18T19:06:48.349518094Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
May 18 19:07:03 minikube dockerd[1319]: time="2025-05-18T19:07:03.783986017Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n" spanID=8a5b6927652b505d traceID=538918126711374fac69b2eaf1c57342
May 18 19:07:03 minikube dockerd[1319]: time="2025-05-18T19:07:03.784247186Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
May 18 19:07:33 minikube dockerd[1319]: time="2025-05-18T19:07:33.778646778Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n" spanID=c33a8db57409ee81 traceID=86e0aaa0652a867947d9a278effe10f2
May 18 19:07:33 minikube dockerd[1319]: time="2025-05-18T19:07:33.778800691Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"


==> container status <==
CONTAINER           IMAGE               CREATED             STATE               NAME                      ATTEMPT             POD ID              POD
bd25d7d887681       6e38f40d628db       16 minutes ago      Running             storage-provisioner       1                   62ca97d46768d       storage-provisioner
9e19a4dfbd209       c69fa2e9cbf5f       17 minutes ago      Running             coredns                   0                   5a60d07e64abd       coredns-668d6bf9bc-xvfzm
82267f62abcaa       6e38f40d628db       17 minutes ago      Exited              storage-provisioner       0                   62ca97d46768d       storage-provisioner
73e5fcb63e07a       040f9f8aac8cd       17 minutes ago      Running             kube-proxy                0                   55efa2e029b84       kube-proxy-bwql9
5c26c571c3c97       a9e7e6b294baf       17 minutes ago      Running             etcd                      0                   a16ab3742e8e0       etcd-minikube
0b7d191bcbf18       a389e107f4ff1       17 minutes ago      Running             kube-scheduler            0                   94545ed7b5598       kube-scheduler-minikube
47c6d5cffd29c       8cab3d2a8bd0f       17 minutes ago      Running             kube-controller-manager   0                   2b645b1760787       kube-controller-manager-minikube
0bb41e721c6b4       c2e17b8d0f4a3       17 minutes ago      Running             kube-apiserver            0                   878a9527d4063       kube-apiserver-minikube


==> coredns [9e19a4dfbd20] <==
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[WARNING] plugin/kubernetes: starting server with unsynced Kubernetes API
.:53
[INFO] plugin/reload: Running configuration SHA512 = 71e303ab2fc034ccd49bcca7d97642fb0604074902c3333ab8f386bb9061b47655b07ee6c5fed328013f70dc3a0894a64ca846e6bc53e8b5627b776749aaf25c
CoreDNS-1.11.3
linux/amd64, go1.21.11, a6338e9
[INFO] 127.0.0.1:56230 - 59670 "HINFO IN 5810746751081708336.5265891451145378271. udp 57 false 512" NXDOMAIN qr,rd,ra 132 0.032538663s
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.29.3/tools/cache/reflector.go:229: failed to list *v1.Namespace: Get "https://10.96.0.1:443/api/v1/namespaces?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: i/o timeout
[INFO] plugin/kubernetes: Trace[615146703]: "Reflector ListAndWatch" name:pkg/mod/k8s.io/client-go@v0.29.3/tools/cache/reflector.go:229 (18-May-2025 18:50:38.722) (total time: 30003ms):
Trace[615146703]: ---"Objects listed" error:Get "https://10.96.0.1:443/api/v1/namespaces?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: i/o timeout 30001ms (18:51:08.723)
Trace[615146703]: [30.003071529s] [30.003071529s] END
[ERROR] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.29.3/tools/cache/reflector.go:229: Failed to watch *v1.Namespace: failed to list *v1.Namespace: Get "https://10.96.0.1:443/api/v1/namespaces?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: i/o timeout
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.29.3/tools/cache/reflector.go:229: failed to list *v1.Service: Get "https://10.96.0.1:443/api/v1/services?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: i/o timeout
[INFO] plugin/kubernetes: Trace[1456342267]: "Reflector ListAndWatch" name:pkg/mod/k8s.io/client-go@v0.29.3/tools/cache/reflector.go:229 (18-May-2025 18:50:38.724) (total time: 30007ms):
Trace[1456342267]: ---"Objects listed" error:Get "https://10.96.0.1:443/api/v1/services?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: i/o timeout 30004ms (18:51:08.729)
Trace[1456342267]: [30.007019883s] [30.007019883s] END
[ERROR] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.29.3/tools/cache/reflector.go:229: Failed to watch *v1.Service: failed to list *v1.Service: Get "https://10.96.0.1:443/api/v1/services?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: i/o timeout
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.29.3/tools/cache/reflector.go:229: failed to list *v1.EndpointSlice: Get "https://10.96.0.1:443/apis/discovery.k8s.io/v1/endpointslices?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: i/o timeout
[INFO] plugin/kubernetes: Trace[1020311758]: "Reflector ListAndWatch" name:pkg/mod/k8s.io/client-go@v0.29.3/tools/cache/reflector.go:229 (18-May-2025 18:50:38.725) (total time: 30016ms):
Trace[1020311758]: ---"Objects listed" error:Get "https://10.96.0.1:443/apis/discovery.k8s.io/v1/endpointslices?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: i/o timeout 30015ms (18:51:08.740)
Trace[1020311758]: [30.016552704s] [30.016552704s] END
[ERROR] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.29.3/tools/cache/reflector.go:229: Failed to watch *v1.EndpointSlice: failed to list *v1.EndpointSlice: Get "https://10.96.0.1:443/apis/discovery.k8s.io/v1/endpointslices?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: i/o timeout


==> describe nodes <==
Name:               minikube
Roles:              control-plane
Labels:             beta.kubernetes.io/arch=amd64
                    beta.kubernetes.io/os=linux
                    kubernetes.io/arch=amd64
                    kubernetes.io/hostname=minikube
                    kubernetes.io/os=linux
                    minikube.k8s.io/commit=dd5d320e41b5451cdf3c01891bc4e13d189586ed
                    minikube.k8s.io/name=minikube
                    minikube.k8s.io/primary=true
                    minikube.k8s.io/updated_at=2025_05_18T21_50_27_0700
                    minikube.k8s.io/version=v1.35.0
                    node-role.kubernetes.io/control-plane=
                    node.kubernetes.io/exclude-from-external-load-balancers=
Annotations:        kubeadm.alpha.kubernetes.io/cri-socket: unix:///var/run/cri-dockerd.sock
                    node.alpha.kubernetes.io/ttl: 0
                    volumes.kubernetes.io/controller-managed-attach-detach: true
CreationTimestamp:  Sun, 18 May 2025 18:50:16 +0000
Taints:             <none>
Unschedulable:      false
Lease:
  HolderIdentity:  minikube
  AcquireTime:     <unset>
  RenewTime:       Sun, 18 May 2025 19:07:43 +0000
Conditions:
  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message
  ----             ------  -----------------                 ------------------                ------                       -------
  MemoryPressure   False   Sun, 18 May 2025 19:02:02 +0000   Sun, 18 May 2025 18:50:10 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available
  DiskPressure     False   Sun, 18 May 2025 19:02:02 +0000   Sun, 18 May 2025 18:50:10 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure
  PIDPressure      False   Sun, 18 May 2025 19:02:02 +0000   Sun, 18 May 2025 18:50:10 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available
  Ready            True    Sun, 18 May 2025 19:02:02 +0000   Sun, 18 May 2025 18:50:31 +0000   KubeletReady                 kubelet is posting ready status
Addresses:
  InternalIP:  192.168.65.3
  Hostname:    minikube
Capacity:
  cpu:                2
  ephemeral-storage:  17734596Ki
  hugepages-2Mi:      0
  memory:             2164336Ki
  pods:               110
Allocatable:
  cpu:                2
  ephemeral-storage:  17734596Ki
  hugepages-2Mi:      0
  memory:             2164336Ki
  pods:               110
System Info:
  Machine ID:                 24b07fdb6486467e9a385934696f4621
  System UUID:                73344ed2-0000-0000-8665-9b3f7d008e6a
  Boot ID:                    9fd05ef0-262e-44d5-929e-5fbd2b794a6f
  Kernel Version:             5.10.207
  OS Image:                   Buildroot 2023.02.9
  Operating System:           linux
  Architecture:               amd64
  Container Runtime Version:  docker://27.4.0
  Kubelet Version:            v1.32.0
  Kube-Proxy Version:         v1.32.0
PodCIDR:                      10.244.0.0/24
PodCIDRs:                     10.244.0.0/24
Non-terminated Pods:          (8 in total)
  Namespace                   Name                                CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age
  ---------                   ----                                ------------  ----------  ---------------  -------------  ---
  default                     thesis-backend-74b5dc74d7-f52vp     0 (0%)        0 (0%)      0 (0%)           0 (0%)         68s
  kube-system                 coredns-668d6bf9bc-xvfzm            100m (5%)     0 (0%)      70Mi (3%)        170Mi (8%)     17m
  kube-system                 etcd-minikube                       100m (5%)     0 (0%)      100Mi (4%)       0 (0%)         17m
  kube-system                 kube-apiserver-minikube             250m (12%)    0 (0%)      0 (0%)           0 (0%)         17m
  kube-system                 kube-controller-manager-minikube    200m (10%)    0 (0%)      0 (0%)           0 (0%)         17m
  kube-system                 kube-proxy-bwql9                    0 (0%)        0 (0%)      0 (0%)           0 (0%)         17m
  kube-system                 kube-scheduler-minikube             100m (5%)     0 (0%)      0 (0%)           0 (0%)         17m
  kube-system                 storage-provisioner                 0 (0%)        0 (0%)      0 (0%)           0 (0%)         17m
Allocated resources:
  (Total limits may be over 100 percent, i.e., overcommitted.)
  Resource           Requests    Limits
  --------           --------    ------
  cpu                750m (37%)  0 (0%)
  memory             170Mi (8%)  170Mi (8%)
  ephemeral-storage  0 (0%)      0 (0%)
  hugepages-2Mi      0 (0%)      0 (0%)
Events:
  Type    Reason                   Age                From             Message
  ----    ------                   ----               ----             -------
  Normal  Starting                 17m                kube-proxy       
  Normal  NodeHasSufficientMemory  17m (x8 over 17m)  kubelet          Node minikube status is now: NodeHasSufficientMemory
  Normal  NodeHasNoDiskPressure    17m (x8 over 17m)  kubelet          Node minikube status is now: NodeHasNoDiskPressure
  Normal  NodeHasSufficientPID     17m (x7 over 17m)  kubelet          Node minikube status is now: NodeHasSufficientPID
  Normal  NodeAllocatableEnforced  17m                kubelet          Updated Node Allocatable limit across pods
  Normal  Starting                 17m                kubelet          Starting kubelet.
  Normal  NodeAllocatableEnforced  17m                kubelet          Updated Node Allocatable limit across pods
  Normal  NodeHasSufficientMemory  17m                kubelet          Node minikube status is now: NodeHasSufficientMemory
  Normal  NodeHasNoDiskPressure    17m                kubelet          Node minikube status is now: NodeHasNoDiskPressure
  Normal  NodeHasSufficientPID     17m                kubelet          Node minikube status is now: NodeHasSufficientPID
  Normal  RegisteredNode           17m                node-controller  Node minikube event: Registered Node minikube in Controller
  Normal  NodeReady                17m                kubelet          Node minikube status is now: NodeReady


==> dmesg <==
[May18 18:48] ERROR: earlyprintk= earlyser already used
[  +0.000000] You have booted with nomodeset. This means your GPU drivers are DISABLED
[  +0.000001] Any video related functionality will be severely degraded, and you may not even be able to suspend the system properly
[  +0.000001] Unless you actually understand what nomodeset does, you should reboot without enabling it
[  +0.088012] ACPI BIOS Warning (bug): Incorrect checksum in table [DSDT] - 0xBE, should be 0x1B (20200925/tbprint-173)
[  +0.008053] RETBleed: WARNING: Spectre v2 mitigation leaves CPU vulnerable to RETBleed attacks, data leaks possible!
[ +20.088243] ACPI Error: Could not enable RealTimeClock event (20200925/evxfevnt-182)
[  +0.000005] ACPI Warning: Could not enable fixed event - RealTimeClock (4) (20200925/evxface-618)
[  +0.027840] platform regulatory.0: Direct firmware load for regulatory.db failed with error -2
[May18 18:49] systemd-fstab-generator[127]: Ignoring "noauto" option for root device
[  +2.738185] NFSD: Using /var/lib/nfs/v4recovery as the NFSv4 state recovery directory
[  +0.000011] NFSD: unable to find recovery directory /var/lib/nfs/v4recovery
[  +0.000002] NFSD: Unable to initialize client recovery tracking! (-2)
[  +3.974884] clocksource: timekeeping watchdog on CPU0: Marking clocksource 'tsc' as unstable because the skew is too large:
[  +0.000002] clocksource:                       'hpet' wd_now: 158c68da wd_last: 14fc6ed2 mask: ffffffff
[  +0.000001] clocksource:                       'tsc' cs_now: 684420bdaa cs_last: 67f5ee5d42 mask: ffffffffffffffff
[  +0.000038] TSC found unstable after boot, most likely due to broken BIOS. Use 'tsc=unstable'.
[  +0.001485] clocksource: Checking clocksource tsc synchronization from CPU 1.
[  +1.317947] systemd-fstab-generator[505]: Ignoring "noauto" option for root device
[  +0.609045] systemd-fstab-generator[523]: Ignoring "noauto" option for root device
[  +4.660776] systemd-fstab-generator[876]: Ignoring "noauto" option for root device
[  +0.277919] kauditd_printk_skb: 69 callbacks suppressed
[  +0.971757] systemd-fstab-generator[917]: Ignoring "noauto" option for root device
[  +0.458934] systemd-fstab-generator[929]: Ignoring "noauto" option for root device
[  +0.887292] systemd-fstab-generator[944]: Ignoring "noauto" option for root device
[  +3.052787] kauditd_printk_skb: 78 callbacks suppressed
[  +2.325986] systemd-fstab-generator[1171]: Ignoring "noauto" option for root device
[  +0.524127] hrtimer: interrupt took 13222593 ns
[  +0.318046] systemd-fstab-generator[1182]: Ignoring "noauto" option for root device
[  +0.599104] systemd-fstab-generator[1195]: Ignoring "noauto" option for root device
[  +0.583736] systemd-fstab-generator[1210]: Ignoring "noauto" option for root device
[ +18.821949] systemd-fstab-generator[1311]: Ignoring "noauto" option for root device
[  +0.162525] kauditd_printk_skb: 118 callbacks suppressed
[  +4.177438] systemd-fstab-generator[1570]: Ignoring "noauto" option for root device
[May18 18:50] systemd-fstab-generator[1734]: Ignoring "noauto" option for root device
[  +0.232849] kauditd_printk_skb: 74 callbacks suppressed
[  +5.496875] kauditd_printk_skb: 52 callbacks suppressed
[ +18.158195] systemd-fstab-generator[2159]: Ignoring "noauto" option for root device
[  +4.822880] systemd-fstab-generator[2229]: Ignoring "noauto" option for root device
[  +4.228860] kauditd_printk_skb: 34 callbacks suppressed
[  +9.975809] kauditd_printk_skb: 57 callbacks suppressed
[May18 18:51] kauditd_printk_skb: 8 callbacks suppressed
[May18 19:06] kauditd_printk_skb: 15 callbacks suppressed
[  +5.417449] kauditd_printk_skb: 13 callbacks suppressed


==> etcd [5c26c571c3c9] <==
{"level":"warn","ts":"2025-05-18T18:50:32.288879Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"198.624237ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/serviceaccounts/kube-system/deployment-controller\" limit:1 ","response":"range_response_count:1 size:207"}
{"level":"info","ts":"2025-05-18T18:50:32.328864Z","caller":"traceutil/trace.go:171","msg":"trace[777597719] range","detail":"{range_begin:/registry/serviceaccounts/kube-system/deployment-controller; range_end:; response_count:1; response_revision:339; }","duration":"243.14421ms","start":"2025-05-18T18:50:32.085689Z","end":"2025-05-18T18:50:32.328835Z","steps":["trace[777597719] 'agreement among raft nodes before linearized reading'  (duration: 202.899386ms)"],"step_count":1}
{"level":"warn","ts":"2025-05-18T18:50:32.289282Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"211.621864ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/serviceaccounts/kube-system/clusterrole-aggregation-controller\" limit:1 ","response":"range_response_count:1 size:234"}
{"level":"info","ts":"2025-05-18T18:50:32.334593Z","caller":"traceutil/trace.go:171","msg":"trace[143608170] range","detail":"{range_begin:/registry/serviceaccounts/kube-system/clusterrole-aggregation-controller; range_end:; response_count:1; response_revision:339; }","duration":"327.166525ms","start":"2025-05-18T18:50:32.007259Z","end":"2025-05-18T18:50:32.334553Z","steps":["trace[143608170] 'agreement among raft nodes before linearized reading'  (duration: 281.744638ms)"],"step_count":1}
{"level":"warn","ts":"2025-05-18T18:50:32.334713Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2025-05-18T18:50:32.007023Z","time spent":"327.649498ms","remote":"127.0.0.1:60208","response type":"/etcdserverpb.KV/Range","request count":0,"request size":76,"response count":1,"response size":256,"request content":"key:\"/registry/serviceaccounts/kube-system/clusterrole-aggregation-controller\" limit:1 "}
{"level":"warn","ts":"2025-05-18T18:50:32.289524Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"227.922024ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/serviceaccounts/kube-system/bootstrap-signer\" limit:1 ","response":"range_response_count:1 size:197"}
{"level":"info","ts":"2025-05-18T18:50:32.335233Z","caller":"traceutil/trace.go:171","msg":"trace[1147886012] range","detail":"{range_begin:/registry/serviceaccounts/kube-system/bootstrap-signer; range_end:; response_count:1; response_revision:339; }","duration":"273.668747ms","start":"2025-05-18T18:50:32.061535Z","end":"2025-05-18T18:50:32.335204Z","steps":["trace[1147886012] 'agreement among raft nodes before linearized reading'  (duration: 227.798941ms)"],"step_count":1}
{"level":"warn","ts":"2025-05-18T18:50:32.289801Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"246.082971ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/serviceaccounts/kube-system/endpoint-controller\" limit:1 ","response":"range_response_count:1 size:203"}
{"level":"info","ts":"2025-05-18T18:50:32.353909Z","caller":"traceutil/trace.go:171","msg":"trace[460832790] range","detail":"{range_begin:/registry/serviceaccounts/kube-system/endpoint-controller; range_end:; response_count:1; response_revision:339; }","duration":"310.199864ms","start":"2025-05-18T18:50:32.043667Z","end":"2025-05-18T18:50:32.353818Z","steps":["trace[460832790] 'agreement among raft nodes before linearized reading'  (duration: 245.946743ms)"],"step_count":1}
{"level":"warn","ts":"2025-05-18T18:50:32.354183Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2025-05-18T18:50:32.043647Z","time spent":"310.488751ms","remote":"127.0.0.1:60208","response type":"/etcdserverpb.KV/Range","request count":0,"request size":61,"response count":1,"response size":225,"request content":"key:\"/registry/serviceaccounts/kube-system/endpoint-controller\" limit:1 "}
{"level":"warn","ts":"2025-05-18T18:50:32.297865Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"254.503417ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/serviceaccounts/kube-system/certificate-controller\" limit:1 ","response":"range_response_count:1 size:209"}
{"level":"info","ts":"2025-05-18T18:50:32.367601Z","caller":"traceutil/trace.go:171","msg":"trace[571661] range","detail":"{range_begin:/registry/serviceaccounts/kube-system/certificate-controller; range_end:; response_count:1; response_revision:339; }","duration":"324.285323ms","start":"2025-05-18T18:50:32.043277Z","end":"2025-05-18T18:50:32.367562Z","steps":["trace[571661] 'agreement among raft nodes before linearized reading'  (duration: 246.567537ms)"],"step_count":1}
{"level":"warn","ts":"2025-05-18T18:50:32.371161Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2025-05-18T18:50:32.043251Z","time spent":"327.857524ms","remote":"127.0.0.1:60208","response type":"/etcdserverpb.KV/Range","request count":0,"request size":64,"response count":1,"response size":231,"request content":"key:\"/registry/serviceaccounts/kube-system/certificate-controller\" limit:1 "}
{"level":"warn","ts":"2025-05-18T18:50:32.298255Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"255.056893ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2025-05-18T18:50:32.388487Z","caller":"traceutil/trace.go:171","msg":"trace[2136578739] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:339; }","duration":"345.339522ms","start":"2025-05-18T18:50:32.042966Z","end":"2025-05-18T18:50:32.388305Z","steps":["trace[2136578739] 'agreement among raft nodes before linearized reading'  (duration: 255.139545ms)"],"step_count":1}
{"level":"warn","ts":"2025-05-18T18:50:32.388583Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2025-05-18T18:50:32.042880Z","time spent":"345.659978ms","remote":"127.0.0.1:60018","response type":"/etcdserverpb.KV/Range","request count":0,"request size":18,"response count":0,"response size":27,"request content":"key:\"/registry/health\" "}
{"level":"info","ts":"2025-05-18T18:50:33.244137Z","caller":"traceutil/trace.go:171","msg":"trace[1171536334] transaction","detail":"{read_only:false; response_revision:350; number_of_response:1; }","duration":"114.191332ms","start":"2025-05-18T18:50:33.129854Z","end":"2025-05-18T18:50:33.243976Z","steps":["trace[1171536334] 'process raft request'  (duration: 98.700779ms)"],"step_count":1}
{"level":"info","ts":"2025-05-18T18:50:33.245192Z","caller":"traceutil/trace.go:171","msg":"trace[1106531187] transaction","detail":"{read_only:false; response_revision:347; number_of_response:1; }","duration":"141.606084ms","start":"2025-05-18T18:50:33.103552Z","end":"2025-05-18T18:50:33.245158Z","steps":["trace[1106531187] 'process raft request'  (duration: 61.408906ms)","trace[1106531187] 'compare'  (duration: 62.580659ms)"],"step_count":2}
{"level":"info","ts":"2025-05-18T18:50:33.263216Z","caller":"traceutil/trace.go:171","msg":"trace[1963994729] transaction","detail":"{read_only:false; response_revision:348; number_of_response:1; }","duration":"147.980399ms","start":"2025-05-18T18:50:33.115182Z","end":"2025-05-18T18:50:33.263161Z","steps":["trace[1963994729] 'process raft request'  (duration: 112.746663ms)"],"step_count":1}
{"level":"info","ts":"2025-05-18T18:50:33.263785Z","caller":"traceutil/trace.go:171","msg":"trace[1626788136] transaction","detail":"{read_only:false; response_revision:349; number_of_response:1; }","duration":"146.381641ms","start":"2025-05-18T18:50:33.117326Z","end":"2025-05-18T18:50:33.263718Z","steps":["trace[1626788136] 'process raft request'  (duration: 110.942818ms)"],"step_count":1}
{"level":"info","ts":"2025-05-18T18:50:33.360561Z","caller":"traceutil/trace.go:171","msg":"trace[546398598] transaction","detail":"{read_only:false; response_revision:352; number_of_response:1; }","duration":"168.019111ms","start":"2025-05-18T18:50:33.192507Z","end":"2025-05-18T18:50:33.360528Z","steps":["trace[546398598] 'process raft request'  (duration: 142.144847ms)","trace[546398598] 'compare'  (duration: 22.98871ms)"],"step_count":2}
{"level":"info","ts":"2025-05-18T18:50:33.382664Z","caller":"traceutil/trace.go:171","msg":"trace[859209065] transaction","detail":"{read_only:false; response_revision:353; number_of_response:1; }","duration":"160.852937ms","start":"2025-05-18T18:50:33.221760Z","end":"2025-05-18T18:50:33.382614Z","steps":["trace[859209065] 'process raft request'  (duration: 136.406808ms)"],"step_count":1}
{"level":"info","ts":"2025-05-18T18:50:33.384633Z","caller":"traceutil/trace.go:171","msg":"trace[213919076] transaction","detail":"{read_only:false; response_revision:354; number_of_response:1; }","duration":"162.353883ms","start":"2025-05-18T18:50:33.222247Z","end":"2025-05-18T18:50:33.384601Z","steps":["trace[213919076] 'process raft request'  (duration: 136.211284ms)"],"step_count":1}
{"level":"info","ts":"2025-05-18T18:50:33.409443Z","caller":"traceutil/trace.go:171","msg":"trace[1842236672] transaction","detail":"{read_only:false; response_revision:356; number_of_response:1; }","duration":"173.517699ms","start":"2025-05-18T18:50:33.235888Z","end":"2025-05-18T18:50:33.409354Z","steps":["trace[1842236672] 'process raft request'  (duration: 172.429914ms)"],"step_count":1}
{"level":"info","ts":"2025-05-18T18:50:33.410694Z","caller":"traceutil/trace.go:171","msg":"trace[922490693] transaction","detail":"{read_only:false; response_revision:355; number_of_response:1; }","duration":"187.292857ms","start":"2025-05-18T18:50:33.223319Z","end":"2025-05-18T18:50:33.410659Z","steps":["trace[922490693] 'process raft request'  (duration: 135.340041ms)","trace[922490693] 'attach lease to kv pair' {req_type:put; key:/registry/pods/kube-system/kube-controller-manager-minikube; req_size:6305; } (duration: 49.150831ms)"],"step_count":2}
{"level":"warn","ts":"2025-05-18T18:50:33.433779Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"115.711004ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/serviceaccounts/kube-system/replicaset-controller\" limit:1 ","response":"range_response_count:1 size:207"}
{"level":"info","ts":"2025-05-18T18:50:33.444817Z","caller":"traceutil/trace.go:171","msg":"trace[1074423288] range","detail":"{range_begin:/registry/serviceaccounts/kube-system/replicaset-controller; range_end:; response_count:1; response_revision:358; }","duration":"116.091495ms","start":"2025-05-18T18:50:33.317873Z","end":"2025-05-18T18:50:33.433925Z","steps":["trace[1074423288] 'agreement among raft nodes before linearized reading'  (duration: 91.957827ms)","trace[1074423288] 'range keys from bolt db'  (duration: 23.63634ms)"],"step_count":2}
{"level":"info","ts":"2025-05-18T18:59:31.533190Z","caller":"traceutil/trace.go:171","msg":"trace[1337028039] linearizableReadLoop","detail":"{readStateIndex:1076; appliedIndex:1076; }","duration":"165.95927ms","start":"2025-05-18T18:59:31.363268Z","end":"2025-05-18T18:59:31.529229Z","steps":["trace[1337028039] 'read index received'  (duration: 165.888597ms)","trace[1337028039] 'applied index is now lower than readState.Index'  (duration: 34.19Âµs)"],"step_count":2}
{"level":"warn","ts":"2025-05-18T18:59:31.566614Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"197.52116ms","expected-duration":"100ms","prefix":"read-only range ","request":"limit:1 keys_only:true ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2025-05-18T18:59:31.573218Z","caller":"traceutil/trace.go:171","msg":"trace[2012642965] range","detail":"{range_begin:; range_end:; response_count:0; response_revision:946; }","duration":"204.768973ms","start":"2025-05-18T18:59:31.363180Z","end":"2025-05-18T18:59:31.567949Z","steps":["trace[2012642965] 'agreement among raft nodes before linearized reading'  (duration: 193.751373ms)"],"step_count":1}
{"level":"warn","ts":"2025-05-18T18:59:41.062678Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"212.927825ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2025-05-18T18:59:41.063032Z","caller":"traceutil/trace.go:171","msg":"trace[1551738770] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:952; }","duration":"213.278953ms","start":"2025-05-18T18:59:40.849575Z","end":"2025-05-18T18:59:41.062921Z","steps":["trace[1551738770] 'agreement among raft nodes before linearized reading'  (duration: 212.760934ms)"],"step_count":1}
{"level":"info","ts":"2025-05-18T19:00:10.152296Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":732}
{"level":"info","ts":"2025-05-18T19:00:10.202888Z","caller":"mvcc/kvstore_compaction.go:72","msg":"finished scheduled compaction","compact-revision":732,"took":"46.63966ms","hash":880946417,"current-db-size-bytes":2367488,"current-db-size":"2.4 MB","current-db-size-in-use-bytes":2367488,"current-db-size-in-use":"2.4 MB"}
{"level":"info","ts":"2025-05-18T19:00:10.203206Z","caller":"mvcc/hash.go:151","msg":"storing new hash","hash":880946417,"revision":732,"compact-revision":-1}
{"level":"info","ts":"2025-05-18T19:00:56.754279Z","caller":"traceutil/trace.go:171","msg":"trace[1657629950] transaction","detail":"{read_only:false; response_revision:1016; number_of_response:1; }","duration":"258.996773ms","start":"2025-05-18T19:00:56.495206Z","end":"2025-05-18T19:00:56.754203Z","steps":["trace[1657629950] 'process raft request'  (duration: 253.707482ms)"],"step_count":1}
{"level":"info","ts":"2025-05-18T19:00:59.882192Z","caller":"traceutil/trace.go:171","msg":"trace[446795774] transaction","detail":"{read_only:false; response_revision:1017; number_of_response:1; }","duration":"1.061162409s","start":"2025-05-18T19:00:58.820654Z","end":"2025-05-18T19:00:59.881815Z","steps":["trace[446795774] 'process raft request'  (duration: 1.059627231s)"],"step_count":1}
{"level":"info","ts":"2025-05-18T19:00:59.882181Z","caller":"traceutil/trace.go:171","msg":"trace[260617174] linearizableReadLoop","detail":"{readStateIndex:1166; appliedIndex:1165; }","duration":"445.171004ms","start":"2025-05-18T19:00:59.435739Z","end":"2025-05-18T19:00:59.880942Z","steps":["trace[260617174] 'read index received'  (duration: 444.642556ms)","trace[260617174] 'applied index is now lower than readState.Index'  (duration: 477.282Âµs)"],"step_count":2}
{"level":"warn","ts":"2025-05-18T19:00:59.883216Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"447.110012ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/networkpolicies/\" range_end:\"/registry/networkpolicies0\" count_only:true ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2025-05-18T19:00:59.883346Z","caller":"traceutil/trace.go:171","msg":"trace[921894378] range","detail":"{range_begin:/registry/networkpolicies/; range_end:/registry/networkpolicies0; response_count:0; response_revision:1017; }","duration":"447.637661ms","start":"2025-05-18T19:00:59.435659Z","end":"2025-05-18T19:00:59.883297Z","steps":["trace[921894378] 'agreement among raft nodes before linearized reading'  (duration: 446.91764ms)"],"step_count":1}
{"level":"warn","ts":"2025-05-18T19:00:59.917719Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"104.166545ms","expected-duration":"100ms","prefix":"read-only range ","request":"limit:1 keys_only:true ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2025-05-18T19:00:59.917859Z","caller":"traceutil/trace.go:171","msg":"trace[1526970850] range","detail":"{range_begin:; range_end:; response_count:0; response_revision:1017; }","duration":"104.40019ms","start":"2025-05-18T19:00:59.813421Z","end":"2025-05-18T19:00:59.917822Z","steps":["trace[1526970850] 'agreement among raft nodes before linearized reading'  (duration: 104.019344ms)"],"step_count":1}
{"level":"warn","ts":"2025-05-18T19:00:59.883464Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2025-05-18T19:00:59.435576Z","time spent":"447.808047ms","remote":"127.0.0.1:60324","response type":"/etcdserverpb.KV/Range","request count":0,"request size":58,"response count":0,"response size":27,"request content":"key:\"/registry/networkpolicies/\" range_end:\"/registry/networkpolicies0\" count_only:true "}
{"level":"warn","ts":"2025-05-18T19:01:00.795188Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2025-05-18T19:00:58.820616Z","time spent":"1.061667273s","remote":"127.0.0.1:60160","response type":"/etcdserverpb.KV/Txn","request count":1,"request size":1093,"response count":0,"response size":38,"request content":"compare:<target:MOD key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" mod_revision:1016 > success:<request_put:<key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" value_size:1020 >> failure:<request_range:<key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" > >"}
{"level":"info","ts":"2025-05-18T19:01:14.561593Z","caller":"traceutil/trace.go:171","msg":"trace[1103607298] linearizableReadLoop","detail":"{readStateIndex:1178; appliedIndex:1178; }","duration":"1.749376009s","start":"2025-05-18T19:01:12.812181Z","end":"2025-05-18T19:01:14.561559Z","steps":["trace[1103607298] 'read index received'  (duration: 1.749191021s)","trace[1103607298] 'applied index is now lower than readState.Index'  (duration: 152.808Âµs)"],"step_count":2}
{"level":"warn","ts":"2025-05-18T19:01:14.563297Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"1.75131124s","expected-duration":"100ms","prefix":"read-only range ","request":"limit:1 keys_only:true ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2025-05-18T19:01:14.563468Z","caller":"traceutil/trace.go:171","msg":"trace[1008045467] range","detail":"{range_begin:; range_end:; response_count:0; response_revision:1026; }","duration":"1.751598067s","start":"2025-05-18T19:01:12.811837Z","end":"2025-05-18T19:01:14.563435Z","steps":["trace[1008045467] 'agreement among raft nodes before linearized reading'  (duration: 1.751116358s)"],"step_count":1}
{"level":"warn","ts":"2025-05-18T19:01:14.569315Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"1.291083185s","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" limit:1 ","response":"range_response_count:1 size:1109"}
{"level":"info","ts":"2025-05-18T19:01:14.569519Z","caller":"traceutil/trace.go:171","msg":"trace[836598918] range","detail":"{range_begin:/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath; range_end:; response_count:1; response_revision:1026; }","duration":"1.291581884s","start":"2025-05-18T19:01:13.277899Z","end":"2025-05-18T19:01:14.569483Z","steps":["trace[836598918] 'agreement among raft nodes before linearized reading'  (duration: 1.290602874s)"],"step_count":1}
{"level":"warn","ts":"2025-05-18T19:01:14.569679Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2025-05-18T19:01:13.277849Z","time spent":"1.291726696s","remote":"127.0.0.1:60160","response type":"/etcdserverpb.KV/Range","request count":0,"request size":69,"response count":1,"response size":1131,"request content":"key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" limit:1 "}
{"level":"warn","ts":"2025-05-18T19:01:14.571681Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"879.655978ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2025-05-18T19:01:14.571846Z","caller":"traceutil/trace.go:171","msg":"trace[1963092917] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:1026; }","duration":"879.928313ms","start":"2025-05-18T19:01:13.691888Z","end":"2025-05-18T19:01:14.571760Z","steps":["trace[1963092917] 'agreement among raft nodes before linearized reading'  (duration: 879.311985ms)"],"step_count":1}
{"level":"warn","ts":"2025-05-18T19:01:14.572295Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2025-05-18T19:01:13.691853Z","time spent":"880.114501ms","remote":"127.0.0.1:60002","response type":"/etcdserverpb.KV/Range","request count":0,"request size":18,"response count":0,"response size":27,"request content":"key:\"/registry/health\" "}
{"level":"info","ts":"2025-05-18T19:05:10.213170Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":976}
{"level":"info","ts":"2025-05-18T19:05:10.464223Z","caller":"mvcc/kvstore_compaction.go:72","msg":"finished scheduled compaction","compact-revision":976,"took":"248.027029ms","hash":4123898939,"current-db-size-bytes":2367488,"current-db-size":"2.4 MB","current-db-size-in-use-bytes":1658880,"current-db-size-in-use":"1.7 MB"}
{"level":"info","ts":"2025-05-18T19:05:10.465378Z","caller":"mvcc/hash.go:151","msg":"storing new hash","hash":4123898939,"revision":976,"compact-revision":732}
{"level":"warn","ts":"2025-05-18T19:05:10.475248Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"138.41345ms","expected-duration":"100ms","prefix":"","request":"header:<ID:12393790608687675541 > lease_revoke:<id:2bff96e4bb66b845>","response":"size:27"}
{"level":"warn","ts":"2025-05-18T19:06:44.367609Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"115.511383ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/serviceaccounts/default/default\" limit:1 ","response":"range_response_count:1 size:171"}
{"level":"info","ts":"2025-05-18T19:06:44.393009Z","caller":"traceutil/trace.go:171","msg":"trace[2050853225] range","detail":"{range_begin:/registry/serviceaccounts/default/default; range_end:; response_count:1; response_revision:1374; }","duration":"145.14515ms","start":"2025-05-18T19:06:44.247600Z","end":"2025-05-18T19:06:44.392746Z","steps":["trace[2050853225] 'agreement among raft nodes before linearized reading'  (duration: 109.84046ms)"],"step_count":1}
{"level":"info","ts":"2025-05-18T19:06:45.331311Z","caller":"traceutil/trace.go:171","msg":"trace[415747192] transaction","detail":"{read_only:false; response_revision:1379; number_of_response:1; }","duration":"138.031312ms","start":"2025-05-18T19:06:45.193228Z","end":"2025-05-18T19:06:45.331260Z","steps":["trace[415747192] 'process raft request'  (duration: 116.032449ms)"],"step_count":1}


==> kernel <==
 19:07:52 up 19 min,  0 users,  load average: 1.31, 1.48, 1.11
Linux minikube 5.10.207 #1 SMP Tue Jan 14 08:15:54 UTC 2025 x86_64 GNU/Linux
PRETTY_NAME="Buildroot 2023.02.9"


==> kube-apiserver [0bb41e721c6b] <==
I0518 18:50:15.683802       1 customresource_discovery_controller.go:292] Starting DiscoveryController
I0518 18:50:15.683999       1 gc_controller.go:78] Starting apiserver lease garbage collector
I0518 18:50:15.684207       1 apf_controller.go:377] Starting API Priority and Fairness config controller
I0518 18:50:15.684277       1 apiservice_controller.go:100] Starting APIServiceRegistrationController
I0518 18:50:15.806382       1 cache.go:32] Waiting for caches to sync for APIServiceRegistrationController controller
I0518 18:50:15.684660       1 aggregator.go:169] waiting for initial CRD sync...
I0518 18:50:15.686326       1 local_available_controller.go:156] Starting LocalAvailability controller
I0518 18:50:15.806663       1 cache.go:32] Waiting for caches to sync for LocalAvailability controller
I0518 18:50:15.686425       1 controller.go:80] Starting OpenAPI V3 AggregationController
I0518 18:50:15.686675       1 controller.go:119] Starting legacy_token_tracking_controller
I0518 18:50:15.807145       1 shared_informer.go:313] Waiting for caches to sync for configmaps
I0518 18:50:15.686844       1 controller.go:78] Starting OpenAPI AggregationController
I0518 18:50:15.687248       1 system_namespaces_controller.go:66] Starting system namespaces controller
I0518 18:50:15.810243       1 dynamic_cafile_content.go:161] "Starting controller" name="client-ca-bundle::/var/lib/minikube/certs/ca.crt"
I0518 18:50:15.810487       1 dynamic_cafile_content.go:161] "Starting controller" name="request-header::/var/lib/minikube/certs/front-proxy-ca.crt"
I0518 18:50:15.810834       1 controller.go:142] Starting OpenAPI controller
I0518 18:50:15.811196       1 controller.go:90] Starting OpenAPI V3 controller
I0518 18:50:15.811285       1 naming_controller.go:294] Starting NamingConditionController
I0518 18:50:15.811447       1 establishing_controller.go:81] Starting EstablishingController
I0518 18:50:15.811539       1 nonstructuralschema_controller.go:195] Starting NonStructuralSchemaConditionController
I0518 18:50:15.811647       1 apiapproval_controller.go:189] Starting KubernetesAPIApprovalPolicyConformantConditionController
I0518 18:50:15.811726       1 crd_finalizer.go:269] Starting CRDFinalizer
I0518 18:50:15.811927       1 crdregistration_controller.go:114] Starting crd-autoregister controller
I0518 18:50:15.812139       1 shared_informer.go:313] Waiting for caches to sync for crd-autoregister
I0518 18:50:16.199366       1 shared_informer.go:320] Caches are synced for cluster_authentication_trust_controller
I0518 18:50:16.214733       1 shared_informer.go:320] Caches are synced for crd-autoregister
I0518 18:50:16.239458       1 aggregator.go:171] initial CRD sync complete...
I0518 18:50:16.240720       1 autoregister_controller.go:144] Starting autoregister controller
I0518 18:50:16.241367       1 cache.go:32] Waiting for caches to sync for autoregister controller
I0518 18:50:16.242200       1 cache.go:39] Caches are synced for autoregister controller
I0518 18:50:16.259551       1 cache.go:39] Caches are synced for LocalAvailability controller
I0518 18:50:16.259766       1 cache.go:39] Caches are synced for APIServiceRegistrationController controller
I0518 18:50:16.259939       1 shared_informer.go:320] Caches are synced for configmaps
I0518 18:50:16.263735       1 handler_discovery.go:451] Starting ResourceDiscoveryManager
I0518 18:50:16.305455       1 apf_controller.go:382] Running API Priority and Fairness config worker
I0518 18:50:16.305556       1 apf_controller.go:385] Running API Priority and Fairness periodic rebalancing process
I0518 18:50:16.334952       1 shared_informer.go:320] Caches are synced for node_authorizer
I0518 18:50:16.365308       1 cache.go:39] Caches are synced for RemoteAvailability controller
I0518 18:50:16.438187       1 shared_informer.go:320] Caches are synced for *generic.policySource[*k8s.io/api/admissionregistration/v1.ValidatingAdmissionPolicy,*k8s.io/api/admissionregistration/v1.ValidatingAdmissionPolicyBinding,k8s.io/apiserver/pkg/admission/plugin/policy/validating.Validator]
I0518 18:50:16.438273       1 policy_source.go:240] refreshing policies
I0518 18:50:16.473285       1 controller.go:615] quota admission added evaluator for: namespaces
E0518 18:50:16.478393       1 controller.go:148] "Unhandled Error" err="while syncing ConfigMap \"kube-system/kube-apiserver-legacy-service-account-token-tracking\", err: namespaces \"kube-system\" not found" logger="UnhandledError"
E0518 18:50:16.485214       1 controller.go:145] "Failed to ensure lease exists, will retry" err="namespaces \"kube-system\" not found" interval="200ms"
I0518 18:50:17.120278       1 controller.go:615] quota admission added evaluator for: leases.coordination.k8s.io
I0518 18:50:17.159526       1 storage_scheduling.go:95] created PriorityClass system-node-critical with value 2000001000
I0518 18:50:17.394711       1 storage_scheduling.go:95] created PriorityClass system-cluster-critical with value 2000000000
I0518 18:50:17.394859       1 storage_scheduling.go:111] all system priority classes are created successfully or already exist.
I0518 18:50:24.156948       1 controller.go:615] quota admission added evaluator for: roles.rbac.authorization.k8s.io
I0518 18:50:24.556189       1 controller.go:615] quota admission added evaluator for: rolebindings.rbac.authorization.k8s.io
I0518 18:50:24.934843       1 alloc.go:330] "allocated clusterIPs" service="default/kubernetes" clusterIPs={"IPv4":"10.96.0.1"}
W0518 18:50:25.015951       1 lease.go:265] Resetting endpoints for master service "kubernetes" to [192.168.65.3]
I0518 18:50:25.022175       1 controller.go:615] quota admission added evaluator for: endpoints
I0518 18:50:25.073518       1 controller.go:615] quota admission added evaluator for: endpointslices.discovery.k8s.io
I0518 18:50:25.172226       1 controller.go:615] quota admission added evaluator for: serviceaccounts
I0518 18:50:26.698423       1 controller.go:615] quota admission added evaluator for: deployments.apps
I0518 18:50:26.880489       1 alloc.go:330] "allocated clusterIPs" service="kube-system/kube-dns" clusterIPs={"IPv4":"10.96.0.10"}
I0518 18:50:27.074736       1 controller.go:615] quota admission added evaluator for: daemonsets.apps
I0518 18:50:33.048317       1 controller.go:615] quota admission added evaluator for: controllerrevisions.apps
I0518 18:50:33.126517       1 controller.go:615] quota admission added evaluator for: replicasets.apps
I0518 18:50:44.510760       1 alloc.go:330] "allocated clusterIPs" service="default/thesis-backend-service" clusterIPs={"IPv4":"10.101.46.216"}


==> kube-controller-manager [47c6d5cffd29] <==
I0518 18:50:38.970700       1 node_lifecycle_controller.go:1057] "Controller detected that some Nodes are Ready. Exiting master disruption mode" logger="node-lifecycle-controller"
I0518 18:50:38.992960       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-668d6bf9bc" duration="150.746Âµs"
I0518 18:50:39.021287       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-668d6bf9bc" duration="90.642Âµs"
I0518 18:50:44.635228       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/thesis-backend-74b5dc74d7" duration="246.786558ms"
I0518 18:50:44.878272       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/thesis-backend-74b5dc74d7" duration="239.79575ms"
I0518 18:50:44.878794       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/thesis-backend-74b5dc74d7" duration="155.159Âµs"
I0518 18:50:49.113404       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/thesis-backend-74b5dc74d7" duration="112.705Âµs"
I0518 18:51:03.155350       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/thesis-backend-74b5dc74d7" duration="84.387Âµs"
I0518 18:51:11.013693       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-668d6bf9bc" duration="149.951691ms"
I0518 18:51:11.014964       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-668d6bf9bc" duration="983.064Âµs"
I0518 18:51:17.159370       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/thesis-backend-74b5dc74d7" duration="221.37Âµs"
I0518 18:51:29.149933       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/thesis-backend-74b5dc74d7" duration="115.999Âµs"
I0518 18:51:43.128504       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/thesis-backend-74b5dc74d7" duration="47.065Âµs"
I0518 18:51:57.114222       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/thesis-backend-74b5dc74d7" duration="277.918Âµs"
I0518 18:52:33.305463       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/thesis-backend-74b5dc74d7" duration="164.31Âµs"
I0518 18:52:47.191434       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/thesis-backend-74b5dc74d7" duration="142.224Âµs"
I0518 18:54:02.463412       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0518 18:54:10.113614       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/thesis-backend-74b5dc74d7" duration="235.307Âµs"
I0518 18:54:21.107479       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/thesis-backend-74b5dc74d7" duration="120.012Âµs"
I0518 18:56:50.098358       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/thesis-backend-74b5dc74d7" duration="4.888274ms"
I0518 18:57:03.114960       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/thesis-backend-74b5dc74d7" duration="186.192Âµs"
I0518 18:59:08.225504       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0518 19:02:00.183207       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/thesis-backend-74b5dc74d7" duration="3.98645ms"
I0518 19:02:02.979298       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0518 19:02:11.110615       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/thesis-backend-74b5dc74d7" duration="754.688Âµs"
I0518 19:02:25.328715       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/thesis-backend-74b5dc74d7" duration="193.439949ms"
I0518 19:02:25.330429       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/thesis-backend-74b5dc74d7" duration="1.214038ms"
I0518 19:02:25.413592       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/thesis-backend-74b5dc74d7" duration="75.774644ms"
I0518 19:02:25.416885       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/thesis-backend-74b5dc74d7" duration="1.042916ms"
I0518 19:02:25.567607       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/thesis-backend-74b5dc74d7" duration="19.889028ms"
I0518 19:02:25.735957       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/thesis-backend-74b5dc74d7" duration="5.637141ms"
I0518 19:02:25.803161       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/thesis-backend-74b5dc74d7" duration="148.346Âµs"
I0518 19:02:26.683221       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/thesis-backend-74b5dc74d7" duration="231.264Âµs"
I0518 19:02:28.282488       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/thesis-backend-74b5dc74d7" duration="75.625Âµs"
I0518 19:02:28.333461       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/thesis-backend-74b5dc74d7" duration="572.932Âµs"
I0518 19:02:28.372984       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/thesis-backend-74b5dc74d7" duration="5.322729ms"
I0518 19:02:28.406510       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/thesis-backend-74b5dc74d7" duration="341.662Âµs"
I0518 19:02:29.309685       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/thesis-backend-74b5dc74d7" duration="119.584Âµs"
I0518 19:02:45.145593       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/thesis-backend-74b5dc74d7" duration="106.416Âµs"
I0518 19:03:00.216974       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/thesis-backend-74b5dc74d7" duration="143.203Âµs"
I0518 19:03:14.121821       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/thesis-backend-74b5dc74d7" duration="42.574Âµs"
I0518 19:03:29.168383       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/thesis-backend-74b5dc74d7" duration="97.045Âµs"
I0518 19:03:42.148865       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/thesis-backend-74b5dc74d7" duration="55.07Âµs"
I0518 19:04:14.182966       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/thesis-backend-74b5dc74d7" duration="104.246Âµs"
I0518 19:04:26.129471       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/thesis-backend-74b5dc74d7" duration="117.04Âµs"
I0518 19:05:39.186669       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/thesis-backend-74b5dc74d7" duration="345.649Âµs"
I0518 19:05:50.249698       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/thesis-backend-74b5dc74d7" duration="197.267Âµs"
I0518 19:06:44.076458       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/thesis-backend-74b5dc74d7" duration="918.177443ms"
I0518 19:06:44.319905       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/thesis-backend-74b5dc74d7" duration="243.189469ms"
I0518 19:06:44.321150       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/thesis-backend-74b5dc74d7" duration="148.705Âµs"
I0518 19:06:44.449841       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/thesis-backend-74b5dc74d7" duration="139.023Âµs"
I0518 19:06:44.804399       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/thesis-backend-74b5dc74d7" duration="162.476Âµs"
I0518 19:06:45.216107       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/thesis-backend-74b5dc74d7" duration="681.76Âµs"
I0518 19:06:45.412044       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/thesis-backend-74b5dc74d7" duration="84.533Âµs"
I0518 19:06:45.469236       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/thesis-backend-74b5dc74d7" duration="1.431587ms"
I0518 19:06:48.522855       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/thesis-backend-74b5dc74d7" duration="114.176Âµs"
I0518 19:07:02.116938       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/thesis-backend-74b5dc74d7" duration="99.035Âµs"
I0518 19:07:17.147278       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/thesis-backend-74b5dc74d7" duration="355.994Âµs"
I0518 19:07:32.112290       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/thesis-backend-74b5dc74d7" duration="49.838Âµs"
I0518 19:07:47.118521       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/thesis-backend-74b5dc74d7" duration="111.28Âµs"


==> kube-proxy [73e5fcb63e07] <==
I0518 18:50:36.919811       1 server_linux.go:66] "Using iptables proxy"
E0518 18:50:37.049478       1 proxier.go:733] "Error cleaning up nftables rules" err=<
	could not run nftables command: /dev/stdin:1:1-24: Error: Could not process rule: Operation not supported
	add table ip kube-proxy
	^^^^^^^^^^^^^^^^^^^^^^^^
 >
E0518 18:50:37.163729       1 proxier.go:733] "Error cleaning up nftables rules" err=<
	could not run nftables command: /dev/stdin:1:1-25: Error: Could not process rule: Operation not supported
	add table ip6 kube-proxy
	^^^^^^^^^^^^^^^^^^^^^^^^^
 >
I0518 18:50:37.264582       1 server.go:698] "Successfully retrieved node IP(s)" IPs=["192.168.65.3"]
E0518 18:50:37.264773       1 server.go:234] "Kube-proxy configuration may be incomplete or incorrect" err="nodePortAddresses is unset; NodePort connections will be accepted on all local IPs. Consider using `--nodeport-addresses primary`"
I0518 18:50:37.507127       1 server_linux.go:147] "No iptables support for family" ipFamily="IPv6"
I0518 18:50:37.507193       1 server.go:245] "kube-proxy running in single-stack mode" ipFamily="IPv4"
I0518 18:50:37.507236       1 server_linux.go:170] "Using iptables Proxier"
I0518 18:50:37.610234       1 proxier.go:255] "Setting route_localnet=1 to allow node-ports on localhost; to change this either disable iptables.localhostNodePorts (--iptables-localhost-nodeports) or set nodePortAddresses (--nodeport-addresses) to filter loopback addresses" ipFamily="IPv4"
I0518 18:50:37.611121       1 server.go:497] "Version info" version="v1.32.0"
I0518 18:50:37.611165       1 server.go:499] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0518 18:50:37.664357       1 config.go:199] "Starting service config controller"
I0518 18:50:37.667251       1 shared_informer.go:313] Waiting for caches to sync for service config
I0518 18:50:37.667564       1 config.go:329] "Starting node config controller"
I0518 18:50:37.667588       1 shared_informer.go:313] Waiting for caches to sync for node config
I0518 18:50:37.718903       1 config.go:105] "Starting endpoint slice config controller"
I0518 18:50:37.719108       1 shared_informer.go:313] Waiting for caches to sync for endpoint slice config
I0518 18:50:37.856711       1 shared_informer.go:320] Caches are synced for node config
I0518 18:50:37.857130       1 shared_informer.go:320] Caches are synced for service config
I0518 18:50:37.921913       1 shared_informer.go:320] Caches are synced for endpoint slice config


==> kube-scheduler [0b7d191bcbf1] <==
E0518 18:50:17.503405       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.CSIStorageCapacity: failed to list *v1.CSIStorageCapacity: csistoragecapacities.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"csistoragecapacities\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError"
W0518 18:50:17.527515       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Namespace: namespaces is forbidden: User "system:kube-scheduler" cannot list resource "namespaces" in API group "" at the cluster scope
E0518 18:50:17.530481       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Namespace: failed to list *v1.Namespace: namespaces is forbidden: User \"system:kube-scheduler\" cannot list resource \"namespaces\" in API group \"\" at the cluster scope" logger="UnhandledError"
W0518 18:50:17.544627       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.CSINode: csinodes.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csinodes" in API group "storage.k8s.io" at the cluster scope
E0518 18:50:17.545154       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.CSINode: failed to list *v1.CSINode: csinodes.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"csinodes\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError"
W0518 18:50:17.559387       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.PodDisruptionBudget: poddisruptionbudgets.policy is forbidden: User "system:kube-scheduler" cannot list resource "poddisruptionbudgets" in API group "policy" at the cluster scope
E0518 18:50:17.559744       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.PodDisruptionBudget: failed to list *v1.PodDisruptionBudget: poddisruptionbudgets.policy is forbidden: User \"system:kube-scheduler\" cannot list resource \"poddisruptionbudgets\" in API group \"policy\" at the cluster scope" logger="UnhandledError"
W0518 18:50:17.629928       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.StorageClass: storageclasses.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "storageclasses" in API group "storage.k8s.io" at the cluster scope
E0518 18:50:17.630065       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.StorageClass: failed to list *v1.StorageClass: storageclasses.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"storageclasses\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError"
W0518 18:50:17.639507       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.StatefulSet: statefulsets.apps is forbidden: User "system:kube-scheduler" cannot list resource "statefulsets" in API group "apps" at the cluster scope
E0518 18:50:17.648312       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.StatefulSet: failed to list *v1.StatefulSet: statefulsets.apps is forbidden: User \"system:kube-scheduler\" cannot list resource \"statefulsets\" in API group \"apps\" at the cluster scope" logger="UnhandledError"
W0518 18:50:17.686749       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Pod: pods is forbidden: User "system:kube-scheduler" cannot list resource "pods" in API group "" at the cluster scope
E0518 18:50:17.687563       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Pod: failed to list *v1.Pod: pods is forbidden: User \"system:kube-scheduler\" cannot list resource \"pods\" in API group \"\" at the cluster scope" logger="UnhandledError"
W0518 18:50:17.694440       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.PersistentVolumeClaim: persistentvolumeclaims is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumeclaims" in API group "" at the cluster scope
E0518 18:50:17.696459       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.PersistentVolumeClaim: failed to list *v1.PersistentVolumeClaim: persistentvolumeclaims is forbidden: User \"system:kube-scheduler\" cannot list resource \"persistentvolumeclaims\" in API group \"\" at the cluster scope" logger="UnhandledError"
W0518 18:50:17.750311       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.ReplicationController: replicationcontrollers is forbidden: User "system:kube-scheduler" cannot list resource "replicationcontrollers" in API group "" at the cluster scope
E0518 18:50:17.753311       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.ReplicationController: failed to list *v1.ReplicationController: replicationcontrollers is forbidden: User \"system:kube-scheduler\" cannot list resource \"replicationcontrollers\" in API group \"\" at the cluster scope" logger="UnhandledError"
W0518 18:50:17.805411       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.CSIDriver: csidrivers.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csidrivers" in API group "storage.k8s.io" at the cluster scope
E0518 18:50:17.805620       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.CSIDriver: failed to list *v1.CSIDriver: csidrivers.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"csidrivers\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError"
W0518 18:50:17.821898       1 reflector.go:569] runtime/asm_amd64.s:1700: failed to list *v1.ConfigMap: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot list resource "configmaps" in API group "" in the namespace "kube-system"
E0518 18:50:17.825872       1 reflector.go:166] "Unhandled Error" err="runtime/asm_amd64.s:1700: Failed to watch *v1.ConfigMap: failed to list *v1.ConfigMap: configmaps \"extension-apiserver-authentication\" is forbidden: User \"system:kube-scheduler\" cannot list resource \"configmaps\" in API group \"\" in the namespace \"kube-system\"" logger="UnhandledError"
W0518 18:50:17.860953       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.PersistentVolume: persistentvolumes is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumes" in API group "" at the cluster scope
E0518 18:50:17.861243       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.PersistentVolume: failed to list *v1.PersistentVolume: persistentvolumes is forbidden: User \"system:kube-scheduler\" cannot list resource \"persistentvolumes\" in API group \"\" at the cluster scope" logger="UnhandledError"
W0518 18:50:17.946163       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Node: nodes is forbidden: User "system:kube-scheduler" cannot list resource "nodes" in API group "" at the cluster scope
E0518 18:50:17.946376       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Node: failed to list *v1.Node: nodes is forbidden: User \"system:kube-scheduler\" cannot list resource \"nodes\" in API group \"\" at the cluster scope" logger="UnhandledError"
W0518 18:50:18.014956       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.VolumeAttachment: volumeattachments.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "volumeattachments" in API group "storage.k8s.io" at the cluster scope
E0518 18:50:18.015123       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.VolumeAttachment: failed to list *v1.VolumeAttachment: volumeattachments.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"volumeattachments\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError"
W0518 18:50:19.448512       1 reflector.go:569] runtime/asm_amd64.s:1700: failed to list *v1.ConfigMap: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot list resource "configmaps" in API group "" in the namespace "kube-system"
E0518 18:50:19.448617       1 reflector.go:166] "Unhandled Error" err="runtime/asm_amd64.s:1700: Failed to watch *v1.ConfigMap: failed to list *v1.ConfigMap: configmaps \"extension-apiserver-authentication\" is forbidden: User \"system:kube-scheduler\" cannot list resource \"configmaps\" in API group \"\" in the namespace \"kube-system\"" logger="UnhandledError"
W0518 18:50:19.536653       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.ReplicaSet: replicasets.apps is forbidden: User "system:kube-scheduler" cannot list resource "replicasets" in API group "apps" at the cluster scope
E0518 18:50:19.536825       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.ReplicaSet: failed to list *v1.ReplicaSet: replicasets.apps is forbidden: User \"system:kube-scheduler\" cannot list resource \"replicasets\" in API group \"apps\" at the cluster scope" logger="UnhandledError"
W0518 18:50:19.586452       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.PersistentVolume: persistentvolumes is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumes" in API group "" at the cluster scope
E0518 18:50:19.587213       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.PersistentVolume: failed to list *v1.PersistentVolume: persistentvolumes is forbidden: User \"system:kube-scheduler\" cannot list resource \"persistentvolumes\" in API group \"\" at the cluster scope" logger="UnhandledError"
W0518 18:50:19.661543       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.PodDisruptionBudget: poddisruptionbudgets.policy is forbidden: User "system:kube-scheduler" cannot list resource "poddisruptionbudgets" in API group "policy" at the cluster scope
E0518 18:50:19.662116       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.PodDisruptionBudget: failed to list *v1.PodDisruptionBudget: poddisruptionbudgets.policy is forbidden: User \"system:kube-scheduler\" cannot list resource \"poddisruptionbudgets\" in API group \"policy\" at the cluster scope" logger="UnhandledError"
W0518 18:50:19.765805       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Pod: pods is forbidden: User "system:kube-scheduler" cannot list resource "pods" in API group "" at the cluster scope
E0518 18:50:19.766512       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Pod: failed to list *v1.Pod: pods is forbidden: User \"system:kube-scheduler\" cannot list resource \"pods\" in API group \"\" at the cluster scope" logger="UnhandledError"
W0518 18:50:19.800707       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Service: services is forbidden: User "system:kube-scheduler" cannot list resource "services" in API group "" at the cluster scope
E0518 18:50:19.801418       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Service: failed to list *v1.Service: services is forbidden: User \"system:kube-scheduler\" cannot list resource \"services\" in API group \"\" at the cluster scope" logger="UnhandledError"
W0518 18:50:19.837377       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.StorageClass: storageclasses.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "storageclasses" in API group "storage.k8s.io" at the cluster scope
E0518 18:50:19.837518       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.StorageClass: failed to list *v1.StorageClass: storageclasses.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"storageclasses\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError"
W0518 18:50:19.848678       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.PersistentVolumeClaim: persistentvolumeclaims is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumeclaims" in API group "" at the cluster scope
E0518 18:50:19.848881       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.PersistentVolumeClaim: failed to list *v1.PersistentVolumeClaim: persistentvolumeclaims is forbidden: User \"system:kube-scheduler\" cannot list resource \"persistentvolumeclaims\" in API group \"\" at the cluster scope" logger="UnhandledError"
W0518 18:50:19.987781       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.CSINode: csinodes.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csinodes" in API group "storage.k8s.io" at the cluster scope
E0518 18:50:19.989145       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.CSINode: failed to list *v1.CSINode: csinodes.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"csinodes\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError"
W0518 18:50:20.167914       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.CSIDriver: csidrivers.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csidrivers" in API group "storage.k8s.io" at the cluster scope
E0518 18:50:20.168333       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.CSIDriver: failed to list *v1.CSIDriver: csidrivers.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"csidrivers\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError"
W0518 18:50:20.179378       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Namespace: namespaces is forbidden: User "system:kube-scheduler" cannot list resource "namespaces" in API group "" at the cluster scope
E0518 18:50:20.179705       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Namespace: failed to list *v1.Namespace: namespaces is forbidden: User \"system:kube-scheduler\" cannot list resource \"namespaces\" in API group \"\" at the cluster scope" logger="UnhandledError"
W0518 18:50:20.233518       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Node: nodes is forbidden: User "system:kube-scheduler" cannot list resource "nodes" in API group "" at the cluster scope
E0518 18:50:20.233683       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Node: failed to list *v1.Node: nodes is forbidden: User \"system:kube-scheduler\" cannot list resource \"nodes\" in API group \"\" at the cluster scope" logger="UnhandledError"
W0518 18:50:20.302256       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.CSIStorageCapacity: csistoragecapacities.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csistoragecapacities" in API group "storage.k8s.io" at the cluster scope
E0518 18:50:20.302418       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.CSIStorageCapacity: failed to list *v1.CSIStorageCapacity: csistoragecapacities.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"csistoragecapacities\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError"
W0518 18:50:20.815149       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.StatefulSet: statefulsets.apps is forbidden: User "system:kube-scheduler" cannot list resource "statefulsets" in API group "apps" at the cluster scope
E0518 18:50:20.815307       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.StatefulSet: failed to list *v1.StatefulSet: statefulsets.apps is forbidden: User \"system:kube-scheduler\" cannot list resource \"statefulsets\" in API group \"apps\" at the cluster scope" logger="UnhandledError"
W0518 18:50:20.901914       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.ReplicationController: replicationcontrollers is forbidden: User "system:kube-scheduler" cannot list resource "replicationcontrollers" in API group "" at the cluster scope
E0518 18:50:20.903217       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.ReplicationController: failed to list *v1.ReplicationController: replicationcontrollers is forbidden: User \"system:kube-scheduler\" cannot list resource \"replicationcontrollers\" in API group \"\" at the cluster scope" logger="UnhandledError"
W0518 18:50:21.020847       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.VolumeAttachment: volumeattachments.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "volumeattachments" in API group "storage.k8s.io" at the cluster scope
E0518 18:50:21.021232       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.VolumeAttachment: failed to list *v1.VolumeAttachment: volumeattachments.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"volumeattachments\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError"
I0518 18:50:25.739169       1 shared_informer.go:320] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::client-ca-file


==> kubelet <==
May 18 19:03:42 minikube kubelet[2166]: E0518 19:03:42.054755    2166 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"thesis-backend\" with ImagePullBackOff: \"Back-off pulling image \\\"thesis-backend\\\": ErrImagePull: Error response from daemon: pull access denied for thesis-backend, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/thesis-backend-74b5dc74d7-b5dtb" podUID="d16d8bdc-1f8c-4fca-a861-e96ce5ad16be"
May 18 19:03:58 minikube kubelet[2166]: E0518 19:03:58.828259    2166 log.go:32] "PullImage from image service failed" err="rpc error: code = Unknown desc = Error response from daemon: pull access denied for thesis-backend, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="thesis-backend:latest"
May 18 19:03:58 minikube kubelet[2166]: E0518 19:03:58.829797    2166 kuberuntime_image.go:55] "Failed to pull image" err="Error response from daemon: pull access denied for thesis-backend, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="thesis-backend:latest"
May 18 19:03:58 minikube kubelet[2166]: E0518 19:03:58.830760    2166 kuberuntime_manager.go:1341] "Unhandled Error" err="container &Container{Name:thesis-backend,Image:thesis-backend,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:8080,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-p97t9,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod thesis-backend-74b5dc74d7-b5dtb_default(d16d8bdc-1f8c-4fca-a861-e96ce5ad16be): ErrImagePull: Error response from daemon: pull access denied for thesis-backend, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" logger="UnhandledError"
May 18 19:03:58 minikube kubelet[2166]: E0518 19:03:58.838136    2166 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"thesis-backend\" with ErrImagePull: \"Error response from daemon: pull access denied for thesis-backend, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/thesis-backend-74b5dc74d7-b5dtb" podUID="d16d8bdc-1f8c-4fca-a861-e96ce5ad16be"
May 18 19:04:14 minikube kubelet[2166]: E0518 19:04:14.055834    2166 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"thesis-backend\" with ImagePullBackOff: \"Back-off pulling image \\\"thesis-backend\\\": ErrImagePull: Error response from daemon: pull access denied for thesis-backend, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/thesis-backend-74b5dc74d7-b5dtb" podUID="d16d8bdc-1f8c-4fca-a861-e96ce5ad16be"
May 18 19:04:26 minikube kubelet[2166]: E0518 19:04:26.055468    2166 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"thesis-backend\" with ImagePullBackOff: \"Back-off pulling image \\\"thesis-backend\\\": ErrImagePull: Error response from daemon: pull access denied for thesis-backend, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/thesis-backend-74b5dc74d7-b5dtb" podUID="d16d8bdc-1f8c-4fca-a861-e96ce5ad16be"
May 18 19:04:27 minikube kubelet[2166]: E0518 19:04:27.157237    2166 iptables.go:577] "Could not set up iptables canary" err=<
May 18 19:04:27 minikube kubelet[2166]:         error creating chain "KUBE-KUBELET-CANARY": exit status 3: Ignoring deprecated --wait-interval option.
May 18 19:04:27 minikube kubelet[2166]:         ip6tables v1.8.9 (legacy): can't initialize ip6tables table `nat': Table does not exist (do you need to insmod?)
May 18 19:04:27 minikube kubelet[2166]:         Perhaps ip6tables or your kernel needs to be upgraded.
May 18 19:04:27 minikube kubelet[2166]:  > table="nat" chain="KUBE-KUBELET-CANARY"
May 18 19:04:41 minikube kubelet[2166]: E0518 19:04:41.054794    2166 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"thesis-backend\" with ImagePullBackOff: \"Back-off pulling image \\\"thesis-backend\\\": ErrImagePull: Error response from daemon: pull access denied for thesis-backend, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/thesis-backend-74b5dc74d7-b5dtb" podUID="d16d8bdc-1f8c-4fca-a861-e96ce5ad16be"
May 18 19:04:54 minikube kubelet[2166]: E0518 19:04:54.055384    2166 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"thesis-backend\" with ImagePullBackOff: \"Back-off pulling image \\\"thesis-backend\\\": ErrImagePull: Error response from daemon: pull access denied for thesis-backend, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/thesis-backend-74b5dc74d7-b5dtb" podUID="d16d8bdc-1f8c-4fca-a861-e96ce5ad16be"
May 18 19:05:09 minikube kubelet[2166]: E0518 19:05:09.058826    2166 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"thesis-backend\" with ImagePullBackOff: \"Back-off pulling image \\\"thesis-backend\\\": ErrImagePull: Error response from daemon: pull access denied for thesis-backend, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/thesis-backend-74b5dc74d7-b5dtb" podUID="d16d8bdc-1f8c-4fca-a861-e96ce5ad16be"
May 18 19:05:23 minikube kubelet[2166]: E0518 19:05:23.990834    2166 log.go:32] "PullImage from image service failed" err="rpc error: code = Unknown desc = Error response from daemon: pull access denied for thesis-backend, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="thesis-backend:latest"
May 18 19:05:23 minikube kubelet[2166]: E0518 19:05:23.991071    2166 kuberuntime_image.go:55] "Failed to pull image" err="Error response from daemon: pull access denied for thesis-backend, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="thesis-backend:latest"
May 18 19:05:23 minikube kubelet[2166]: E0518 19:05:23.991396    2166 kuberuntime_manager.go:1341] "Unhandled Error" err="container &Container{Name:thesis-backend,Image:thesis-backend,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:8080,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-p97t9,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod thesis-backend-74b5dc74d7-b5dtb_default(d16d8bdc-1f8c-4fca-a861-e96ce5ad16be): ErrImagePull: Error response from daemon: pull access denied for thesis-backend, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" logger="UnhandledError"
May 18 19:05:23 minikube kubelet[2166]: E0518 19:05:23.993255    2166 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"thesis-backend\" with ErrImagePull: \"Error response from daemon: pull access denied for thesis-backend, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/thesis-backend-74b5dc74d7-b5dtb" podUID="d16d8bdc-1f8c-4fca-a861-e96ce5ad16be"
May 18 19:05:27 minikube kubelet[2166]: E0518 19:05:27.171506    2166 iptables.go:577] "Could not set up iptables canary" err=<
May 18 19:05:27 minikube kubelet[2166]:         error creating chain "KUBE-KUBELET-CANARY": exit status 3: Ignoring deprecated --wait-interval option.
May 18 19:05:27 minikube kubelet[2166]:         ip6tables v1.8.9 (legacy): can't initialize ip6tables table `nat': Table does not exist (do you need to insmod?)
May 18 19:05:27 minikube kubelet[2166]:         Perhaps ip6tables or your kernel needs to be upgraded.
May 18 19:05:27 minikube kubelet[2166]:  > table="nat" chain="KUBE-KUBELET-CANARY"
May 18 19:05:39 minikube kubelet[2166]: E0518 19:05:39.056289    2166 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"thesis-backend\" with ImagePullBackOff: \"Back-off pulling image \\\"thesis-backend\\\": ErrImagePull: Error response from daemon: pull access denied for thesis-backend, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/thesis-backend-74b5dc74d7-b5dtb" podUID="d16d8bdc-1f8c-4fca-a861-e96ce5ad16be"
May 18 19:05:50 minikube kubelet[2166]: E0518 19:05:50.054771    2166 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"thesis-backend\" with ImagePullBackOff: \"Back-off pulling image \\\"thesis-backend\\\": ErrImagePull: Error response from daemon: pull access denied for thesis-backend, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/thesis-backend-74b5dc74d7-b5dtb" podUID="d16d8bdc-1f8c-4fca-a861-e96ce5ad16be"
May 18 19:06:05 minikube kubelet[2166]: E0518 19:06:05.054731    2166 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"thesis-backend\" with ImagePullBackOff: \"Back-off pulling image \\\"thesis-backend\\\": ErrImagePull: Error response from daemon: pull access denied for thesis-backend, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/thesis-backend-74b5dc74d7-b5dtb" podUID="d16d8bdc-1f8c-4fca-a861-e96ce5ad16be"
May 18 19:06:19 minikube kubelet[2166]: E0518 19:06:19.063504    2166 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"thesis-backend\" with ImagePullBackOff: \"Back-off pulling image \\\"thesis-backend\\\": ErrImagePull: Error response from daemon: pull access denied for thesis-backend, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/thesis-backend-74b5dc74d7-b5dtb" podUID="d16d8bdc-1f8c-4fca-a861-e96ce5ad16be"
May 18 19:06:27 minikube kubelet[2166]: E0518 19:06:27.186281    2166 iptables.go:577] "Could not set up iptables canary" err=<
May 18 19:06:27 minikube kubelet[2166]:         error creating chain "KUBE-KUBELET-CANARY": exit status 3: Ignoring deprecated --wait-interval option.
May 18 19:06:27 minikube kubelet[2166]:         ip6tables v1.8.9 (legacy): can't initialize ip6tables table `nat': Table does not exist (do you need to insmod?)
May 18 19:06:27 minikube kubelet[2166]:         Perhaps ip6tables or your kernel needs to be upgraded.
May 18 19:06:27 minikube kubelet[2166]:  > table="nat" chain="KUBE-KUBELET-CANARY"
May 18 19:06:34 minikube kubelet[2166]: E0518 19:06:34.057213    2166 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"thesis-backend\" with ImagePullBackOff: \"Back-off pulling image \\\"thesis-backend\\\": ErrImagePull: Error response from daemon: pull access denied for thesis-backend, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/thesis-backend-74b5dc74d7-b5dtb" podUID="d16d8bdc-1f8c-4fca-a861-e96ce5ad16be"
May 18 19:06:44 minikube kubelet[2166]: I0518 19:06:44.123210    2166 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-6t75m\" (UniqueName: \"kubernetes.io/projected/3762640f-a668-4eb1-82cd-ee461dc20216-kube-api-access-6t75m\") pod \"thesis-backend-74b5dc74d7-f52vp\" (UID: \"3762640f-a668-4eb1-82cd-ee461dc20216\") " pod="default/thesis-backend-74b5dc74d7-f52vp"
May 18 19:06:44 minikube kubelet[2166]: I0518 19:06:44.837171    2166 reconciler_common.go:162] "operationExecutor.UnmountVolume started for volume \"kube-api-access-p97t9\" (UniqueName: \"kubernetes.io/projected/d16d8bdc-1f8c-4fca-a861-e96ce5ad16be-kube-api-access-p97t9\") pod \"d16d8bdc-1f8c-4fca-a861-e96ce5ad16be\" (UID: \"d16d8bdc-1f8c-4fca-a861-e96ce5ad16be\") "
May 18 19:06:44 minikube kubelet[2166]: I0518 19:06:44.851565    2166 operation_generator.go:780] UnmountVolume.TearDown succeeded for volume "kubernetes.io/projected/d16d8bdc-1f8c-4fca-a861-e96ce5ad16be-kube-api-access-p97t9" (OuterVolumeSpecName: "kube-api-access-p97t9") pod "d16d8bdc-1f8c-4fca-a861-e96ce5ad16be" (UID: "d16d8bdc-1f8c-4fca-a861-e96ce5ad16be"). InnerVolumeSpecName "kube-api-access-p97t9". PluginName "kubernetes.io/projected", VolumeGIDValue ""
May 18 19:06:44 minikube kubelet[2166]: I0518 19:06:44.951277    2166 reconciler_common.go:299] "Volume detached for volume \"kube-api-access-p97t9\" (UniqueName: \"kubernetes.io/projected/d16d8bdc-1f8c-4fca-a861-e96ce5ad16be-kube-api-access-p97t9\") on node \"minikube\" DevicePath \"\""
May 18 19:06:46 minikube kubelet[2166]: I0518 19:06:46.293234    2166 pod_container_deletor.go:80] "Container not found in pod's containers" containerID="2f8d82c14c38236050d4328624cb7398d2aeea072b569438bd50a4a6d8c90cb6"
May 18 19:06:47 minikube kubelet[2166]: I0518 19:06:47.078715    2166 kubelet_volumes.go:163] "Cleaned up orphaned pod volumes dir" podUID="d16d8bdc-1f8c-4fca-a861-e96ce5ad16be" path="/var/lib/kubelet/pods/d16d8bdc-1f8c-4fca-a861-e96ce5ad16be/volumes"
May 18 19:06:48 minikube kubelet[2166]: E0518 19:06:48.361465    2166 log.go:32] "PullImage from image service failed" err="rpc error: code = Unknown desc = Error response from daemon: pull access denied for thesis-backend, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="thesis-backend:latest"
May 18 19:06:48 minikube kubelet[2166]: E0518 19:06:48.361620    2166 kuberuntime_image.go:55] "Failed to pull image" err="Error response from daemon: pull access denied for thesis-backend, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="thesis-backend:latest"
May 18 19:06:48 minikube kubelet[2166]: E0518 19:06:48.364479    2166 kuberuntime_manager.go:1341] "Unhandled Error" err="container &Container{Name:thesis-backend,Image:thesis-backend,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:8080,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-6t75m,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod thesis-backend-74b5dc74d7-f52vp_default(3762640f-a668-4eb1-82cd-ee461dc20216): ErrImagePull: Error response from daemon: pull access denied for thesis-backend, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" logger="UnhandledError"
May 18 19:06:48 minikube kubelet[2166]: E0518 19:06:48.370800    2166 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"thesis-backend\" with ErrImagePull: \"Error response from daemon: pull access denied for thesis-backend, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/thesis-backend-74b5dc74d7-f52vp" podUID="3762640f-a668-4eb1-82cd-ee461dc20216"
May 18 19:06:48 minikube kubelet[2166]: E0518 19:06:48.428764    2166 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"thesis-backend\" with ImagePullBackOff: \"Back-off pulling image \\\"thesis-backend\\\": ErrImagePull: Error response from daemon: pull access denied for thesis-backend, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/thesis-backend-74b5dc74d7-f52vp" podUID="3762640f-a668-4eb1-82cd-ee461dc20216"
May 18 19:07:03 minikube kubelet[2166]: E0518 19:07:03.791768    2166 log.go:32] "PullImage from image service failed" err="rpc error: code = Unknown desc = Error response from daemon: pull access denied for thesis-backend, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="thesis-backend:latest"
May 18 19:07:03 minikube kubelet[2166]: E0518 19:07:03.792134    2166 kuberuntime_image.go:55] "Failed to pull image" err="Error response from daemon: pull access denied for thesis-backend, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="thesis-backend:latest"
May 18 19:07:03 minikube kubelet[2166]: E0518 19:07:03.792449    2166 kuberuntime_manager.go:1341] "Unhandled Error" err="container &Container{Name:thesis-backend,Image:thesis-backend,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:8080,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-6t75m,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod thesis-backend-74b5dc74d7-f52vp_default(3762640f-a668-4eb1-82cd-ee461dc20216): ErrImagePull: Error response from daemon: pull access denied for thesis-backend, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" logger="UnhandledError"
May 18 19:07:03 minikube kubelet[2166]: E0518 19:07:03.793976    2166 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"thesis-backend\" with ErrImagePull: \"Error response from daemon: pull access denied for thesis-backend, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/thesis-backend-74b5dc74d7-f52vp" podUID="3762640f-a668-4eb1-82cd-ee461dc20216"
May 18 19:07:17 minikube kubelet[2166]: E0518 19:07:17.054272    2166 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"thesis-backend\" with ImagePullBackOff: \"Back-off pulling image \\\"thesis-backend\\\": ErrImagePull: Error response from daemon: pull access denied for thesis-backend, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/thesis-backend-74b5dc74d7-f52vp" podUID="3762640f-a668-4eb1-82cd-ee461dc20216"
May 18 19:07:27 minikube kubelet[2166]: E0518 19:07:27.155807    2166 iptables.go:577] "Could not set up iptables canary" err=<
May 18 19:07:27 minikube kubelet[2166]:         error creating chain "KUBE-KUBELET-CANARY": exit status 3: Ignoring deprecated --wait-interval option.
May 18 19:07:27 minikube kubelet[2166]:         ip6tables v1.8.9 (legacy): can't initialize ip6tables table `nat': Table does not exist (do you need to insmod?)
May 18 19:07:27 minikube kubelet[2166]:         Perhaps ip6tables or your kernel needs to be upgraded.
May 18 19:07:27 minikube kubelet[2166]:  > table="nat" chain="KUBE-KUBELET-CANARY"
May 18 19:07:33 minikube kubelet[2166]: E0518 19:07:33.788212    2166 log.go:32] "PullImage from image service failed" err="rpc error: code = Unknown desc = Error response from daemon: pull access denied for thesis-backend, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="thesis-backend:latest"
May 18 19:07:33 minikube kubelet[2166]: E0518 19:07:33.788402    2166 kuberuntime_image.go:55] "Failed to pull image" err="Error response from daemon: pull access denied for thesis-backend, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="thesis-backend:latest"
May 18 19:07:33 minikube kubelet[2166]: E0518 19:07:33.788767    2166 kuberuntime_manager.go:1341] "Unhandled Error" err="container &Container{Name:thesis-backend,Image:thesis-backend,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:8080,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-6t75m,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod thesis-backend-74b5dc74d7-f52vp_default(3762640f-a668-4eb1-82cd-ee461dc20216): ErrImagePull: Error response from daemon: pull access denied for thesis-backend, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" logger="UnhandledError"
May 18 19:07:33 minikube kubelet[2166]: E0518 19:07:33.791278    2166 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"thesis-backend\" with ErrImagePull: \"Error response from daemon: pull access denied for thesis-backend, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/thesis-backend-74b5dc74d7-f52vp" podUID="3762640f-a668-4eb1-82cd-ee461dc20216"
May 18 19:07:47 minikube kubelet[2166]: E0518 19:07:47.054907    2166 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"thesis-backend\" with ImagePullBackOff: \"Back-off pulling image \\\"thesis-backend\\\": ErrImagePull: Error response from daemon: pull access denied for thesis-backend, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/thesis-backend-74b5dc74d7-f52vp" podUID="3762640f-a668-4eb1-82cd-ee461dc20216"


==> storage-provisioner [82267f62abca] <==
I0518 18:50:37.640135       1 storage_provisioner.go:116] Initializing the minikube storage provisioner...
F0518 18:51:07.805006       1 main.go:39] error getting server version: Get "https://10.96.0.1:443/version?timeout=32s": dial tcp 10.96.0.1:443: i/o timeout


==> storage-provisioner [bd25d7d88768] <==
I0518 18:51:09.814765       1 storage_provisioner.go:116] Initializing the minikube storage provisioner...
I0518 18:51:09.944798       1 storage_provisioner.go:141] Storage provisioner initialized, now starting service!
I0518 18:51:09.945140       1 leaderelection.go:243] attempting to acquire leader lease kube-system/k8s.io-minikube-hostpath...
I0518 18:51:10.134313       1 leaderelection.go:253] successfully acquired lease kube-system/k8s.io-minikube-hostpath
I0518 18:51:10.160399       1 event.go:282] Event(v1.ObjectReference{Kind:"Endpoints", Namespace:"kube-system", Name:"k8s.io-minikube-hostpath", UID:"43da2329-fdfc-4475-ba54-4072a7f7ff30", APIVersion:"v1", ResourceVersion:"490", FieldPath:""}): type: 'Normal' reason: 'LeaderElection' minikube_2d10fd68-afc4-4ed5-aa66-43bb7057aac9 became leader
I0518 18:51:10.201702       1 controller.go:835] Starting provisioner controller k8s.io/minikube-hostpath_minikube_2d10fd68-afc4-4ed5-aa66-43bb7057aac9!
I0518 18:51:10.537623       1 controller.go:884] Started provisioner controller k8s.io/minikube-hostpath_minikube_2d10fd68-afc4-4ed5-aa66-43bb7057aac9!

